{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9de35f",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Encoder-to-Decoder LSTM\n",
    "\n",
    "## Overview\n",
    "This notebook implements Sequence-to-Sequence (Seq2Seq) models using Encoder-to-Decoder LSTM architecture for machine translation. We'll work with the **WMT14 German-English** translation dataset.\n",
    "\n",
    "### Objectives:\n",
    "1. **Deep Learning Models**: Implement using both PyTorch and TensorFlow\n",
    "2. **Dataset**: WMT14 German-English translation pairs\n",
    "3. **Evaluation Metrics**: BLEU Score, Accuracy, Precision, Recall, F1-Score\n",
    "4. **Hyperparameter Tuning**: Comprehensive parameter optimization\n",
    "5. **Target Performance**: 80%+ accuracy on training and testing sets\n",
    "6. **GPU Optimization**: Designed for Google Colab T4 GPU/TPU\n",
    "\n",
    "### Architecture:\n",
    "- **Encoder**: LSTM that reads the source sentence and creates a context vector\n",
    "- **Decoder**: LSTM that generates the target sentence using the context vector\n",
    "- **Attention Mechanism**: Optional enhancement for better performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9264efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU Available (TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU Available (PyTorch):\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "try:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled for TensorFlow\")\n",
    "except:\n",
    "    print(\"Mixed precision not available\")\n",
    "\n",
    "# Device configuration for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5913d4",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing\n",
    "\n",
    "Loading the WMT14 German-English translation dataset and preparing it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WMT14 German-English dataset\n",
    "print(\"Loading WMT14 German-English dataset...\")\n",
    "print(\"Note: This may take several minutes for the first time...\")\n",
    "\n",
    "try:\n",
    "    # Load dataset with a subset for faster processing\n",
    "    dataset = load_dataset(\"wmt14\", \"de-en\", split=\"train[:50000]\")  # Using subset for demo\n",
    "    test_dataset = load_dataset(\"wmt14\", \"de-en\", split=\"test[:5000]\")\n",
    "    \n",
    "    print(f\"Training samples: {len(dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nSample translation pairs:\")\n",
    "    for i in range(3):\n",
    "        german = dataset[i]['translation']['de']\n",
    "        english = dataset[i]['translation']['en']\n",
    "        print(f\"{i+1}. DE: {german}\")\n",
    "        print(f\"   EN: {english}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data if dataset loading fails\n",
    "    sample_data = [\n",
    "        {\"de\": \"Hallo, wie geht es dir?\", \"en\": \"Hello, how are you?\"},\n",
    "        {\"de\": \"Ich bin müde.\", \"en\": \"I am tired.\"},\n",
    "        {\"de\": \"Das Wetter ist schön heute.\", \"en\": \"The weather is nice today.\"},\n",
    "        {\"de\": \"Ich liebe es zu reisen.\", \"en\": \"I love to travel.\"},\n",
    "        {\"de\": \"Können Sie mir helfen?\", \"en\": \"Can you help me?\"},\n",
    "    ] * 1000  # Repeat to create larger dataset\n",
    "    \n",
    "    dataset = sample_data\n",
    "    test_dataset = sample_data[:500]\n",
    "    \n",
    "    print(f\"Using sample dataset - Training: {len(dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Text preprocessing functions\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 10000\n",
    "        self.max_length = 50\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if isinstance(text, dict):\n",
    "            return text  # Already processed\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Add start and end tokens\n",
    "        text = '<start> ' + text.strip() + ' <end>'\n",
    "        return text\n",
    "    \n",
    "    def create_tokenizer(self, texts):\n",
    "        \"\"\"Create tokenizer from texts\"\"\"\n",
    "        tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<unk>')\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        return tokenizer\n",
    "    \n",
    "    def texts_to_sequences(self, tokenizer, texts):\n",
    "        \"\"\"Convert texts to sequences\"\"\"\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Extract German and English texts\n",
    "if isinstance(dataset[0], dict) and 'translation' in dataset[0]:\n",
    "    german_texts = [preprocessor.clean_text(item['translation']['de']) for item in dataset]\n",
    "    english_texts = [preprocessor.clean_text(item['translation']['en']) for item in english_texts]\n",
    "    \n",
    "    test_german_texts = [preprocessor.clean_text(item['translation']['de']) for item in test_dataset]\n",
    "    test_english_texts = [preprocessor.clean_text(item['translation']['en']) for item in test_dataset]\n",
    "else:\n",
    "    german_texts = [preprocessor.clean_text(item['de']) for item in dataset]\n",
    "    english_texts = [preprocessor.clean_text(item['en']) for item in dataset]\n",
    "    \n",
    "    test_german_texts = [preprocessor.clean_text(item['de']) for item in test_dataset]\n",
    "    test_english_texts = [preprocessor.clean_text(item['en']) for item in test_dataset]\n",
    "\n",
    "print(f\"Preprocessed {len(german_texts)} training pairs\")\n",
    "print(f\"Preprocessed {len(test_german_texts)} test pairs\")\n",
    "\n",
    "# Sample preprocessed data\n",
    "print(\"\\nSample preprocessed data:\")\n",
    "for i in range(2):\n",
    "    print(f\"DE: {german_texts[i]}\")\n",
    "    print(f\"EN: {english_texts[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70713e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizers for German and English\n",
    "print(\"Creating tokenizers...\")\n",
    "\n",
    "# German tokenizer (encoder input)\n",
    "german_tokenizer = preprocessor.create_tokenizer(german_texts)\n",
    "german_vocab_size = len(german_tokenizer.word_index) + 1\n",
    "\n",
    "# English tokenizer (decoder input/output)\n",
    "english_tokenizer = preprocessor.create_tokenizer(english_texts)\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"German vocabulary size: {german_vocab_size}\")\n",
    "print(f\"English vocabulary size: {english_vocab_size}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "print(\"Converting texts to sequences...\")\n",
    "\n",
    "# Training data\n",
    "encoder_input_train = preprocessor.texts_to_sequences(german_tokenizer, german_texts)\n",
    "decoder_input_train = preprocessor.texts_to_sequences(english_tokenizer, english_texts)\n",
    "\n",
    "# For decoder target, we need to shift by one position (remove <start>, keep <end>)\n",
    "decoder_target_train = np.zeros_like(decoder_input_train)\n",
    "decoder_target_train[:, :-1] = decoder_input_train[:, 1:]\n",
    "\n",
    "# Test data\n",
    "encoder_input_test = preprocessor.texts_to_sequences(german_tokenizer, test_german_texts)\n",
    "decoder_input_test = preprocessor.texts_to_sequences(english_tokenizer, test_english_texts)\n",
    "decoder_target_test = np.zeros_like(decoder_input_test)\n",
    "decoder_target_test[:, :-1] = decoder_input_test[:, 1:]\n",
    "\n",
    "print(f\"Encoder input shape: {encoder_input_train.shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_train.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target_train.shape}\")\n",
    "\n",
    "# Create reverse word index for decoding\n",
    "english_reverse_word_index = {v: k for k, v in english_tokenizer.word_index.items()}\n",
    "german_reverse_word_index = {v: k for k, v in german_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_sequence(sequence, reverse_word_index):\n",
    "    \"\"\"Convert sequence back to text\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i, '<unk>') for i in sequence if i > 0])\n",
    "\n",
    "# Sample encoded data\n",
    "print(\"\\nSample encoded sequences:\")\n",
    "print(f\"German: {encoder_input_train[0][:10]}\")\n",
    "print(f\"English input: {decoder_input_train[0][:10]}\")\n",
    "print(f\"English target: {decoder_target_train[0][:10]}\")\n",
    "\n",
    "print(\"\\nDecoded back:\")\n",
    "print(f\"German: {decode_sequence(encoder_input_train[0], german_reverse_word_index)}\")\n",
    "print(f\"English: {decode_sequence(decoder_input_train[0], english_reverse_word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af34a7",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras Seq2Seq Implementation\n",
    "\n",
    "### Encoder-Decoder LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Seq2Seq Model\n",
    "class Seq2SeqTensorFlow:\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, embedding_dim=256, \n",
    "                 lstm_units=512, max_length=50):\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the complete Seq2Seq model\"\"\"\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_inputs = layers.Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding = layers.Embedding(self.encoder_vocab_size, self.embedding_dim, \n",
    "                                           mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
    "        encoder_lstm = layers.LSTM(self.lstm_units, return_state=True, dropout=0.2, \n",
    "                                 recurrent_dropout=0.2, name='encoder_lstm')\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_inputs = layers.Input(shape=(None,), name='decoder_inputs')\n",
    "        decoder_embedding = layers.Embedding(self.decoder_vocab_size, self.embedding_dim, \n",
    "                                           mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "        decoder_lstm = layers.LSTM(self.lstm_units, return_sequences=True, return_state=True,\n",
    "                                 dropout=0.2, recurrent_dropout=0.2, name='decoder_lstm')\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "        \n",
    "        # Dense layer for output\n",
    "        decoder_dense = layers.Dense(self.decoder_vocab_size, activation='softmax', \n",
    "                                   name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # Create model\n",
    "        model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        # Store components for inference\n",
    "        self.model = model\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_states = encoder_states\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_lstm = decoder_lstm\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.encoder_embedding = encoder_embedding\n",
    "        self.decoder_embedding = decoder_embedding\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_inference_models(self):\n",
    "        \"\"\"Build separate encoder and decoder models for inference\"\"\"\n",
    "        \n",
    "        # Encoder model for inference\n",
    "        encoder_model = models.Model(self.encoder_inputs, self.encoder_states)\n",
    "        \n",
    "        # Decoder model for inference\n",
    "        decoder_state_input_h = layers.Input(shape=(self.lstm_units,))\n",
    "        decoder_state_input_c = layers.Input(shape=(self.lstm_units,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        \n",
    "        decoder_embedding_inf = self.decoder_embedding(self.decoder_inputs)\n",
    "        decoder_outputs_inf, state_h_inf, state_c_inf = self.decoder_lstm(\n",
    "            decoder_embedding_inf, initial_state=decoder_states_inputs)\n",
    "        decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "        decoder_outputs_inf = self.decoder_dense(decoder_outputs_inf)\n",
    "        \n",
    "        decoder_model = models.Model([self.decoder_inputs] + decoder_states_inputs,\n",
    "                                   [decoder_outputs_inf] + decoder_states_inf)\n",
    "        \n",
    "        return encoder_model, decoder_model\n",
    "\n",
    "# Create TensorFlow model\n",
    "print(\"Creating TensorFlow Seq2Seq model...\")\n",
    "tf_seq2seq = Seq2SeqTensorFlow(\n",
    "    encoder_vocab_size=german_vocab_size,\n",
    "    decoder_vocab_size=english_vocab_size,\n",
    "    embedding_dim=256,\n",
    "    lstm_units=512\n",
    ")\n",
    "\n",
    "tf_model = tf_seq2seq.build_model()\n",
    "print(tf_model.summary())\n",
    "\n",
    "# Visualize model architecture\n",
    "try:\n",
    "    plot_model(tf_model, to_file='seq2seq_model.png', show_shapes=True, show_layer_names=True)\n",
    "    print(\"Model architecture saved as seq2seq_model.png\")\n",
    "except:\n",
    "    print(\"Could not generate model visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f60e97",
   "metadata": {},
   "source": [
    "## PyTorch Seq2Seq Implementation\n",
    "\n",
    "### Encoder-Decoder LSTM with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Seq2Seq Implementation\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1, dropout=0.2):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout if n_layers > 1 else 0, \n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1, dropout=0.2):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers > 1 else 0,\n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_seq, hidden, cell):\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.output_projection(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2SeqPyTorch(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqPyTorch, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to decoder is <start> token\n",
    "        decoder_input = trg[:, 0].unsqueeze(1)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Decode one step\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            # Teacher forcing: use actual next token as next input\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)\n",
    "            decoder_input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# Initialize PyTorch models\n",
    "print(\"Creating PyTorch Seq2Seq model...\")\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Create encoder and decoder\n",
    "encoder = EncoderLSTM(german_vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = DecoderLSTM(english_vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "# Create Seq2Seq model\n",
    "pytorch_model = Seq2SeqPyTorch(encoder, decoder, device).to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"PyTorch model has {count_parameters(pytorch_model):,} trainable parameters\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "optimizer_pytorch = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_pytorch, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"PyTorch model created successfully!\")\n",
    "\n",
    "# PyTorch Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, decoder_target):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_target = decoder_target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.encoder_input[idx], dtype=torch.long),\n",
    "            torch.tensor(self.decoder_input[idx], dtype=torch.long),\n",
    "            torch.tensor(self.decoder_target[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TranslationDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TranslationDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd9e2d",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Functions\n",
    "\n",
    "Implementing comprehensive evaluation metrics including BLEU score, accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4af5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, english_tokenizer, english_reverse_word_index):\n",
    "        self.english_tokenizer = english_tokenizer\n",
    "        self.english_reverse_word_index = english_reverse_word_index\n",
    "        \n",
    "    def decode_sequences(self, sequences):\n",
    "        \"\"\"Decode sequences back to text\"\"\"\n",
    "        decoded_texts = []\n",
    "        for seq in sequences:\n",
    "            decoded = []\n",
    "            for token_id in seq:\n",
    "                if token_id == 0:  # Padding\n",
    "                    break\n",
    "                if token_id in self.english_reverse_word_index:\n",
    "                    word = self.english_reverse_word_index[token_id]\n",
    "                    if word not in ['<start>', '<end>']:\n",
    "                        decoded.append(word)\n",
    "                else:\n",
    "                    decoded.append('<unk>')\n",
    "            decoded_texts.append(' '.join(decoded))\n",
    "        return decoded_texts\n",
    "    \n",
    "    def calculate_bleu_score(self, references, candidates):\n",
    "        \"\"\"Calculate BLEU score\"\"\"\n",
    "        # Convert texts to tokens for BLEU calculation\n",
    "        ref_tokens = [ref.split() for ref in references]\n",
    "        cand_tokens = [cand.split() for cand in candidates]\n",
    "        \n",
    "        # Calculate individual BLEU scores\n",
    "        bleu_scores = []\n",
    "        for ref, cand in zip(ref_tokens, cand_tokens):\n",
    "            if len(cand) == 0:\n",
    "                bleu_scores.append(0.0)\n",
    "            else:\n",
    "                score = sentence_bleu([ref], cand, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "                bleu_scores.append(score)\n",
    "        \n",
    "        # Calculate corpus BLEU\n",
    "        corpus_bleu_score = corpus_bleu([ref_tokens], [cand_tokens])\n",
    "        \n",
    "        return np.mean(bleu_scores), corpus_bleu_score\n",
    "    \n",
    "    def calculate_accuracy_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate token-level accuracy metrics\"\"\"\n",
    "        # Flatten sequences and remove padding\n",
    "        true_tokens = []\n",
    "        pred_tokens = []\n",
    "        \n",
    "        for true_seq, pred_seq in zip(y_true, y_pred):\n",
    "            for t, p in zip(true_seq, pred_seq):\n",
    "                if t != 0:  # Skip padding tokens\n",
    "                    true_tokens.append(t)\n",
    "                    pred_tokens.append(p)\n",
    "        \n",
    "        if len(true_tokens) == 0:\n",
    "            return 0, 0, 0, 0\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_tokens, pred_tokens)\n",
    "        precision = precision_score(true_tokens, pred_tokens, average='weighted', zero_division=0)\n",
    "        recall = recall_score(true_tokens, pred_tokens, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(true_tokens, pred_tokens, average='weighted', zero_division=0)\n",
    "        \n",
    "        return accuracy, precision, recall, f1\n",
    "    \n",
    "    def evaluate_model_comprehensive(self, model, encoder_input, decoder_input, decoder_target, \n",
    "                                   model_type=\"tensorflow\", batch_size=32):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        print(f\"\\n=== {model_type.upper()} Model Evaluation ===\")\n",
    "        \n",
    "        if model_type == \"tensorflow\":\n",
    "            # TensorFlow evaluation\n",
    "            predictions = model.predict([encoder_input, decoder_input], \n",
    "                                      batch_size=batch_size, verbose=0)\n",
    "            pred_sequences = np.argmax(predictions, axis=-1)\n",
    "            \n",
    "        else:\n",
    "            # PyTorch evaluation\n",
    "            model.eval()\n",
    "            pred_sequences = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(encoder_input), batch_size):\n",
    "                    batch_encoder = torch.tensor(encoder_input[i:i+batch_size], dtype=torch.long).to(device)\n",
    "                    batch_decoder = torch.tensor(decoder_input[i:i+batch_size], dtype=torch.long).to(device)\n",
    "                    \n",
    "                    outputs = model(batch_encoder, batch_decoder, teacher_forcing_ratio=0)\n",
    "                    pred_batch = outputs.argmax(dim=-1).cpu().numpy()\n",
    "                    pred_sequences.extend(pred_batch)\n",
    "            \n",
    "            pred_sequences = np.array(pred_sequences)\n",
    "        \n",
    "        # Decode sequences\n",
    "        reference_texts = self.decode_sequences(decoder_target)\n",
    "        candidate_texts = self.decode_sequences(pred_sequences)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        avg_bleu, corpus_bleu = self.calculate_bleu_score(reference_texts, candidate_texts)\n",
    "        \n",
    "        # Calculate token-level accuracy metrics\n",
    "        accuracy, precision, recall, f1 = self.calculate_accuracy_metrics(\n",
    "            decoder_target, pred_sequences)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "        print(f\"Corpus BLEU Score: {corpus_bleu:.4f}\")\n",
    "        print(f\"Token Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Token Precision: {precision:.4f}\")\n",
    "        print(f\"Token Recall: {recall:.4f}\")\n",
    "        print(f\"Token F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # Show sample translations\n",
    "        print(\"\\nSample Translations:\")\n",
    "        for i in range(min(5, len(reference_texts))):\n",
    "            print(f\"Reference: {reference_texts[i]}\")\n",
    "            print(f\"Predicted: {candidate_texts[i]}\")\n",
    "            print()\n",
    "        \n",
    "        return {\n",
    "            'avg_bleu': avg_bleu,\n",
    "            'corpus_bleu': corpus_bleu,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'references': reference_texts,\n",
    "            'predictions': candidate_texts\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = TranslationEvaluator(english_tokenizer, english_reverse_word_index)\n",
    "\n",
    "# Plotting functions\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    if 'accuracy' in history:\n",
    "        axes[0].plot(history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in history:\n",
    "            axes[0].plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0].set_title(f'{title} - Accuracy')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    if 'loss' in history:\n",
    "        axes[1].plot(history['loss'], label='Training Loss')\n",
    "        if 'val_loss' in history:\n",
    "            axes[1].plot(history['val_loss'], label='Validation Loss')\n",
    "        axes[1].set_title(f'{title} - Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pytorch_history(train_losses, val_losses, train_accuracies, val_accuracies, title=\"PyTorch Training\"):\n",
    "    \"\"\"Plot PyTorch training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(train_accuracies, label='Training Accuracy')\n",
    "    axes[0].plot(val_accuracies, label='Validation Accuracy')\n",
    "    axes[0].set_title(f'{title} - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(train_losses, label='Training Loss')\n",
    "    axes[1].plot(val_losses, label='Validation Loss')\n",
    "    axes[1].set_title(f'{title} - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation functions created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe574f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Systematic hyperparameter optimization for both TensorFlow and PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size):\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.best_params = {}\n",
    "        self.best_scores = {}\n",
    "    \n",
    "    def tune_tensorflow_model(self, train_data, val_data, max_trials=3):\n",
    "        \"\"\"Hyperparameter tuning for TensorFlow model\"\"\"\n",
    "        print(\"=== TensorFlow Hyperparameter Tuning ===\")\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        param_combinations = [\n",
    "            {'embedding_dim': 128, 'lstm_units': 256, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "            {'embedding_dim': 256, 'lstm_units': 512, 'learning_rate': 0.0005, 'batch_size': 64},\n",
    "            {'embedding_dim': 256, 'lstm_units': 256, 'learning_rate': 0.001, 'batch_size': 32},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        encoder_input_train, decoder_input_train, decoder_target_train = train_data\n",
    "        encoder_input_val, decoder_input_val, decoder_target_val = val_data\n",
    "        \n",
    "        for i, params in enumerate(param_combinations[:max_trials]):\n",
    "            print(f\"\\nTrial {i+1}/{max_trials}: {params}\")\n",
    "            \n",
    "            try:\n",
    "                # Create model with current parameters\n",
    "                tf_seq2seq_trial = Seq2SeqTensorFlow(\n",
    "                    encoder_vocab_size=self.encoder_vocab_size,\n",
    "                    decoder_vocab_size=self.decoder_vocab_size,\n",
    "                    embedding_dim=params['embedding_dim'],\n",
    "                    lstm_units=params['lstm_units']\n",
    "                )\n",
    "                \n",
    "                model = tf_seq2seq_trial.build_model()\n",
    "                \n",
    "                # Compile with current learning rate\n",
    "                optimizer = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "                model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
    "                            metrics=['accuracy'])\\n                \n",
    "                # Train for few epochs\n",
    "                history = model.fit(\n",
    "                    [encoder_input_train, decoder_input_train], \n",
    "                    decoder_target_train,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=5,  # Quick training for tuning\n",
    "                    validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Get best validation accuracy\n",
    "                best_val_acc = max(history.history['val_accuracy'])\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "                \n",
    "                if best_val_acc > best_score:\n",
    "                    best_score = best_val_acc\n",
    "                    best_params = params\n",
    "                    best_model = model\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in trial {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params['tensorflow'] = best_params\n",
    "        self.best_scores['tensorflow'] = best_score\n",
    "        \n",
    "        print(f\"\\nBest TensorFlow parameters: {best_params}\")\n",
    "        print(f\"Best TensorFlow score: {best_score:.4f}\")\n",
    "        \n",
    "        return best_model, best_params\n",
    "    \n",
    "    def tune_pytorch_model(self, train_loader, val_loader, max_trials=3):\n",
    "        \"\"\"Hyperparameter tuning for PyTorch model\"\"\"\n",
    "        print(\"\\\\n=== PyTorch Hyperparameter Tuning ===\")\n",
    "        \n",
    "        # Define hyperparameter search space\n",
    "        param_combinations = [\n",
    "            {'embedding_dim': 128, 'hidden_dim': 256, 'n_layers': 1, 'learning_rate': 0.001, 'dropout': 0.2},\n",
    "            {'embedding_dim': 256, 'hidden_dim': 512, 'n_layers': 2, 'learning_rate': 0.0005, 'dropout': 0.3},\n",
    "            {'embedding_dim': 256, 'hidden_dim': 256, 'n_layers': 2, 'learning_rate': 0.001, 'dropout': 0.2},\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        \n",
    "        for i, params in enumerate(param_combinations[:max_trials]):\n",
    "            print(f\"\\\\nTrial {i+1}/{max_trials}: {params}\")\n",
    "            \n",
    "            try:\n",
    "                # Create model with current parameters\n",
    "                encoder = EncoderLSTM(\n",
    "                    self.encoder_vocab_size, \n",
    "                    params['embedding_dim'], \n",
    "                    params['hidden_dim'], \n",
    "                    params['n_layers'], \n",
    "                    params['dropout']\n",
    "                )\n",
    "                decoder = DecoderLSTM(\n",
    "                    self.decoder_vocab_size, \n",
    "                    params['embedding_dim'], \n",
    "                    params['hidden_dim'], \n",
    "                    params['n_layers'], \n",
    "                    params['dropout']\n",
    "                )\n",
    "                \n",
    "                model = Seq2SeqPyTorch(encoder, decoder, device).to(device)\n",
    "                \n",
    "                # Setup training\n",
    "                criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "                \n",
    "                # Quick training for tuning\n",
    "                model.train()\n",
    "                train_losses = []\n",
    "                val_accuracies = []\n",
    "                \n",
    "                for epoch in range(3):  # Quick training\n",
    "                    epoch_loss = 0\n",
    "                    for batch_idx, (src, trg_input, trg_output) in enumerate(train_loader):\n",
    "                        if batch_idx >= 10:  # Limit batches for quick tuning\n",
    "                            break\n",
    "                            \n",
    "                        src, trg_input, trg_output = src.to(device), trg_input.to(device), trg_output.to(device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(src, trg_input)\n",
    "                        \n",
    "                        # Reshape for loss calculation\n",
    "                        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "                        trg_output = trg_output[:, 1:].reshape(-1)\n",
    "                        \n",
    "                        loss = criterion(output, trg_output)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        epoch_loss += loss.item()\n",
    "                    \n",
    "                    train_losses.append(epoch_loss / min(10, len(train_loader)))\n",
    "                \n",
    "                # Quick validation\n",
    "                model.eval()\n",
    "                val_acc = self.quick_pytorch_validation(model, val_loader)\n",
    "                val_accuracies.append(val_acc)\n",
    "                \n",
    "                best_val_acc = max(val_accuracies)\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "                \n",
    "                if best_val_acc > best_score:\n",
    "                    best_score = best_val_acc\n",
    "                    best_params = params\n",
    "                    best_model = model\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in trial {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.best_params['pytorch'] = best_params\n",
    "        self.best_scores['pytorch'] = best_score\n",
    "        \n",
    "        print(f\"\\\\nBest PyTorch parameters: {best_params}\")\n",
    "        print(f\"Best PyTorch score: {best_score:.4f}\")\n",
    "        \n",
    "        return best_model, best_params\n",
    "    \n",
    "    def quick_pytorch_validation(self, model, val_loader):\n",
    "        \"\"\"Quick validation for hyperparameter tuning\"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (src, trg_input, trg_output) in enumerate(val_loader):\n",
    "                if batch_idx >= 5:  # Limit for quick validation\n",
    "                    break\n",
    "                    \n",
    "                src, trg_input, trg_output = src.to(device), trg_input.to(device), trg_output.to(device)\n",
    "                output = model(src, trg_input, teacher_forcing_ratio=0)\n",
    "                \n",
    "                pred = output.argmax(dim=-1)\n",
    "                mask = trg_output != 0  # Ignore padding\n",
    "                correct += (pred == trg_output)[mask].sum().item()\n",
    "                total += mask.sum().item()\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "# Initialize hyperparameter tuner\n",
    "tuner = HyperparameterTuner(german_vocab_size, english_vocab_size)\n",
    "\n",
    "# Prepare validation data (split from training data)\n",
    "val_split = int(0.1 * len(encoder_input_train))\n",
    "encoder_input_val = encoder_input_train[:val_split]\n",
    "decoder_input_val = decoder_input_train[:val_split]\n",
    "decoder_target_val = decoder_target_train[:val_split]\n",
    "\n",
    "encoder_input_train_split = encoder_input_train[val_split:]\n",
    "decoder_input_train_split = decoder_input_train[val_split:]\n",
    "decoder_target_train_split = decoder_target_train[val_split:]\n",
    "\n",
    "print(\"Hyperparameter tuning setup complete!\")\n",
    "print(f\"Training samples: {len(encoder_input_train_split)}\")\n",
    "print(f\"Validation samples: {len(encoder_input_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927f04b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training both TensorFlow and PyTorch models with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35539dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Hyperparameter Tuning and Training\n",
    "\n",
    "print(\"Starting hyperparameter tuning and model training...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# 1. TensorFlow Model Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TENSORFLOW MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "train_data_tf = (encoder_input_train_split, decoder_input_train_split, decoder_target_train_split)\n",
    "val_data_tf = (encoder_input_val, decoder_input_val, decoder_target_val)\n",
    "\n",
    "best_tf_model, best_tf_params = tuner.tune_tensorflow_model(train_data_tf, val_data_tf, max_trials=3)\n",
    "\n",
    "# Train final TensorFlow model with best parameters\n",
    "print(\"\\nTraining final TensorFlow model...\")\n",
    "final_tf_seq2seq = Seq2SeqTensorFlow(\n",
    "    encoder_vocab_size=german_vocab_size,\n",
    "    decoder_vocab_size=english_vocab_size,\n",
    "    **best_tf_params\n",
    ")\n",
    "\n",
    "final_tf_model = final_tf_seq2seq.build_model()\n",
    "\n",
    "# Callbacks for training\n",
    "tf_callbacks = [\n",
    "    callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001),\n",
    "    callbacks.ModelCheckpoint('best_tf_seq2seq.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Training TensorFlow model...\")\n",
    "tf_history = final_tf_model.fit(\n",
    "    [encoder_input_train, decoder_input_train], \n",
    "    decoder_target_train,\n",
    "    batch_size=best_tf_params['batch_size'],\n",
    "    epochs=20,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "    callbacks=tf_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(tf_history.history, \"TensorFlow Seq2Seq\")\n",
    "\n",
    "# Evaluate TensorFlow model\n",
    "print(\"\\nEvaluating TensorFlow model on test data...\")\n",
    "tf_test_results = evaluator.evaluate_model_comprehensive(\n",
    "    final_tf_model, encoder_input_test, decoder_input_test, decoder_target_test, \n",
    "    model_type=\"tensorflow\"\n",
    ")\n",
    "\n",
    "# Check if model meets 80% accuracy requirement\n",
    "tf_train_acc = max(tf_history.history['accuracy'])\n",
    "tf_test_acc = tf_test_results['accuracy']\n",
    "\n",
    "print(f\"\\nTensorFlow Model Performance:\")\n",
    "print(f\"Training Accuracy: {tf_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {tf_test_acc:.4f}\")\n",
    "print(f\"Test BLEU Score: {tf_test_results['avg_bleu']:.4f}\")\n",
    "\n",
    "if tf_train_acc >= 0.8 and tf_test_acc >= 0.8:\n",
    "    print(\"✅ TensorFlow model meets the 80% accuracy requirement!\")\n",
    "else:\n",
    "    print(\"❌ TensorFlow model needs improvement to reach 80% accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PyTorch Model Training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PYTORCH MODEL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create validation data loader\n",
    "val_dataset_pytorch = TranslationDataset(encoder_input_val, decoder_input_val, decoder_target_val)\n",
    "val_loader_pytorch = DataLoader(val_dataset_pytorch, batch_size=32, shuffle=False)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "best_pytorch_model, best_pytorch_params = tuner.tune_pytorch_model(\n",
    "    train_loader, val_loader_pytorch, max_trials=3\n",
    ")\n",
    "\n",
    "# Train final PyTorch model with best parameters\n",
    "print(\"\\nTraining final PyTorch model...\")\n",
    "\n",
    "# Create final model with best parameters\n",
    "final_encoder = EncoderLSTM(\n",
    "    german_vocab_size, \n",
    "    best_pytorch_params['embedding_dim'], \n",
    "    best_pytorch_params['hidden_dim'], \n",
    "    best_pytorch_params['n_layers'], \n",
    "    best_pytorch_params['dropout']\n",
    ")\n",
    "final_decoder = DecoderLSTM(\n",
    "    english_vocab_size, \n",
    "    best_pytorch_params['embedding_dim'], \n",
    "    best_pytorch_params['hidden_dim'], \n",
    "    best_pytorch_params['n_layers'], \n",
    "    best_pytorch_params['dropout']\n",
    ")\n",
    "\n",
    "final_pytorch_model = Seq2SeqPyTorch(final_encoder, final_decoder, device).to(device)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer_final = optim.Adam(final_pytorch_model.parameters(), lr=best_pytorch_params['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_final, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop\n",
    "def train_pytorch_model(model, train_loader, val_loader, num_epochs=15):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (src, trg_input, trg_output) in enumerate(train_loader):\n",
    "            src, trg_input, trg_output = src.to(device), trg_input.to(device), trg_output.to(device)\n",
    "            \n",
    "            optimizer_final.zero_grad()\n",
    "            output = model(src, trg_input)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output_reshaped = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg_output_reshaped = trg_output[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output_reshaped, trg_output_reshaped)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer_final.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = output.argmax(dim=-1)\n",
    "            mask = trg_output != 0\n",
    "            train_correct += (pred == trg_output)[mask].sum().item()\n",
    "            train_total += mask.sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for src, trg_input, trg_output in val_loader:\n",
    "                src, trg_input, trg_output = src.to(device), trg_input.to(device), trg_output.to(device)\n",
    "                \n",
    "                output = model(src, trg_input, teacher_forcing_ratio=0)\n",
    "                \n",
    "                output_reshaped = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "                trg_output_reshaped = trg_output[:, 1:].reshape(-1)\n",
    "                \n",
    "                loss = criterion(output_reshaped, trg_output_reshaped)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "                pred = output.argmax(dim=-1)\n",
    "                mask = trg_output != 0\n",
    "                val_correct += (pred == trg_output)[mask].sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total if train_total > 0 else 0\n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_pytorch_seq2seq.pth')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the PyTorch model\n",
    "print(\"Training PyTorch model...\")\n",
    "pytorch_train_losses, pytorch_val_losses, pytorch_train_accs, pytorch_val_accs = train_pytorch_model(\n",
    "    final_pytorch_model, train_loader, val_loader_pytorch, num_epochs=15\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_pytorch_history(pytorch_train_losses, pytorch_val_losses, \n",
    "                    pytorch_train_accs, pytorch_val_accs, \"PyTorch Seq2Seq\")\n",
    "\n",
    "# Load best model\n",
    "final_pytorch_model.load_state_dict(torch.load('best_pytorch_seq2seq.pth'))\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "print(\"\\nEvaluating PyTorch model on test data...\")\n",
    "pytorch_test_results = evaluator.evaluate_model_comprehensive(\n",
    "    final_pytorch_model, encoder_input_test, decoder_input_test, decoder_target_test, \n",
    "    model_type=\"pytorch\"\n",
    ")\n",
    "\n",
    "# Check if model meets 80% accuracy requirement\n",
    "pytorch_train_acc = max(pytorch_train_accs)\n",
    "pytorch_test_acc = pytorch_test_results['accuracy']\n",
    "\n",
    "print(f\"\\nPyTorch Model Performance:\")\n",
    "print(f\"Training Accuracy: {pytorch_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {pytorch_test_acc:.4f}\")\n",
    "print(f\"Test BLEU Score: {pytorch_test_results['avg_bleu']:.4f}\")\n",
    "\n",
    "if pytorch_train_acc >= 0.8 and pytorch_test_acc >= 0.8:\n",
    "    print(\"✅ PyTorch model meets the 80% accuracy requirement!\")\n",
    "else:\n",
    "    print(\"❌ PyTorch model needs improvement to reach 80% accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036dfde9",
   "metadata": {},
   "source": [
    "## Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Results Analysis\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create results summary DataFrame\n",
    "results_summary = pd.DataFrame({\n",
    "    'Framework': ['TensorFlow', 'PyTorch'],\n",
    "    'Training_Accuracy': [tf_train_acc, pytorch_train_acc],\n",
    "    'Test_Accuracy': [tf_test_acc, pytorch_test_acc],\n",
    "    'Test_BLEU': [tf_test_results['avg_bleu'], pytorch_test_results['avg_bleu']],\n",
    "    'Test_Precision': [tf_test_results['precision'], pytorch_test_results['precision']],\n",
    "    'Test_Recall': [tf_test_results['recall'], pytorch_test_results['recall']],\n",
    "    'Test_F1_Score': [tf_test_results['f1_score'], pytorch_test_results['f1_score']],\n",
    "    'Best_Params': [str(best_tf_params), str(best_pytorch_params)]\n",
    "})\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_summary.round(4))\n",
    "\n",
    "# Visualize results comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "frameworks = ['TensorFlow', 'PyTorch']\n",
    "train_accs = [tf_train_acc, pytorch_train_acc]\n",
    "test_accs = [tf_test_acc, pytorch_test_acc]\n",
    "\n",
    "x = np.arange(len(frameworks))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_accs, width, label='Training Accuracy', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "axes[0, 0].axhline(y=0.8, color='r', linestyle='--', label='80% Target')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(frameworks)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# BLEU Score comparison\n",
    "bleu_scores = [tf_test_results['avg_bleu'], pytorch_test_results['avg_bleu']]\n",
    "axes[0, 1].bar(frameworks, bleu_scores, color=['blue', 'orange'], alpha=0.7)\n",
    "axes[0, 1].set_ylabel('BLEU Score')\n",
    "axes[0, 1].set_title('BLEU Score Comparison')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score comparison\n",
    "f1_scores = [tf_test_results['f1_score'], pytorch_test_results['f1_score']]\n",
    "axes[1, 0].bar(frameworks, f1_scores, color=['green', 'red'], alpha=0.7)\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].set_title('F1-Score Comparison')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision vs Recall\n",
    "precisions = [tf_test_results['precision'], pytorch_test_results['precision']]\n",
    "recalls = [tf_test_results['recall'], pytorch_test_results['recall']]\n",
    "\n",
    "axes[1, 1].scatter(precisions, recalls, s=200, alpha=0.7)\n",
    "for i, framework in enumerate(frameworks):\n",
    "    axes[1, 1].annotate(framework, (precisions[i], recalls[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points')\n",
    "axes[1, 1].set_xlabel('Precision')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_title('Precision vs Recall')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check which models meet the 80% accuracy requirement\n",
    "models_meeting_requirement = []\n",
    "if tf_train_acc >= 0.8 and tf_test_acc >= 0.8:\n",
    "    models_meeting_requirement.append(\"TensorFlow\")\n",
    "if pytorch_train_acc >= 0.8 and pytorch_test_acc >= 0.8:\n",
    "    models_meeting_requirement.append(\"PyTorch\")\n",
    "\n",
    "print(f\"\\nModels meeting 80% accuracy requirement: {models_meeting_requirement}\")\n",
    "\n",
    "# Best performing model\n",
    "best_model = 'TensorFlow' if tf_test_acc > pytorch_test_acc else 'PyTorch'\n",
    "print(f\"Best performing model (by test accuracy): {best_model}\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\nDetailed Analysis:\")\n",
    "print(f\"1. TensorFlow Model:\")\n",
    "print(f\"   - Training Accuracy: {tf_train_acc:.4f}\")\n",
    "print(f\"   - Test Accuracy: {tf_test_acc:.4f}\")\n",
    "print(f\"   - BLEU Score: {tf_test_results['avg_bleu']:.4f}\")\n",
    "print(f\"   - Best Parameters: {best_tf_params}\")\n",
    "\n",
    "print(f\"\\n2. PyTorch Model:\")\n",
    "print(f\"   - Training Accuracy: {pytorch_train_acc:.4f}\")\n",
    "print(f\"   - Test Accuracy: {pytorch_test_acc:.4f}\")\n",
    "print(f\"   - BLEU Score: {pytorch_test_results['avg_bleu']:.4f}\")\n",
    "print(f\"   - Best Parameters: {best_pytorch_params}\")\n",
    "\n",
    "# Translation quality analysis\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"TRANSLATION QUALITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nSample TensorFlow Translations:\")\n",
    "for i in range(3):\n",
    "    if i < len(tf_test_results['references']):\n",
    "        print(f\"Reference: {tf_test_results['references'][i]}\")\n",
    "        print(f\"Predicted: {tf_test_results['predictions'][i]}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nSample PyTorch Translations:\")\n",
    "for i in range(3):\n",
    "    if i < len(pytorch_test_results['references']):\n",
    "        print(f\"Reference: {pytorch_test_results['references'][i]}\")\n",
    "        print(f\"Predicted: {pytorch_test_results['predictions'][i]}\")\n",
    "        print()\n",
    "\n",
    "# Save results\n",
    "results_summary.to_csv('seq2seq_results_comparison.csv', index=False)\n",
    "print(\"Results saved to 'seq2seq_results_comparison.csv'\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. If accuracy is below 80%:\")\n",
    "print(\"   - Increase model size (more LSTM units)\")\n",
    "print(\"   - Add attention mechanism\")\n",
    "print(\"   - Use pre-trained embeddings\")\n",
    "print(\"   - Increase training data\")\n",
    "print(\"   - Implement beam search for decoding\")\n",
    "\n",
    "print(\"\\n2. For better BLEU scores:\")\n",
    "print(\"   - Implement attention mechanism\")\n",
    "print(\"   - Use bidirectional encoder\")\n",
    "print(\"   - Apply beam search decoding\")\n",
    "print(\"   - Use subword tokenization (BPE)\")\n",
    "\n",
    "print(\"\\n3. For production deployment:\")\n",
    "print(\"   - Implement proper inference pipeline\")\n",
    "print(\"   - Add model quantization for speed\")\n",
    "print(\"   - Use larger vocabulary\")\n",
    "print(\"   - Implement length penalty\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb88f7",
   "metadata": {},
   "source": [
    "## Conclusion and Google Colab Optimization\n",
    "\n",
    "### Summary of Implementation\n",
    "\n",
    "This notebook successfully implements **Sequence-to-Sequence (Seq2Seq) learning** using **Encoder-to-Decoder LSTM** architecture for German-English machine translation from the WMT14 dataset.\n",
    "\n",
    "### ✅ **Achievements:**\n",
    "\n",
    "1. **Dual Framework Implementation**: Both TensorFlow/Keras and PyTorch implementations\n",
    "2. **WMT14 Dataset**: German-English translation pairs with proper preprocessing\n",
    "3. **Comprehensive Evaluation**: BLEU scores, accuracy, precision, recall, F1-score\n",
    "4. **Hyperparameter Tuning**: Systematic optimization of model parameters\n",
    "5. **Target Performance**: Aimed for 80%+ accuracy on training and testing sets\n",
    "6. **GPU Optimization**: Designed for Google Colab T4 GPU/TPU acceleration\n",
    "\n",
    "### 🏗️ **Architecture Features:**\n",
    "\n",
    "- **Encoder**: LSTM that processes German input sequences\n",
    "- **Decoder**: LSTM that generates English output sequences\n",
    "- **Embedding Layers**: Dense vector representations for words\n",
    "- **Attention Mechanism**: Enhanced focusing on relevant input parts (PyTorch)\n",
    "- **Teacher Forcing**: Training technique for faster convergence\n",
    "- **Dropout & Regularization**: Preventing overfitting\n",
    "\n",
    "### 📊 **Evaluation Metrics:**\n",
    "\n",
    "- **BLEU Score**: Standard machine translation quality metric\n",
    "- **Token Accuracy**: Word-level prediction accuracy\n",
    "- **Precision/Recall/F1**: Comprehensive classification metrics\n",
    "- **Training vs Test Performance**: Monitoring for overfitting\n",
    "\n",
    "### 🔧 **Hyperparameters Optimized:**\n",
    "\n",
    "- **Embedding Dimension**: 128, 256\n",
    "- **LSTM Hidden Units**: 256, 512\n",
    "- **Number of Layers**: 1, 2\n",
    "- **Learning Rate**: 0.001, 0.0005\n",
    "- **Dropout Rate**: 0.2, 0.3\n",
    "- **Batch Size**: 32, 64\n",
    "\n",
    "### 🚀 **Google Colab Optimization Tips:**\n",
    "\n",
    "```python\n",
    "# 1. Enable GPU/TPU\n",
    "# Runtime > Change runtime type > Hardware accelerator > GPU/TPU\n",
    "\n",
    "# 2. Check GPU availability\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "# 3. Enable mixed precision training for TensorFlow\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# 4. Optimize memory usage\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Clear PyTorch GPU cache\n",
    "\n",
    "# 5. Use larger batch sizes with GPU\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 32\n",
    "\n",
    "# 6. Enable XLA compilation for TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True)\n",
    "```\n",
    "\n",
    "### 🎯 **Performance Targets:**\n",
    "\n",
    "- **Minimum 80% accuracy** on both training and testing sets\n",
    "- **BLEU score > 0.3** for reasonable translation quality\n",
    "- **Balanced precision and recall** for robust performance\n",
    "\n",
    "### 🔄 **Future Improvements:**\n",
    "\n",
    "1. **Attention Mechanisms**: Implement full attention for better alignment\n",
    "2. **Transformer Architecture**: Modern state-of-the-art approach\n",
    "3. **Beam Search Decoding**: Better inference strategy\n",
    "4. **Subword Tokenization**: Handle out-of-vocabulary words\n",
    "5. **Pre-trained Embeddings**: Initialize with Word2Vec/GloVe\n",
    "6. **Data Augmentation**: Back-translation and paraphrasing\n",
    "\n",
    "### 💡 **Key Learnings:**\n",
    "\n",
    "- **Seq2Seq models** are powerful for sequence transduction tasks\n",
    "- **Teacher forcing** significantly speeds up training\n",
    "- **Hyperparameter tuning** is crucial for optimal performance\n",
    "- **Both TensorFlow and PyTorch** have their strengths for implementation\n",
    "- **GPU acceleration** dramatically reduces training time\n",
    "- **BLEU scores** provide meaningful translation quality assessment\n",
    "\n",
    "This implementation provides a solid foundation for machine translation tasks and can be extended to other sequence-to-sequence problems like text summarization, dialogue systems, and question answering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
