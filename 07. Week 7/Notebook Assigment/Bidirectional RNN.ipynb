{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pandu98-pkh/DeepLearning/blob/main/07.%20Week%207/Notebook%20Assigment/Bidirectional%20RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631690a2",
      "metadata": {
        "id": "631690a2"
      },
      "source": [
        "# Bidirectional RNN for Text Classification\n",
        "\n",
        "## Overview\n",
        "This notebook implements Bidirectional RNN models for text classification tasks using three different datasets:\n",
        "1. **IMDb Dataset** - Movie review sentiment analysis\n",
        "2. **ReviewTokoBaju.csv** - Clothing review sentiment analysis\n",
        "3. **DeteksiSarkasme.json** - Sarcasm detection\n",
        "\n",
        "## Objectives\n",
        "- Build BiRNN models using TensorFlow/Keras\n",
        "- Achieve minimum 90% accuracy on both training and testing sets\n",
        "- Implement comprehensive evaluation metrics (Accuracy, Precision, Recall, F1-Score, AUC, ROC)\n",
        "- Perform hyperparameter tuning\n",
        "- Visualize training metrics and confusion matrices\n",
        "\n",
        "## Hardware Recommendation\n",
        "This notebook is optimized for Google Colab with T4 GPU or TPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f04de3fc",
      "metadata": {
        "id": "f04de3fc",
        "outputId": "88874e83-c768-4386-f8bb-51e9d566232a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout, GlobalMaxPooling1D, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# NLTK for text preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a1254f00",
      "metadata": {
        "id": "a1254f00",
        "outputId": "98f88f59-c1b2-4f7c-96f5-274fd23bc71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and evaluation functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing functions\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text data\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
        "    \"\"\"Advanced text preprocessing with stopword removal and lemmatization\"\"\"\n",
        "    text = clean_text(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Evaluation functions\n",
        "def plot_training_history(history, title=\"Model Training History\"):\n",
        "    \"\"\"Plot training and validation accuracy and loss\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "    ax1.set_title(f'{title} - Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Loss plot\n",
        "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
        "    ax2.set_title(f'{title} - Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, y_pred_proba, dataset_name):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    # Predictions\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # AUC-ROC (handle binary and multiclass)\n",
        "    if len(np.unique(y_test)) == 2:\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "\n",
        "    print(f\"\\n=== {dataset_name} Model Evaluation ===\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{dataset_name} - Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    print(f\"\\nClassification Report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc_roc': auc\n",
        "    }\n",
        "\n",
        "def plot_roc_curve(y_test, y_pred_proba, dataset_name):\n",
        "    \"\"\"Plot ROC curve for binary classification\"\"\"\n",
        "    if len(np.unique(y_test)) == 2:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{dataset_name} - ROC Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "print(\"Preprocessing and evaluation functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea70f5f",
      "metadata": {
        "id": "6ea70f5f"
      },
      "source": [
        "## Dataset 1: IMDb Movie Reviews\n",
        "\n",
        "IMDb dataset contains movie reviews with binary sentiment labels (positive/negative). This is a classic sentiment analysis task perfect for BiRNN implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c007f53",
      "metadata": {
        "id": "4c007f53",
        "outputId": "820900b0-e7e7-40b3-c335-e41d1a78ec39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDb dataset...\n",
            "Converting encoded reviews back to text...\n",
            "Preprocessing IMDb text data...\n",
            "IMDb dataset shape:\n",
            "Training set: (25000, 500)\n",
            "Test set: (25000, 500)\n",
            "Labels distribution - Train: [12500 12500]\n",
            "Labels distribution - Test: [12500 12500]\n",
            "\n",
            "Sample original review: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the sa...\n",
            "Sample preprocessed review: film brilliant casting location scenery story direction everyones really suited part played could imagine robert amazing actor director father came scottish island loved fact real connection film witt...\n",
            "Label: 1 (Positive)\n"
          ]
        }
      ],
      "source": [
        "# Load IMDb dataset\n",
        "print(\"Loading IMDb dataset...\")\n",
        "(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "# Get word index\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "def decode_review(encoded_review):\n",
        "    \"\"\"Decode numerical review back to text\"\"\"\n",
        "    return ' '.join([reverse_word_index.get(i-3, '?') for i in encoded_review])\n",
        "\n",
        "# Convert back to text for preprocessing\n",
        "print(\"Converting encoded reviews back to text...\")\n",
        "X_train_imdb_text = [decode_review(review) for review in X_train_imdb]\n",
        "X_test_imdb_text = [decode_review(review) for review in X_test_imdb]\n",
        "\n",
        "# Preprocess text data\n",
        "print(\"Preprocessing IMDb text data...\")\n",
        "X_train_imdb_clean = [preprocess_text(text) for text in X_train_imdb_text]\n",
        "X_test_imdb_clean = [preprocess_text(text) for text in X_test_imdb_text]\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer_imdb = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer_imdb.fit_on_texts(X_train_imdb_clean)\n",
        "\n",
        "X_train_imdb_seq = tokenizer_imdb.texts_to_sequences(X_train_imdb_clean)\n",
        "X_test_imdb_seq = tokenizer_imdb.texts_to_sequences(X_test_imdb_clean)\n",
        "\n",
        "# Padding sequences\n",
        "max_length_imdb = 500\n",
        "X_train_imdb_pad = pad_sequences(X_train_imdb_seq, maxlen=max_length_imdb, padding='post', truncating='post')\n",
        "X_test_imdb_pad = pad_sequences(X_test_imdb_seq, maxlen=max_length_imdb, padding='post', truncating='post')\n",
        "\n",
        "print(f\"IMDb dataset shape:\")\n",
        "print(f\"Training set: {X_train_imdb_pad.shape}\")\n",
        "print(f\"Test set: {X_test_imdb_pad.shape}\")\n",
        "print(f\"Labels distribution - Train: {np.bincount(y_train_imdb)}\")\n",
        "print(f\"Labels distribution - Test: {np.bincount(y_test_imdb)}\")\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\nSample original review: {X_train_imdb_text[0][:200]}...\")\n",
        "print(f\"Sample preprocessed review: {X_train_imdb_clean[0][:200]}...\")\n",
        "print(f\"Label: {y_train_imdb[0]} ({'Positive' if y_train_imdb[0] == 1 else 'Negative'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02246ce0",
      "metadata": {
        "id": "02246ce0",
        "outputId": "a73ca5b5-f80b-41a2-adec-92dddd31ad4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDb BiRNN Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create Bidirectional RNN model for IMDb\n",
        "def create_birnn_model(vocab_size, embedding_dim, max_length, rnn_units, dropout_rate):\n",
        "    \"\"\"Create a Bidirectional RNN model\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
        "        Bidirectional(LSTM(rnn_units//2, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Hyperparameters for IMDb\n",
        "vocab_size_imdb = 10000\n",
        "embedding_dim_imdb = 128\n",
        "rnn_units_imdb = 64\n",
        "dropout_rate_imdb = 0.3\n",
        "\n",
        "# Create model\n",
        "model_imdb = create_birnn_model(\n",
        "    vocab_size=vocab_size_imdb,\n",
        "    embedding_dim=embedding_dim_imdb,\n",
        "    max_length=max_length_imdb,\n",
        "    rnn_units=rnn_units_imdb,\n",
        "    dropout_rate=dropout_rate_imdb\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model_imdb.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"IMDb BiRNN Model Architecture:\")\n",
        "model_imdb.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ea8aab31",
      "metadata": {
        "id": "ea8aab31",
        "outputId": "81ad35cc-0239-42b9-ffb3-2666d2807831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training IMDb BiRNN model...\n",
            "Epoch 1/20\n",
            "\u001b[1m 79/157\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5:03\u001b[0m 4s/step - accuracy: 0.5500 - loss: 0.6671"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1294572440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history_imdb = model_imdb.fit(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_train_imdb_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_imdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training IMDb model\n",
        "print(\"Training IMDb BiRNN model...\")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Train model\n",
        "history_imdb = model_imdb.fit(\n",
        "    X_train_imdb_pad, y_train_imdb,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history_imdb, \"IMDb BiRNN\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating IMDb model on test set...\")\n",
        "test_loss, test_accuracy = model_imdb.evaluate(X_test_imdb_pad, y_test_imdb, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predictions for detailed evaluation\n",
        "y_pred_proba_imdb = model_imdb.predict(X_test_imdb_pad).flatten()\n",
        "results_imdb = evaluate_model(model_imdb, X_test_imdb_pad, y_test_imdb, y_pred_proba_imdb, \"IMDb\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plot_roc_curve(y_test_imdb, y_pred_proba_imdb, \"IMDb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174750c8",
      "metadata": {
        "id": "174750c8"
      },
      "source": [
        "## Dataset 2: ReviewTokoBaju.csv\n",
        "\n",
        "This dataset contains clothing reviews with ratings. We'll convert the ratings to binary sentiment labels for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbee27e",
      "metadata": {
        "id": "7dbee27e"
      },
      "outputs": [],
      "source": [
        "# Load ReviewTokoBaju dataset\n",
        "print(\"Loading ReviewTokoBaju dataset...\")\n",
        "df_review = pd.read_csv(r\"https://raw.githubusercontent.com/Pandu98-pkh/DeepLearning/refs/heads/main/05.%20Week%205/Dataset/ReviewTokoBaju.csv\")\n",
        "\n",
        "print(f\"Dataset shape: {df_review.shape}\")\n",
        "print(f\"Columns: {df_review.columns.tolist()}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_review.head())\n",
        "\n",
        "# Check rating distribution\n",
        "print(f\"\\nRating distribution:\")\n",
        "print(df_review['Rating'].value_counts().sort_index())\n",
        "\n",
        "# Data preprocessing\n",
        "# Combine Title and Review Text for more context\n",
        "df_review['Combined_Text'] = df_review['Title'].fillna('') + ' ' + df_review['Review Text'].fillna('')\n",
        "df_review['Combined_Text'] = df_review['Combined_Text'].str.strip()\n",
        "\n",
        "# Remove rows with empty text\n",
        "df_review = df_review[df_review['Combined_Text'].str.len() > 0].copy()\n",
        "\n",
        "# Convert ratings to binary sentiment (1-3: negative, 4-5: positive)\n",
        "df_review['Sentiment'] = (df_review['Rating'] >= 4).astype(int)\n",
        "\n",
        "print(f\"\\nAfter preprocessing:\")\n",
        "print(f\"Dataset shape: {df_review.shape}\")\n",
        "print(f\"Sentiment distribution: {df_review['Sentiment'].value_counts()}\")\n",
        "\n",
        "# Preprocess text\n",
        "print(\"Preprocessing text data...\")\n",
        "df_review['Clean_Text'] = df_review['Combined_Text'].apply(preprocess_text)\n",
        "\n",
        "# Remove rows with very short text after preprocessing\n",
        "df_review = df_review[df_review['Clean_Text'].str.len() >= 10].copy()\n",
        "\n",
        "print(f\"Final dataset shape: {df_review.shape}\")\n",
        "\n",
        "# Split data\n",
        "X_review = df_review['Clean_Text'].values\n",
        "y_review = df_review['Sentiment'].values\n",
        "\n",
        "X_train_review, X_test_review, y_train_review, y_test_review = train_test_split(\n",
        "    X_review, y_review, test_size=0.2, random_state=42, stratify=y_review\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train_review.shape}\")\n",
        "print(f\"Test set: {X_test_review.shape}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train_review)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test_review)}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nSample review: {X_review[0][:200]}...\")\n",
        "print(f\"Sample clean text: {df_review['Clean_Text'].iloc[0][:200]}...\")\n",
        "print(f\"Sentiment: {y_review[0]} ({'Positive' if y_review[0] == 1 else 'Negative'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46fc4b3e",
      "metadata": {
        "id": "46fc4b3e"
      },
      "outputs": [],
      "source": [
        "# Tokenization and padding for ReviewTokoBaju\n",
        "tokenizer_review = Tokenizer(num_words=15000, oov_token='<OOV>')\n",
        "tokenizer_review.fit_on_texts(X_train_review)\n",
        "\n",
        "X_train_review_seq = tokenizer_review.texts_to_sequences(X_train_review)\n",
        "X_test_review_seq = tokenizer_review.texts_to_sequences(X_test_review)\n",
        "\n",
        "# Padding sequences\n",
        "max_length_review = 300\n",
        "X_train_review_pad = pad_sequences(X_train_review_seq, maxlen=max_length_review, padding='post', truncating='post')\n",
        "X_test_review_pad = pad_sequences(X_test_review_seq, maxlen=max_length_review, padding='post', truncating='post')\n",
        "\n",
        "print(f\"ReviewTokoBaju sequences shape:\")\n",
        "print(f\"Training set: {X_train_review_pad.shape}\")\n",
        "print(f\"Test set: {X_test_review_pad.shape}\")\n",
        "\n",
        "# Hyperparameters for ReviewTokoBaju\n",
        "vocab_size_review = 15000\n",
        "embedding_dim_review = 100\n",
        "rnn_units_review = 80\n",
        "dropout_rate_review = 0.4\n",
        "\n",
        "# Create model\n",
        "model_review = create_birnn_model(\n",
        "    vocab_size=vocab_size_review,\n",
        "    embedding_dim=embedding_dim_review,\n",
        "    max_length=max_length_review,\n",
        "    rnn_units=rnn_units_review,\n",
        "    dropout_rate=dropout_rate_review\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model_review.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"ReviewTokoBaju BiRNN Model Architecture:\")\n",
        "model_review.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f9a65d",
      "metadata": {
        "id": "52f9a65d"
      },
      "outputs": [],
      "source": [
        "# Training ReviewTokoBaju model\n",
        "print(\"Training ReviewTokoBaju BiRNN model...\")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping_review = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "reduce_lr_review = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Train model\n",
        "history_review = model_review.fit(\n",
        "    X_train_review_pad, y_train_review,\n",
        "    validation_split=0.2,\n",
        "    epochs=25,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping_review, reduce_lr_review],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history_review, \"ReviewTokoBaju BiRNN\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating ReviewTokoBaju model on test set...\")\n",
        "test_loss_review, test_accuracy_review = model_review.evaluate(X_test_review_pad, y_test_review, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy_review:.4f}\")\n",
        "print(f\"Test Loss: {test_loss_review:.4f}\")\n",
        "\n",
        "# Predictions for detailed evaluation\n",
        "y_pred_proba_review = model_review.predict(X_test_review_pad).flatten()\n",
        "results_review = evaluate_model(model_review, X_test_review_pad, y_test_review, y_pred_proba_review, \"ReviewTokoBaju\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plot_roc_curve(y_test_review, y_pred_proba_review, \"ReviewTokoBaju\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa86472",
      "metadata": {
        "id": "0aa86472"
      },
      "source": [
        "## Dataset 3: DeteksiSarkasme.json\n",
        "\n",
        "This dataset contains news headlines with sarcasm labels. This is a challenging task as sarcasm detection requires understanding context and subtle language patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b932f970",
      "metadata": {
        "id": "b932f970"
      },
      "outputs": [],
      "source": [
        "# Load DeteksiSarkasme dataset\n",
        "print(\"Loading DeteksiSarkasme dataset...\")\n",
        "sarcasm_data = []\n",
        "\n",
        "with open(r\"https://raw.githubusercontent.com/Pandu98-pkh/DeepLearning/refs/heads/main/06.%20Week%206/Dataset/DeteksiSarkasme.json\", 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        sarcasm_data.append(json.loads(line))\n",
        "\n",
        "df_sarcasm = pd.DataFrame(sarcasm_data)\n",
        "\n",
        "print(f\"Dataset shape: {df_sarcasm.shape}\")\n",
        "print(f\"Columns: {df_sarcasm.columns.tolist()}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_sarcasm.head())\n",
        "\n",
        "# Check label distribution\n",
        "print(f\"\\nSarcasm distribution:\")\n",
        "print(df_sarcasm['is_sarcastic'].value_counts())\n",
        "\n",
        "# Data preprocessing\n",
        "print(\"Preprocessing sarcasm text data...\")\n",
        "df_sarcasm['Clean_Headline'] = df_sarcasm['headline'].apply(preprocess_text)\n",
        "\n",
        "# Remove rows with very short text after preprocessing\n",
        "df_sarcasm = df_sarcasm[df_sarcasm['Clean_Headline'].str.len() >= 5].copy()\n",
        "\n",
        "print(f\"After preprocessing:\")\n",
        "print(f\"Dataset shape: {df_sarcasm.shape}\")\n",
        "print(f\"Sarcasm distribution: {df_sarcasm['is_sarcastic'].value_counts()}\")\n",
        "\n",
        "# Split data\n",
        "X_sarcasm = df_sarcasm['Clean_Headline'].values\n",
        "y_sarcasm = df_sarcasm['is_sarcastic'].values\n",
        "\n",
        "X_train_sarcasm, X_test_sarcasm, y_train_sarcasm, y_test_sarcasm = train_test_split(\n",
        "    X_sarcasm, y_sarcasm, test_size=0.2, random_state=42, stratify=y_sarcasm\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train_sarcasm.shape}\")\n",
        "print(f\"Test set: {X_test_sarcasm.shape}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train_sarcasm)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test_sarcasm)}\")\n",
        "\n",
        "# Display samples\n",
        "print(f\"\\nSample non-sarcastic headline: {df_sarcasm[df_sarcasm['is_sarcastic']==0]['headline'].iloc[0]}\")\n",
        "print(f\"Sample sarcastic headline: {df_sarcasm[df_sarcasm['is_sarcastic']==1]['headline'].iloc[0]}\")\n",
        "print(f\"Sample clean headline: {df_sarcasm['Clean_Headline'].iloc[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eacdf7d",
      "metadata": {
        "id": "9eacdf7d"
      },
      "outputs": [],
      "source": [
        "# Tokenization and padding for DeteksiSarkasme\n",
        "tokenizer_sarcasm = Tokenizer(num_words=12000, oov_token='<OOV>')\n",
        "tokenizer_sarcasm.fit_on_texts(X_train_sarcasm)\n",
        "\n",
        "X_train_sarcasm_seq = tokenizer_sarcasm.texts_to_sequences(X_train_sarcasm)\n",
        "X_test_sarcasm_seq = tokenizer_sarcasm.texts_to_sequences(X_test_sarcasm)\n",
        "\n",
        "# Padding sequences (headlines are shorter than reviews)\n",
        "max_length_sarcasm = 100\n",
        "X_train_sarcasm_pad = pad_sequences(X_train_sarcasm_seq, maxlen=max_length_sarcasm, padding='post', truncating='post')\n",
        "X_test_sarcasm_pad = pad_sequences(X_test_sarcasm_seq, maxlen=max_length_sarcasm, padding='post', truncating='post')\n",
        "\n",
        "print(f\"DeteksiSarkasme sequences shape:\")\n",
        "print(f\"Training set: {X_train_sarcasm_pad.shape}\")\n",
        "print(f\"Test set: {X_test_sarcasm_pad.shape}\")\n",
        "\n",
        "# Hyperparameters for DeteksiSarkasme\n",
        "vocab_size_sarcasm = 12000\n",
        "embedding_dim_sarcasm = 128\n",
        "rnn_units_sarcasm = 64\n",
        "dropout_rate_sarcasm = 0.5\n",
        "\n",
        "# Create model (with slightly different architecture for sarcasm detection)\n",
        "def create_sarcasm_birnn_model(vocab_size, embedding_dim, max_length, rnn_units, dropout_rate):\n",
        "    \"\"\"Create a specialized Bidirectional RNN model for sarcasm detection\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
        "        Bidirectional(GRU(rnn_units//2, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_sarcasm = create_sarcasm_birnn_model(\n",
        "    vocab_size=vocab_size_sarcasm,\n",
        "    embedding_dim=embedding_dim_sarcasm,\n",
        "    max_length=max_length_sarcasm,\n",
        "    rnn_units=rnn_units_sarcasm,\n",
        "    dropout_rate=dropout_rate_sarcasm\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model_sarcasm.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"DeteksiSarkasme BiRNN Model Architecture:\")\n",
        "model_sarcasm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f58f9c",
      "metadata": {
        "id": "b1f58f9c"
      },
      "outputs": [],
      "source": [
        "# Training DeteksiSarkasme model\n",
        "print(\"Training DeteksiSarkasme BiRNN model...\")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping_sarcasm = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\n",
        "reduce_lr_sarcasm = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001)\n",
        "\n",
        "# Train model\n",
        "history_sarcasm = model_sarcasm.fit(\n",
        "    X_train_sarcasm_pad, y_train_sarcasm,\n",
        "    validation_split=0.2,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping_sarcasm, reduce_lr_sarcasm],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history_sarcasm, \"DeteksiSarkasme BiRNN\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating DeteksiSarkasme model on test set...\")\n",
        "test_loss_sarcasm, test_accuracy_sarcasm = model_sarcasm.evaluate(X_test_sarcasm_pad, y_test_sarcasm, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy_sarcasm:.4f}\")\n",
        "print(f\"Test Loss: {test_loss_sarcasm:.4f}\")\n",
        "\n",
        "# Predictions for detailed evaluation\n",
        "y_pred_proba_sarcasm = model_sarcasm.predict(X_test_sarcasm_pad).flatten()\n",
        "results_sarcasm = evaluate_model(model_sarcasm, X_test_sarcasm_pad, y_test_sarcasm, y_pred_proba_sarcasm, \"DeteksiSarkasme\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plot_roc_curve(y_test_sarcasm, y_pred_proba_sarcasm, \"DeteksiSarkasme\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae55104",
      "metadata": {
        "id": "3ae55104"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Let's perform hyperparameter tuning to optimize our models further. We'll use a systematic approach to test different combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d783b1",
      "metadata": {
        "id": "63d783b1"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning function\n",
        "def hyperparameter_tuning(X_train, y_train, X_val, y_val, dataset_name, max_length, vocab_size):\n",
        "    \"\"\"Perform hyperparameter tuning for BiRNN models\"\"\"\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    param_grid = {\n",
        "        'embedding_dim': [64, 128, 256],\n",
        "        'rnn_units': [32, 64, 128],\n",
        "        'dropout_rate': [0.2, 0.3, 0.5],\n",
        "        'learning_rate': [0.001, 0.01, 0.0001],\n",
        "        'batch_size': [32, 64, 128]\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "    results = []\n",
        "\n",
        "    print(f\"Starting hyperparameter tuning for {dataset_name}...\")\n",
        "    print(f\"Total combinations to test: {len(param_grid['embedding_dim']) * len(param_grid['rnn_units']) * len(param_grid['dropout_rate']) * len(param_grid['learning_rate']) * len(param_grid['batch_size'])}\")\n",
        "\n",
        "    # Random search (testing subset of combinations)\n",
        "    import random\n",
        "    random.seed(42)\n",
        "\n",
        "    # Generate random combinations\n",
        "    n_trials = 10  # Reduced for demonstration\n",
        "\n",
        "    for trial in range(n_trials):\n",
        "        # Random hyperparameter selection\n",
        "        embedding_dim = random.choice(param_grid['embedding_dim'])\n",
        "        rnn_units = random.choice(param_grid['rnn_units'])\n",
        "        dropout_rate = random.choice(param_grid['dropout_rate'])\n",
        "        learning_rate = random.choice(param_grid['learning_rate'])\n",
        "        batch_size = random.choice(param_grid['batch_size'])\n",
        "\n",
        "        print(f\"\\nTrial {trial + 1}/{n_trials}\")\n",
        "        print(f\"Params: embedding_dim={embedding_dim}, rnn_units={rnn_units}, dropout_rate={dropout_rate}, lr={learning_rate}, batch_size={batch_size}\")\n",
        "\n",
        "        try:\n",
        "            # Create model\n",
        "            model = create_birnn_model(\n",
        "                vocab_size=vocab_size,\n",
        "                embedding_dim=embedding_dim,\n",
        "                max_length=max_length,\n",
        "                rnn_units=rnn_units,\n",
        "                dropout_rate=dropout_rate\n",
        "            )\n",
        "\n",
        "            # Compile model\n",
        "            model.compile(\n",
        "                optimizer=Adam(learning_rate=learning_rate),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            # Train model (fewer epochs for tuning)\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=5,  # Reduced for faster tuning\n",
        "                batch_size=batch_size,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Get validation accuracy\n",
        "            val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'embedding_dim': embedding_dim,\n",
        "                'rnn_units': rnn_units,\n",
        "                'dropout_rate': dropout_rate,\n",
        "                'learning_rate': learning_rate,\n",
        "                'batch_size': batch_size,\n",
        "                'val_accuracy': val_accuracy\n",
        "            })\n",
        "\n",
        "            # Update best parameters\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_params = {\n",
        "                    'embedding_dim': embedding_dim,\n",
        "                    'rnn_units': rnn_units,\n",
        "                    'dropout_rate': dropout_rate,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'batch_size': batch_size\n",
        "                }\n",
        "                print(f\"New best accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "            # Clean up\n",
        "            del model\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in trial {trial + 1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n=== Hyperparameter Tuning Results for {dataset_name} ===\")\n",
        "    print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    # Display top 5 results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('val_accuracy', ascending=False)\n",
        "\n",
        "    print(f\"\\nTop 5 Results:\")\n",
        "    print(results_df.head())\n",
        "\n",
        "    return best_params, results_df\n",
        "\n",
        "# Perform hyperparameter tuning for IMDb dataset (example)\n",
        "print(\"=== HYPERPARAMETER TUNING FOR IMDB DATASET ===\")\n",
        "\n",
        "# Create validation split from training data\n",
        "X_train_imdb_tune, X_val_imdb_tune, y_train_imdb_tune, y_val_imdb_tune = train_test_split(\n",
        "    X_train_imdb_pad, y_train_imdb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "best_params_imdb, results_imdb_tune = hyperparameter_tuning(\n",
        "    X_train_imdb_tune, y_train_imdb_tune,\n",
        "    X_val_imdb_tune, y_val_imdb_tune,\n",
        "    \"IMDb\", max_length_imdb, vocab_size_imdb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec9123a",
      "metadata": {
        "id": "6ec9123a"
      },
      "outputs": [],
      "source": [
        "# Train optimized models with best hyperparameters\n",
        "def train_optimized_model(X_train, y_train, X_test, y_test, best_params, dataset_name, max_length, vocab_size):\n",
        "    \"\"\"Train final optimized model with best hyperparameters\"\"\"\n",
        "\n",
        "    print(f\"\\n=== Training Optimized {dataset_name} Model ===\")\n",
        "    print(f\"Using best parameters: {best_params}\")\n",
        "\n",
        "    # Create optimized model\n",
        "    model_optimized = create_birnn_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=best_params['embedding_dim'],\n",
        "        max_length=max_length,\n",
        "        rnn_units=best_params['rnn_units'],\n",
        "        dropout_rate=best_params['dropout_rate']\n",
        "    )\n",
        "\n",
        "    # Compile with best learning rate\n",
        "    model_optimized.compile(\n",
        "        optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "    # Train model\n",
        "    history_optimized = model_optimized.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=50,  # More epochs for final training\n",
        "        batch_size=best_params['batch_size'],\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history_optimized, f\"Optimized {dataset_name} BiRNN\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_accuracy = model_optimized.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"\\nOptimized {dataset_name} Model Results:\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Detailed evaluation\n",
        "    y_pred_proba = model_optimized.predict(X_test).flatten()\n",
        "    results = evaluate_model(model_optimized, X_test, y_test, y_pred_proba, f\"Optimized {dataset_name}\")\n",
        "\n",
        "    return model_optimized, history_optimized, results\n",
        "\n",
        "# Train optimized IMDb model\n",
        "if 'best_params_imdb' in locals() and best_params_imdb:\n",
        "    model_imdb_optimized, history_imdb_optimized, results_imdb_optimized = train_optimized_model(\n",
        "        X_train_imdb_pad, y_train_imdb, X_test_imdb_pad, y_test_imdb,\n",
        "        best_params_imdb, \"IMDb\", max_length_imdb, vocab_size_imdb\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping optimized IMDb model training - using default hyperparameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99021d3a",
      "metadata": {
        "id": "99021d3a"
      },
      "source": [
        "## Results Summary and Model Comparison\n",
        "\n",
        "Let's create a comprehensive summary of all our models' performances and compare them side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b08a767",
      "metadata": {
        "id": "2b08a767"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Results Summary\n",
        "def create_results_summary():\n",
        "    \"\"\"Create a comprehensive summary of all model results\"\"\"\n",
        "\n",
        "    # Collect all results\n",
        "    summary_data = []\n",
        "\n",
        "    # Check if results exist and add them\n",
        "    if 'results_imdb' in locals():\n",
        "        summary_data.append({\n",
        "            'Dataset': 'IMDb Movies',\n",
        "            'Model': 'BiRNN (LSTM)',\n",
        "            'Accuracy': results_imdb['accuracy'],\n",
        "            'Precision': results_imdb['precision'],\n",
        "            'Recall': results_imdb['recall'],\n",
        "            'F1-Score': results_imdb['f1_score'],\n",
        "            'AUC-ROC': results_imdb['auc_roc'],\n",
        "            'Data Size': f\"{len(X_train_imdb_pad) + len(X_test_imdb_pad):,}\",\n",
        "            'Task': 'Sentiment Analysis'\n",
        "        })\n",
        "\n",
        "    if 'results_review' in locals():\n",
        "        summary_data.append({\n",
        "            'Dataset': 'ReviewTokoBaju',\n",
        "            'Model': 'BiRNN (LSTM)',\n",
        "            'Accuracy': results_review['accuracy'],\n",
        "            'Precision': results_review['precision'],\n",
        "            'Recall': results_review['recall'],\n",
        "            'F1-Score': results_review['f1_score'],\n",
        "            'AUC-ROC': results_review['auc_roc'],\n",
        "            'Data Size': f\"{len(X_train_review_pad) + len(X_test_review_pad):,}\",\n",
        "            'Task': 'Review Sentiment'\n",
        "        })\n",
        "\n",
        "    if 'results_sarcasm' in locals():\n",
        "        summary_data.append({\n",
        "            'Dataset': 'DeteksiSarkasme',\n",
        "            'Model': 'BiRNN (LSTM+GRU)',\n",
        "            'Accuracy': results_sarcasm['accuracy'],\n",
        "            'Precision': results_sarcasm['precision'],\n",
        "            'Recall': results_sarcasm['recall'],\n",
        "            'F1-Score': results_sarcasm['f1_score'],\n",
        "            'AUC-ROC': results_sarcasm['auc_roc'],\n",
        "            'Data Size': f\"{len(X_train_sarcasm_pad) + len(X_test_sarcasm_pad):,}\",\n",
        "            'Task': 'Sarcasm Detection'\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "        print(\"=== COMPREHENSIVE RESULTS SUMMARY ===\")\n",
        "        print(summary_df.round(4))\n",
        "\n",
        "        # Check 90% accuracy requirement\n",
        "        print(\"\\n=== ACCURACY REQUIREMENT CHECK (≥90%) ===\")\n",
        "        for _, row in summary_df.iterrows():\n",
        "            status = \"✅ PASSED\" if row['Accuracy'] >= 0.90 else \"❌ NEEDS IMPROVEMENT\"\n",
        "            print(f\"{row['Dataset']}: {row['Accuracy']:.4f} ({row['Accuracy']*100:.2f}%) - {status}\")\n",
        "\n",
        "        # Visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Accuracy comparison\n",
        "        axes[0,0].bar(summary_df['Dataset'], summary_df['Accuracy'], color=['skyblue', 'lightgreen', 'salmon'])\n",
        "        axes[0,0].set_title('Model Accuracy Comparison')\n",
        "        axes[0,0].set_ylabel('Accuracy')\n",
        "        axes[0,0].axhline(y=0.90, color='red', linestyle='--', label='90% Target')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # F1-Score comparison\n",
        "        axes[0,1].bar(summary_df['Dataset'], summary_df['F1-Score'], color=['orange', 'purple', 'brown'])\n",
        "        axes[0,1].set_title('F1-Score Comparison')\n",
        "        axes[0,1].set_ylabel('F1-Score')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # AUC-ROC comparison\n",
        "        axes[1,0].bar(summary_df['Dataset'], summary_df['AUC-ROC'], color=['gold', 'teal', 'pink'])\n",
        "        axes[1,0].set_title('AUC-ROC Comparison')\n",
        "        axes[1,0].set_ylabel('AUC-ROC')\n",
        "        axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Comprehensive metrics heatmap\n",
        "        metrics_data = summary_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']].values\n",
        "        sns.heatmap(metrics_data,\n",
        "                   xticklabels=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],\n",
        "                   yticklabels=summary_df['Dataset'],\n",
        "                   annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1,1])\n",
        "        axes[1,1].set_title('All Metrics Heatmap')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return summary_df\n",
        "    else:\n",
        "        print(\"No results available for summary\")\n",
        "        return None\n",
        "\n",
        "# Create results summary\n",
        "summary_df = create_results_summary()\n",
        "\n",
        "# Model Architecture Comparison\n",
        "print(\"\\n=== MODEL ARCHITECTURE COMPARISON ===\")\n",
        "architecture_info = [\n",
        "    {\n",
        "        'Dataset': 'IMDb',\n",
        "        'Architecture': 'Embedding → BiLSTM → BiLSTM → Dense → Output',\n",
        "        'Sequence Length': max_length_imdb,\n",
        "        'Vocab Size': vocab_size_imdb,\n",
        "        'Parameters': '~2M (estimated)'\n",
        "    },\n",
        "    {\n",
        "        'Dataset': 'ReviewTokoBaju',\n",
        "        'Architecture': 'Embedding → BiLSTM → BiLSTM → Dense → Output',\n",
        "        'Sequence Length': max_length_review,\n",
        "        'Vocab Size': vocab_size_review,\n",
        "        'Parameters': '~2.5M (estimated)'\n",
        "    },\n",
        "    {\n",
        "        'Dataset': 'DeteksiSarkasme',\n",
        "        'Architecture': 'Embedding → BiLSTM → BiGRU → GlobalMaxPool → Dense → Output',\n",
        "        'Sequence Length': max_length_sarcasm,\n",
        "        'Vocab Size': vocab_size_sarcasm,\n",
        "        'Parameters': '~1.8M (estimated)'\n",
        "    }\n",
        "]\n",
        "\n",
        "arch_df = pd.DataFrame(architecture_info)\n",
        "print(arch_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7f27fa",
      "metadata": {
        "id": "fc7f27fa"
      },
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "def save_models():\n",
        "    \"\"\"Save all trained models\"\"\"\n",
        "    print(\"Saving trained models...\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    import os\n",
        "    save_dir = \"saved_models\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save models\n",
        "    if 'model_imdb' in locals():\n",
        "        model_imdb.save(f\"{save_dir}/imdb_birnn_model.keras\")\n",
        "        print(\"✅ IMDb BiRNN model saved\")\n",
        "\n",
        "    if 'model_review' in locals():\n",
        "        model_review.save(f\"{save_dir}/review_birnn_model.keras\")\n",
        "        print(\"✅ ReviewTokoBaju BiRNN model saved\")\n",
        "\n",
        "    if 'model_sarcasm' in locals():\n",
        "        model_sarcasm.save(f\"{save_dir}/sarcasm_birnn_model.keras\")\n",
        "        print(\"✅ DeteksiSarkasme BiRNN model saved\")\n",
        "\n",
        "    # Save tokenizers\n",
        "    import pickle\n",
        "\n",
        "    if 'tokenizer_imdb' in locals():\n",
        "        with open(f\"{save_dir}/imdb_tokenizer.pkl\", 'wb') as f:\n",
        "            pickle.dump(tokenizer_imdb, f)\n",
        "        print(\"✅ IMDb tokenizer saved\")\n",
        "\n",
        "    if 'tokenizer_review' in locals():\n",
        "        with open(f\"{save_dir}/review_tokenizer.pkl\", 'wb') as f:\n",
        "            pickle.dump(tokenizer_review, f)\n",
        "        print(\"✅ ReviewTokoBaju tokenizer saved\")\n",
        "\n",
        "    if 'tokenizer_sarcasm' in locals():\n",
        "        with open(f\"{save_dir}/sarcasm_tokenizer.pkl\", 'wb') as f:\n",
        "            pickle.dump(tokenizer_sarcasm, f)\n",
        "        print(\"✅ DeteksiSarkasme tokenizer saved\")\n",
        "\n",
        "# Save all models\n",
        "save_models()\n",
        "\n",
        "# Final performance summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "performance_summary = \"\"\"\n",
        "🎯 PROJECT OBJECTIVES STATUS:\n",
        "\n",
        "1. ✅ Bidirectional RNN Models Created:\n",
        "   - IMDb: BiLSTM architecture for movie review sentiment analysis\n",
        "   - ReviewTokoBaju: BiLSTM architecture for clothing review sentiment\n",
        "   - DeteksiSarkasme: BiLSTM+BiGRU hybrid for sarcasm detection\n",
        "\n",
        "2. ✅ Comprehensive Evaluation Metrics Implemented:\n",
        "   - Accuracy, Precision, Recall, F1-Score, AUC-ROC\n",
        "   - Confusion matrices and ROC curves\n",
        "   - Training/validation loss and accuracy visualization\n",
        "\n",
        "3. ✅ Hyperparameter Tuning:\n",
        "   - Systematic search across embedding dimensions, RNN units, dropout rates\n",
        "   - Learning rate and batch size optimization\n",
        "   - Early stopping and learning rate reduction callbacks\n",
        "\n",
        "4. 🎯 Accuracy Target (≥90%):\n",
        "   - Models trained with early stopping and regularization\n",
        "   - Performance varies by dataset complexity\n",
        "   - Sarcasm detection being most challenging task\n",
        "\n",
        "5. ✅ TensorFlow/Keras Implementation:\n",
        "   - Professional-grade code with proper preprocessing\n",
        "   - GPU/TPU compatible architecture\n",
        "   - Modular and reusable functions\n",
        "\n",
        "📊 KEY INSIGHTS:\n",
        "- Bidirectional RNNs effectively capture context from both directions\n",
        "- Proper text preprocessing significantly impacts performance\n",
        "- Hyperparameter tuning is crucial for optimal results\n",
        "- Different datasets require different architectural approaches\n",
        "- Sarcasm detection remains a challenging NLP task\n",
        "\n",
        "🚀 RECOMMENDATIONS:\n",
        "- Use Google Colab with T4 GPU for faster training\n",
        "- Consider ensemble methods for better accuracy\n",
        "- Implement attention mechanisms for improved performance\n",
        "- Try BERT or other transformer models for comparison\n",
        "\"\"\"\n",
        "\n",
        "print(performance_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a351484",
      "metadata": {
        "id": "0a351484"
      },
      "source": [
        "## Google Colab Setup Instructions\n",
        "\n",
        "For optimal performance using Google Colab with T4 GPU or TPU, follow these steps:\n",
        "\n",
        "### 1. Enable GPU/TPU Runtime\n",
        "```python\n",
        "# In Google Colab, go to Runtime → Change runtime type → Select GPU (T4) or TPU\n",
        "# Then run this cell to verify:\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"TPU Available: \", tf.config.list_physical_devices('TPU'))\n",
        "```\n",
        "\n",
        "### 2. Mount Google Drive (if datasets are stored there)\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "### 3. Install Additional Packages (if needed)\n",
        "```python\n",
        "!pip install -q seaborn matplotlib nltk\n",
        "```\n",
        "\n",
        "### 4. Memory Management for Large Models\n",
        "```python\n",
        "# Clear memory between model training sessions\n",
        "import gc\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "```\n",
        "\n",
        "### 5. Download Datasets\n",
        "```python\n",
        "# For IMDb - already available in Keras\n",
        "# For other datasets, upload to Colab or download from URLs\n",
        "!wget \"your_dataset_url\" -O dataset.csv\n",
        "```\n",
        "\n",
        "**Note**: This notebook is designed to work both locally and in Google Colab. Adjust file paths accordingly."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}