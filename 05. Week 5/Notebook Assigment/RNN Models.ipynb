{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1901c487",
   "metadata": {},
   "source": [
    "# RNN Models for ReviewTokoBaju Dataset\n",
    "\n",
    "## Objective\n",
    "Membuat model Deep Learning menggunakan RNN (Recurrent Neural Network) untuk klasifikasi review toko baju dengan:\n",
    "1. **PyTorch dan TensorFlow implementation**\n",
    "2. **Evaluasi model dengan berbagai metrik** (Accuracy, Precision, Recall, F1-Score, AUC, ROC)\n",
    "3. **Visualisasi matrix confusion dan loss**\n",
    "4. **Penjelasan persamaan matematika**\n",
    "5. **Target akurasi minimal 70%** pada training dan testing set\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### RNN (Recurrent Neural Network)\n",
    "RNN adalah arsitektur neural network yang dirancang untuk memproses data sequential. Persamaan dasar RNN:\n",
    "\n",
    "**Hidden State Update:**\n",
    "```\n",
    "h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "\n",
    "Dimana:\n",
    "- `h_t`: hidden state pada waktu t\n",
    "- `x_t`: input pada waktu t  \n",
    "- `W_hh`: weight matrix untuk hidden-to-hidden connections\n",
    "- `W_xh`: weight matrix untuk input-to-hidden connections\n",
    "- `W_hy`: weight matrix untuk hidden-to-output connections\n",
    "- `b_h`, `b_y`: bias terms\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "LSTM mengatasi masalah vanishing gradient pada RNN vanilla dengan menggunakan gates:\n",
    "\n",
    "**Forget Gate:**\n",
    "```\n",
    "f_t = œÉ(W_f * [h_{t-1}, x_t] + b_f)\n",
    "```\n",
    "\n",
    "**Input Gate:**\n",
    "```\n",
    "i_t = œÉ(W_i * [h_{t-1}, x_t] + b_i)\n",
    "CÃÉ_t = tanh(W_C * [h_{t-1}, x_t] + b_C)\n",
    "```\n",
    "\n",
    "**Cell State Update:**\n",
    "```\n",
    "C_t = f_t * C_{t-1} + i_t * CÃÉ_t\n",
    "```\n",
    "\n",
    "**Output Gate:**\n",
    "```\n",
    "o_t = œÉ(W_o * [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t * tanh(C_t)\n",
    "```\n",
    "\n",
    "### GRU (Gated Recurrent Unit)\n",
    "GRU adalah versi simplified dari LSTM:\n",
    "\n",
    "**Reset Gate:**\n",
    "```\n",
    "r_t = œÉ(W_r * [h_{t-1}, x_t] + b_r)\n",
    "```\n",
    "\n",
    "**Update Gate:**\n",
    "```\n",
    "z_t = œÉ(W_z * [h_{t-1}, x_t] + b_z)\n",
    "```\n",
    "\n",
    "**New Hidden State:**\n",
    "```\n",
    "hÃÉ_t = tanh(W_h * [r_t * h_{t-1}, x_t] + b_h)\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * hÃÉ_t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# For text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    print(\"NLTK download failed, using basic preprocessing\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "print(f\"TensorFlow GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Download from GitHub repository\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"Download ReviewTokoBaju.csv from GitHub repository\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/farrelrassya/teachingMLDL/main/02.%20Deep%20Learning/04.%20Week%204/Dataset/ReviewTokoBaju.csv\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Read CSV from response content\n",
    "        df = pd.read_csv(io.StringIO(response.text))\n",
    "        print(\"‚úÖ Dataset downloaded successfully!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "        # Create sample data if download fails\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        sample_data = {\n",
    "            'review': [\n",
    "                'Produk bagus sekali, kualitas terjamin',\n",
    "                'Barang jelek, tidak sesuai ekspektasi',\n",
    "                'Pelayanan memuaskan, akan beli lagi',\n",
    "                'Pengiriman lambat, produk biasa saja',\n",
    "                'Sangat puas dengan pembelian ini',\n",
    "                'Kecewa dengan kualitas produk',\n",
    "                'Recommended seller, barang original',\n",
    "                'Tidak recommended, barang rusak'\n",
    "            ],\n",
    "            'sentiment': ['positive', 'negative', 'positive', 'negative', \n",
    "                         'positive', 'negative', 'positive', 'negative']\n",
    "        }\n",
    "        return pd.DataFrame(sample_data)\n",
    "\n",
    "# Load the dataset\n",
    "df = download_dataset()\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"üîç Exploratory Data Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìà Dataset Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üìù Missing Values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Check data types and unique values\n",
    "for col in df.columns:\n",
    "    print(f\"\\nüî¢ Column '{col}':\")\n",
    "    print(f\"   Data type: {df[col].dtype}\")\n",
    "    print(f\"   Unique values: {df[col].nunique()}\")\n",
    "    if df[col].nunique() < 20:\n",
    "        print(f\"   Values: {df[col].unique()}\")\n",
    "\n",
    "# Distribution of target variable\n",
    "print(\"\\nüéØ Target Distribution:\")\n",
    "if 'sentiment' in df.columns:\n",
    "    target_col = 'sentiment'\n",
    "elif 'label' in df.columns:\n",
    "    target_col = 'label'\n",
    "elif 'rating' in df.columns:\n",
    "    target_col = 'rating'\n",
    "else:\n",
    "    # Try to find target column\n",
    "    target_col = df.columns[-1]  # Assume last column is target\n",
    "    \n",
    "print(f\"Target column: {target_col}\")\n",
    "print(df[target_col].value_counts())\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df[target_col].value_counts().plot(kind='bar', ax=plt.gca())\n",
    "plt.title(f'Distribution of {target_col}')\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=plt.gca())\n",
    "plt.title(f'Proportion of {target_col}')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "text_col = [col for col in df.columns if col != target_col][0]\n",
    "print(f\"\\nüìù Text Analysis (Column: {text_col}):\")\n",
    "df['text_length'] = df[text_col].str.len()\n",
    "df['word_count'] = df[text_col].str.split().str.len()\n",
    "\n",
    "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
    "print(f\"Max text length: {df['text_length'].max()} characters\")\n",
    "print(f\"Max word count: {df['word_count'].max()} words\")\n",
    "\n",
    "# Visualize text statistics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['text_length'], bins=30, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Text Length')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['word_count'], bins=30, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df.boxplot(column='word_count', by=target_col, ax=plt.gca())\n",
    "plt.title('Word Count by Target')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ EDA completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2333e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "print(\"üîß Text Preprocessing\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing class\"\"\"\n",
    "    \n",
    "    def __init__(self, language='indonesian'):\n",
    "        self.language = language\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Indonesian stopwords (basic set)\n",
    "        self.indonesian_stopwords = {\n",
    "            'yang', 'ini', 'itu', 'adalah', 'ada', 'dan', 'atau', 'dengan', 'untuk',\n",
    "            'dalam', 'pada', 'dari', 'ke', 'di', 'akan', 'telah', 'sudah', 'tidak',\n",
    "            'belum', 'juga', 'dapat', 'bisa', 'harus', 'saya', 'anda', 'dia', 'kita',\n",
    "            'mereka', 'nya', 'mu', 'ku', 'lah', 'kah', 'pun', 'nya'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Try to get English stopwords\n",
    "            self.english_stopwords = set(stopwords.words('english'))\n",
    "        except:\n",
    "            self.english_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except:\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove both Indonesian and English stopwords\n",
    "        filtered_tokens = [\n",
    "            word for word in tokens \n",
    "            if word not in self.indonesian_stopwords and word not in self.english_stopwords\n",
    "            and len(word) > 2  # Remove very short words\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(filtered_tokens)\n",
    "    \n",
    "    def stem_text(self, text):\n",
    "        \"\"\"Apply stemming\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except:\n",
    "            tokens = text.split()\n",
    "        \n",
    "        stemmed_tokens = [self.stemmer.stem(word) for word in tokens]\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    \n",
    "    def preprocess_text(self, text, remove_stopwords=True, apply_stemming=False):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            text = self.remove_stopwords(text)\n",
    "        \n",
    "        if apply_stemming:\n",
    "            text = self.stem_text(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"üîÑ Applying text preprocessing...\")\n",
    "df['processed_text'] = df[text_col].apply(\n",
    "    lambda x: preprocessor.preprocess_text(x, remove_stopwords=True, apply_stemming=False)\n",
    ")\n",
    "\n",
    "# Remove empty texts after preprocessing\n",
    "df = df[df['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing completed!\")\n",
    "print(f\"üìä Data shape after preprocessing: {df.shape}\")\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"\\nüìù Preprocessing Examples:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Original: {df[text_col].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Update text statistics\n",
    "df['processed_length'] = df['processed_text'].str.len()\n",
    "df['processed_word_count'] = df['processed_text'].str.split().str.len()\n",
    "\n",
    "print(f\"\\nüìà After preprocessing:\")\n",
    "print(f\"Average text length: {df['processed_length'].mean():.2f} characters\")\n",
    "print(f\"Average word count: {df['processed_word_count'].mean():.2f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Encoding\n",
    "print(\"üìã Data Preparation and Encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_target'] = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "print(f\"üéØ Target encoding:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = sum(df['encoded_target'] == i)\n",
    "    print(f\"   {class_name} -> {i} (count: {count})\")\n",
    "\n",
    "# Split data\n",
    "X = df['processed_text'].values\n",
    "y = df['encoded_target'].values\n",
    "\n",
    "print(f\"\\nüìä Data split:\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Create vocabulary and tokenization\n",
    "class Vocabulary:\n",
    "    \"\"\"Custom vocabulary class for text tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_count = {}\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "        \n",
    "        # Sort by frequency and take top words\n",
    "        sorted_words = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for word, count in sorted_words[:self.max_vocab_size-2]:  # -2 for PAD and UNK\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        print(f\"‚úÖ Vocabulary built with {len(self.word2idx)} words\")\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of indices\"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = [\n",
    "                self.word2idx.get(word, self.word2idx['<UNK>']) \n",
    "                for word in text.split()\n",
    "            ]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "    def pad_sequences(self, sequences, max_length=None):\n",
    "        \"\"\"Pad sequences to same length\"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = max(len(seq) for seq in sequences)\n",
    "        \n",
    "        padded = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) >= max_length:\n",
    "                padded.append(seq[:max_length])\n",
    "            else:\n",
    "                padded.append(seq + [0] * (max_length - len(seq)))\n",
    "        \n",
    "        return np.array(padded), max_length\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(max_vocab_size=5000)\n",
    "vocab.build_vocab(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_sequences = vocab.texts_to_sequences(X_train)\n",
    "val_sequences = vocab.texts_to_sequences(X_val)\n",
    "test_sequences = vocab.texts_to_sequences(X_test)\n",
    "\n",
    "# Determine sequence length\n",
    "sequence_lengths = [len(seq) for seq in train_sequences]\n",
    "max_length = min(100, int(np.percentile(sequence_lengths, 95)))  # Use 95th percentile, max 100\n",
    "print(f\"üìè Max sequence length: {max_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad, _ = vocab.pad_sequences(train_sequences, max_length)\n",
    "X_val_pad, _ = vocab.pad_sequences(val_sequences, max_length)\n",
    "X_test_pad, _ = vocab.pad_sequences(test_sequences, max_length)\n",
    "\n",
    "print(f\"‚úÖ Sequences padded to length {max_length}\")\n",
    "print(f\"üìä Final data shapes:\")\n",
    "print(f\"   X_train: {X_train_pad.shape}\")\n",
    "print(f\"   X_val: {X_val_pad.shape}\")\n",
    "print(f\"   X_test: {X_test_pad.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   y_val: {y_val.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "\n",
    "# Store important variables\n",
    "vocab_size = len(vocab.word2idx)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"üìö Vocabulary size: {vocab_size}\")\n",
    "print(f\"üéØ Number of classes: {num_classes}\")\n",
    "\n",
    "# Show sequence example\n",
    "print(f\"\\nüìù Example sequence:\")\n",
    "print(f\"Original: {X_train[0]}\")\n",
    "print(f\"Sequence: {train_sequences[0][:10]}...\")  # Show first 10 tokens\n",
    "print(f\"Padded shape: {X_train_pad[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch RNN Models Implementation\n",
    "print(\"üî• PyTorch RNN Models\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(X_train_pad, y_train)\n",
    "val_dataset = TextDataset(X_val_pad, y_val)\n",
    "test_dataset = TextDataset(X_test_pad, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ PyTorch datasets created\")\n",
    "print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "# Define RNN Models\n",
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"Vanilla RNN for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.5):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, hidden = self.rnn(embedded)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Use last output\n",
    "        last_output = rnn_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Dropout and classification\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.fc(output)  # (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"LSTM for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Use last output\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Dropout and classification\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.fc(output)  # (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"GRU for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.5):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # GRU\n",
    "        gru_out, hidden = self.gru(embedded)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Use last output\n",
    "        last_output = gru_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Dropout and classification\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.fc(output)  # (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(f\"üèóÔ∏è Model parameters:\")\n",
    "print(f\"   Embedding dimension: {embed_dim}\")\n",
    "print(f\"   Hidden dimension: {hidden_dim}\")\n",
    "print(f\"   Number of layers: {num_layers}\")\n",
    "print(f\"   Dropout: {dropout}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RNN': RNNClassifier(vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout),\n",
    "    'LSTM': LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout),\n",
    "    'GRU': GRUClassifier(vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout)\n",
    "}\n",
    "\n",
    "# Move models to device\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    print(f\"‚úÖ {name} model created and moved to {device}\")\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\nüèóÔ∏è LSTM Model Architecture:\")\n",
    "print(models['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Training and Evaluation Functions\n",
    "print(\"üöÄ PyTorch Training Functions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001):\n",
    "    \"\"\"Train PyTorch model with validation\"\"\"\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"üèÉ‚Äç‚ôÇÔ∏è Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "            print(f\"  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "            print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_accuracy': train_accuracies,\n",
    "        'val_loss': val_losses,\n",
    "        'val_accuracy': val_accuracies\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    \"\"\"Evaluate PyTorch model and return detailed metrics\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Get probabilities\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    # AUC (for binary classification or macro average for multiclass)\n",
    "    try:\n",
    "        if num_classes == 2:\n",
    "            auc = roc_auc_score(all_targets, all_probabilities[:, 1])\n",
    "        else:\n",
    "            auc = roc_auc_score(all_targets, all_probabilities, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Validation Loss', marker='s')\n",
    "    axes[0].set_title(f'{model_name} - Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(history['train_accuracy'], label='Training Accuracy', marker='o')\n",
    "    axes[1].plot(history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    axes[1].set_title(f'{model_name} - Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "print(\"‚úÖ Training and evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyTorch Models\n",
    "print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Training PyTorch Models\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Store results\n",
    "pytorch_results = {}\n",
    "pytorch_histories = {}\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüî• Training {model_name} Model\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_pytorch_model(\n",
    "        model, train_loader, val_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = evaluate_pytorch_model(trained_model, test_loader)\n",
    "    \n",
    "    # Store results\n",
    "    pytorch_results[model_name] = test_metrics\n",
    "    pytorch_histories[model_name] = history\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä {model_name} Test Results:\")\n",
    "    print(f\"   Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "    print(f\"   AUC: {test_metrics['auc']:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, f\"PyTorch {model_name}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    class_names = label_encoder.classes_\n",
    "    plot_confusion_matrix(\n",
    "        test_metrics['targets'], \n",
    "        test_metrics['predictions'], \n",
    "        f\"PyTorch {model_name}\",\n",
    "        class_names\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} training and evaluation completed!\")\n",
    "\n",
    "# Summary of PyTorch results\n",
    "print(f\"\\nüìã PyTorch Models Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, metrics in pytorch_results.items():\n",
    "    print(f\"{model_name:<10} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} \"\n",
    "          f\"{metrics['recall']:<10.4f} {metrics['f1_score']:<10.4f} {metrics['auc']:<10.4f}\")\n",
    "\n",
    "# Find best PyTorch model\n",
    "best_pytorch_model = max(pytorch_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\nüèÜ Best PyTorch Model: {best_pytorch_model[0]} with accuracy: {best_pytorch_model[1]['accuracy']:.4f}\")\n",
    "\n",
    "# Check if accuracy meets requirement (>= 70%)\n",
    "pytorch_meets_requirement = best_pytorch_model[1]['accuracy'] >= 0.70\n",
    "print(f\"‚úÖ Meets 70% accuracy requirement: {pytorch_meets_requirement}\")\n",
    "\n",
    "if not pytorch_meets_requirement:\n",
    "    print(\"‚ö†Ô∏è  Warning: Best model accuracy is below 70%. Consider:\")\n",
    "    print(\"   - Increasing training epochs\")\n",
    "    print(\"   - Adjusting learning rate\")\n",
    "    print(\"   - Adding more data\")\n",
    "    print(\"   - Tuning hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f56fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow/Keras RNN Models Implementation\n",
    "print(\"üåü TensorFlow RNN Models\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data for TensorFlow\n",
    "X_train_tf = X_train_pad\n",
    "X_val_tf = X_val_pad\n",
    "X_test_tf = X_test_pad\n",
    "\n",
    "# One-hot encode labels for TensorFlow (if multiclass)\n",
    "if num_classes > 2:\n",
    "    y_train_tf = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_tf = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "    y_test_tf = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    loss_function = 'categorical_crossentropy'\n",
    "    metrics = ['accuracy']\n",
    "else:\n",
    "    y_train_tf = y_train\n",
    "    y_val_tf = y_val\n",
    "    y_test_tf = y_test\n",
    "    loss_function = 'sparse_categorical_crossentropy'\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "print(f\"‚úÖ TensorFlow data prepared\")\n",
    "print(f\"üìä Input shape: {X_train_tf.shape}\")\n",
    "print(f\"üéØ Output shape: {y_train_tf.shape if hasattr(y_train_tf, 'shape') else len(y_train_tf)}\")\n",
    "\n",
    "# Define TensorFlow model creation functions\n",
    "def create_rnn_model(vocab_size, embed_dim, hidden_dim, num_classes, max_length):\n",
    "    \"\"\"Create vanilla RNN model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "        Dropout(0.2),\n",
    "        tf.keras.layers.SimpleRNN(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        tf.keras.layers.SimpleRNN(hidden_dim//2, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dropout(0.5),\n",
    "        Dense(hidden_dim//4, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model(vocab_size, embed_dim, hidden_dim, num_classes, max_length):\n",
    "    \"\"\"Create LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        LSTM(hidden_dim//2, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dropout(0.5),\n",
    "        Dense(hidden_dim//4, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gru_model(vocab_size, embed_dim, hidden_dim, num_classes, max_length):\n",
    "    \"\"\"Create GRU model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        GRU(hidden_dim//2, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dropout(0.5),\n",
    "        Dense(hidden_dim//4, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm_model(vocab_size, embed_dim, hidden_dim, num_classes, max_length):\n",
    "    \"\"\"Create Bidirectional LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(hidden_dim//2, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        Bidirectional(LSTM(hidden_dim//4, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        Dropout(0.5),\n",
    "        Dense(hidden_dim//4, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model parameters for TensorFlow\n",
    "tf_embed_dim = 128\n",
    "tf_hidden_dim = 256\n",
    "\n",
    "# Create TensorFlow models\n",
    "tf_models = {\n",
    "    'RNN': create_rnn_model(vocab_size, tf_embed_dim, tf_hidden_dim, num_classes, max_length),\n",
    "    'LSTM': create_lstm_model(vocab_size, tf_embed_dim, tf_hidden_dim, num_classes, max_length),\n",
    "    'GRU': create_gru_model(vocab_size, tf_embed_dim, tf_hidden_dim, num_classes, max_length),\n",
    "    'BiLSTM': create_bidirectional_lstm_model(vocab_size, tf_embed_dim, tf_hidden_dim, num_classes, max_length)\n",
    "}\n",
    "\n",
    "# Compile models\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "for name, model in tf_models.items():\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_function,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    print(f\"‚úÖ {name} model created and compiled\")\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\nüèóÔ∏è TensorFlow LSTM Model Architecture:\")\n",
    "tf_models['LSTM'].summary()\n",
    "\n",
    "# Setup callbacks\n",
    "def get_callbacks(model_name):\n",
    "    \"\"\"Get callbacks for training\"\"\"\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Optional: Save best model\n",
    "    # callbacks.append(\n",
    "    #     ModelCheckpoint(\n",
    "    #         f'best_{model_name.lower()}_model.keras',\n",
    "    #         monitor='val_accuracy',\n",
    "    #         save_best_only=True,\n",
    "    #         verbose=1\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(f\"üéØ TensorFlow models ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TensorFlow Models\n",
    "print(\"üöÄ Training TensorFlow Models\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training parameters\n",
    "tf_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Store results\n",
    "tf_results = {}\n",
    "tf_histories = {}\n",
    "\n",
    "def evaluate_tf_model(model, X_test, y_test, y_test_original):\n",
    "    \"\"\"Evaluate TensorFlow model and return detailed metrics\"\"\"\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    if num_classes > 2:\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        y_true = y_test_original\n",
    "    else:\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        y_true = y_test_original\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # AUC calculation\n",
    "    try:\n",
    "        if num_classes == 2:\n",
    "            auc = roc_auc_score(y_true, y_pred_proba.flatten())\n",
    "        else:\n",
    "            auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'targets': y_true,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_tf_training_history(history, model_name):\n",
    "    \"\"\"Plot TensorFlow training history\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    axes[0].set_title(f'{model_name} - Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    axes[1].set_title(f'{model_name} - Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train each TensorFlow model\n",
    "for model_name, model in tf_models.items():\n",
    "    print(f\"\\nüî• Training {model_name} Model\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get callbacks\n",
    "    callbacks = get_callbacks(model_name)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        validation_data=(X_val_tf, y_val_tf),\n",
    "        epochs=tf_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = evaluate_tf_model(model, X_test_tf, y_test_tf, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    tf_results[model_name] = test_metrics\n",
    "    tf_histories[model_name] = history\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä {model_name} Test Results:\")\n",
    "    print(f\"   Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "    print(f\"   AUC: {test_metrics['auc']:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_tf_training_history(history, f\"TensorFlow {model_name}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    class_names = label_encoder.classes_\n",
    "    plot_confusion_matrix(\n",
    "        test_metrics['targets'], \n",
    "        test_metrics['predictions'], \n",
    "        f\"TensorFlow {model_name}\",\n",
    "        class_names\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} training and evaluation completed!\")\n",
    "\n",
    "# Summary of TensorFlow results\n",
    "print(f\"\\nüìã TensorFlow Models Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, metrics in tf_results.items():\n",
    "    print(f\"{model_name:<10} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} \"\n",
    "          f\"{metrics['recall']:<10.4f} {metrics['f1_score']:<10.4f} {metrics['auc']:<10.4f}\")\n",
    "\n",
    "# Find best TensorFlow model\n",
    "best_tf_model = max(tf_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\nüèÜ Best TensorFlow Model: {best_tf_model[0]} with accuracy: {best_tf_model[1]['accuracy']:.4f}\")\n",
    "\n",
    "# Check if accuracy meets requirement (>= 70%)\n",
    "tf_meets_requirement = best_tf_model[1]['accuracy'] >= 0.70\n",
    "print(f\"‚úÖ Meets 70% accuracy requirement: {tf_meets_requirement}\")\n",
    "\n",
    "if not tf_meets_requirement:\n",
    "    print(\"‚ö†Ô∏è  Warning: Best model accuracy is below 70%. Consider:\")\n",
    "    print(\"   - Increasing training epochs\")\n",
    "    print(\"   - Adjusting learning rate\")\n",
    "    print(\"   - Adding more data\")\n",
    "    print(\"   - Tuning hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66dfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Comparison and Analysis\n",
    "print(\"üìä Model Comparison and ROC Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine all results\n",
    "all_results = {}\n",
    "all_results.update({f\"PyTorch_{k}\": v for k, v in pytorch_results.items()})\n",
    "all_results.update({f\"TensorFlow_{k}\": v for k, v in tf_results.items()})\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_data = []\n",
    "for model_name, metrics in all_results.items():\n",
    "    results_data.append({\n",
    "        'Model': model_name,\n",
    "        'Framework': model_name.split('_')[0],\n",
    "        'Architecture': model_name.split('_')[1],\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'AUC': metrics['auc']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display comprehensive results\n",
    "print(\"üèÜ All Models Performance Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "results_df.plot(x='Model', y='Accuracy', kind='bar', ax=axes[0, 0], \n",
    "                color=['skyblue' if 'PyTorch' in x else 'lightcoral' for x in results_df['Model']])\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].axhline(y=0.70, color='red', linestyle='--', label='70% Threshold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1-Score comparison\n",
    "results_df.plot(x='Model', y='F1-Score', kind='bar', ax=axes[0, 1],\n",
    "                color=['skyblue' if 'PyTorch' in x else 'lightcoral' for x in results_df['Model']])\n",
    "axes[0, 1].set_title('Model F1-Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# AUC comparison\n",
    "results_df.plot(x='Model', y='AUC', kind='bar', ax=axes[1, 0],\n",
    "                color=['skyblue' if 'PyTorch' in x else 'lightcoral' for x in results_df['Model']])\n",
    "axes[1, 0].set_title('Model AUC Comparison')\n",
    "axes[1, 0].set_ylabel('AUC')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Framework comparison (average metrics)\n",
    "framework_comparison = results_df.groupby('Framework')[['Accuracy', 'F1-Score', 'AUC']].mean()\n",
    "framework_comparison.plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Framework Average Performance')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Analysis (for binary classification or best models)\n",
    "def plot_roc_curves():\n",
    "    \"\"\"Plot ROC curves for all models\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
    "    color_idx = 0\n",
    "    \n",
    "    for model_name, metrics in all_results.items():\n",
    "        try:\n",
    "            y_true = metrics['targets']\n",
    "            y_proba = metrics['probabilities']\n",
    "            \n",
    "            # For binary classification\n",
    "            if num_classes == 2:\n",
    "                if len(y_proba.shape) > 1:\n",
    "                    y_proba = y_proba[:, 1]  # Positive class probability\n",
    "                else:\n",
    "                    y_proba = y_proba.flatten()\n",
    "                \n",
    "                fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "                auc_score = metrics['auc']\n",
    "                \n",
    "                plt.plot(fpr, tpr, color=colors[color_idx % len(colors)], \n",
    "                        label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "                color_idx += 1\n",
    "            \n",
    "            # For multiclass - use macro average\n",
    "            elif num_classes > 2:\n",
    "                # Calculate ROC for each class and average\n",
    "                fpr_list = []\n",
    "                tpr_list = []\n",
    "                \n",
    "                for i in range(num_classes):\n",
    "                    y_true_binary = (y_true == i).astype(int)\n",
    "                    y_proba_class = y_proba[:, i] if len(y_proba.shape) > 1 else y_proba\n",
    "                    \n",
    "                    fpr, tpr, _ = roc_curve(y_true_binary, y_proba_class)\n",
    "                    fpr_list.append(fpr)\n",
    "                    tpr_list.append(tpr)\n",
    "                \n",
    "                # Use the first class ROC as representative\n",
    "                if fpr_list:\n",
    "                    plt.plot(fpr_list[0], tpr_list[0], color=colors[color_idx % len(colors)], \n",
    "                            label=f'{model_name} (AUC = {metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "                    color_idx += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot ROC for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Plot diagonal line\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves - All Models')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "print(\"\\nüìà ROC Curve Analysis:\")\n",
    "plot_roc_curves()\n",
    "\n",
    "# Best overall model\n",
    "best_overall = max(all_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\nü•á Best Overall Model: {best_overall[0]}\")\n",
    "print(f\"   Accuracy: {best_overall[1]['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_overall[1]['f1_score']:.4f}\")\n",
    "print(f\"   AUC: {best_overall[1]['auc']:.4f}\")\n",
    "\n",
    "# Check requirements\n",
    "models_meeting_requirement = [name for name, metrics in all_results.items() if metrics['accuracy'] >= 0.70]\n",
    "print(f\"\\n‚úÖ Models meeting 70% accuracy requirement:\")\n",
    "for model in models_meeting_requirement:\n",
    "    acc = all_results[model]['accuracy']\n",
    "    print(f\"   {model}: {acc:.4f}\")\n",
    "\n",
    "if not models_meeting_requirement:\n",
    "    print(\"‚ùå No models meet the 70% accuracy requirement\")\n",
    "    print(\"\\nüí° Recommendations for improvement:\")\n",
    "    print(\"   1. Increase training epochs (current: ~25-30)\")\n",
    "    print(\"   2. Use larger embedding dimensions\")\n",
    "    print(\"   3. Add more sophisticated preprocessing\")\n",
    "    print(\"   4. Try ensemble methods\")\n",
    "    print(\"   5. Use pre-trained embeddings (Word2Vec, GloVe)\")\n",
    "    print(\"   6. Add more data if possible\")\n",
    "    print(\"   7. Fine-tune hyperparameters\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ {len(models_meeting_requirement)} models successfully meet the requirement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a64d78",
   "metadata": {},
   "source": [
    "## Mathematical Explanations and Analysis\n",
    "\n",
    "### 1. Evaluation Metrics Explained\n",
    "\n",
    "#### Accuracy\n",
    "**Formula:** \n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "- **TP**: True Positive, **TN**: True Negative\n",
    "- **FP**: False Positive, **FN**: False Negative\n",
    "- **Interpretasi**: Proporsi prediksi yang benar dari total prediksi\n",
    "\n",
    "#### Precision\n",
    "**Formula:**\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "- **Interpretasi**: Dari semua prediksi positif, berapa yang benar-benar positif\n",
    "- **Penting untuk**: Mengurangi False Positive\n",
    "\n",
    "#### Recall (Sensitivity)\n",
    "**Formula:**\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "- **Interpretasi**: Dari semua data positif yang sebenarnya, berapa yang berhasil diprediksi\n",
    "- **Penting untuk**: Mengurangi False Negative\n",
    "\n",
    "#### F1-Score\n",
    "**Formula:**\n",
    "```\n",
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "```\n",
    "- **Interpretasi**: Harmonic mean dari Precision dan Recall\n",
    "- **Berguna untuk**: Dataset yang tidak seimbang\n",
    "\n",
    "#### AUC-ROC (Area Under Curve - Receiver Operating Characteristic)\n",
    "**Formula:**\n",
    "```\n",
    "AUC = ‚à´[0,1] TPR(FPR‚Åª¬π(x)) dx\n",
    "```\n",
    "Where:\n",
    "- **TPR**: True Positive Rate = Recall\n",
    "- **FPR**: False Positive Rate = FP / (FP + TN)\n",
    "- **Interpretasi**: Kemampuan model untuk membedakan antar kelas\n",
    "\n",
    "### 2. Loss Functions Explained\n",
    "\n",
    "#### Cross-Entropy Loss (Multi-class)\n",
    "**Formula:**\n",
    "```\n",
    "Loss = -‚àë[i=1,N] ‚àë[c=1,C] y_ic √ó log(p_ic)\n",
    "```\n",
    "Where:\n",
    "- **y_ic**: True label (1 if sample i belongs to class c, 0 otherwise)\n",
    "- **p_ic**: Predicted probability for sample i belonging to class c\n",
    "- **N**: Number of samples, **C**: Number of classes\n",
    "\n",
    "#### Binary Cross-Entropy Loss\n",
    "**Formula:**\n",
    "```\n",
    "Loss = -‚àë[i=1,N] [y_i √ó log(p_i) + (1-y_i) √ó log(1-p_i)]\n",
    "```\n",
    "Where:\n",
    "- **y_i**: True label (0 or 1)\n",
    "- **p_i**: Predicted probability\n",
    "\n",
    "### 3. Optimization Algorithms\n",
    "\n",
    "#### Adam Optimizer\n",
    "**Formula:**\n",
    "```\n",
    "m_t = Œ≤‚ÇÅ √ó m_{t-1} + (1 - Œ≤‚ÇÅ) √ó g_t\n",
    "v_t = Œ≤‚ÇÇ √ó v_{t-1} + (1 - Œ≤‚ÇÇ) √ó g_t¬≤\n",
    "mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ·µó)\n",
    "vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ·µó)\n",
    "Œ∏_t = Œ∏_{t-1} - Œ± √ó mÃÇ_t / (‚àövÃÇ_t + Œµ)\n",
    "```\n",
    "Where:\n",
    "- **g_t**: Gradient at time t\n",
    "- **m_t**: First moment estimate (mean)\n",
    "- **v_t**: Second moment estimate (variance)\n",
    "- **Œ≤‚ÇÅ, Œ≤‚ÇÇ**: Decay rates (typically 0.9, 0.999)\n",
    "- **Œ±**: Learning rate\n",
    "- **Œµ**: Small constant for numerical stability\n",
    "\n",
    "### 4. Regularization Techniques\n",
    "\n",
    "#### Dropout\n",
    "**Formula:**\n",
    "```\n",
    "y = x ‚äô m / p\n",
    "```\n",
    "Where:\n",
    "- **x**: Input\n",
    "- **m**: Bernoulli random mask\n",
    "- **p**: Keep probability\n",
    "- **‚äô**: Element-wise multiplication\n",
    "\n",
    "**Tujuan**: Mencegah overfitting dengan randomly \"dropping out\" neurons\n",
    "\n",
    "#### L2 Regularization (Weight Decay)\n",
    "**Formula:**\n",
    "```\n",
    "Loss_total = Loss_original + Œª √ó ‚àë(w¬≤)\n",
    "```\n",
    "Where:\n",
    "- **Œª**: Regularization parameter\n",
    "- **w**: Model weights\n",
    "\n",
    "### 5. Gradient Clipping\n",
    "**Formula:**\n",
    "```\n",
    "if ||g|| > threshold:\n",
    "    g = g √ó threshold / ||g||\n",
    "```\n",
    "Where:\n",
    "- **g**: Gradient vector\n",
    "- **||g||**: L2 norm of gradient\n",
    "- **threshold**: Clipping threshold\n",
    "\n",
    "**Tujuan**: Mencegah exploding gradients dalam RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Analysis and Conclusions\n",
    "print(\"üìã Final Analysis and Conclusions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create detailed analysis\n",
    "print(\"üîç Detailed Model Analysis:\")\n",
    "print(\"\\n1. **Architecture Comparison:**\")\n",
    "\n",
    "architecture_performance = results_df.groupby('Architecture')[['Accuracy', 'F1-Score', 'AUC']].agg(['mean', 'std'])\n",
    "print(\"\\nüìä Average Performance by Architecture:\")\n",
    "print(architecture_performance.round(4))\n",
    "\n",
    "framework_performance = results_df.groupby('Framework')[['Accuracy', 'F1-Score', 'AUC']].agg(['mean', 'std'])\n",
    "print(\"\\nüìä Average Performance by Framework:\")\n",
    "print(framework_performance.round(4))\n",
    "\n",
    "print(\"\\n2. **Key Findings:**\")\n",
    "\n",
    "# Find best performing architecture\n",
    "best_arch = results_df.groupby('Architecture')['Accuracy'].mean().idxmax()\n",
    "best_arch_acc = results_df.groupby('Architecture')['Accuracy'].mean().max()\n",
    "\n",
    "# Find best performing framework\n",
    "best_framework = results_df.groupby('Framework')['Accuracy'].mean().idxmax()\n",
    "best_framework_acc = results_df.groupby('Framework')['Accuracy'].mean().max()\n",
    "\n",
    "print(f\"   üèÜ Best Architecture: {best_arch} (Avg. Accuracy: {best_arch_acc:.4f})\")\n",
    "print(f\"   üèÜ Best Framework: {best_framework} (Avg. Accuracy: {best_framework_acc:.4f})\")\n",
    "\n",
    "# Count models meeting requirement\n",
    "meeting_req_count = len([m for m in all_results.values() if m['accuracy'] >= 0.70])\n",
    "total_models = len(all_results)\n",
    "\n",
    "print(f\"   ‚úÖ Models meeting 70% requirement: {meeting_req_count}/{total_models}\")\n",
    "\n",
    "# Performance insights\n",
    "print(\"\\n3. **Performance Insights:**\")\n",
    "\n",
    "if 'LSTM' in results_df['Architecture'].values:\n",
    "    lstm_performance = results_df[results_df['Architecture'] == 'LSTM']['Accuracy'].mean()\n",
    "    print(f\"   üìà LSTM Average Accuracy: {lstm_performance:.4f}\")\n",
    "\n",
    "if 'GRU' in results_df['Architecture'].values:\n",
    "    gru_performance = results_df[results_df['Architecture'] == 'GRU']['Accuracy'].mean()\n",
    "    print(f\"   üìà GRU Average Accuracy: {gru_performance:.4f}\")\n",
    "\n",
    "if 'RNN' in results_df['Architecture'].values:\n",
    "    rnn_performance = results_df[results_df['Architecture'] == 'RNN']['Accuracy'].mean()\n",
    "    print(f\"   üìà Vanilla RNN Average Accuracy: {rnn_performance:.4f}\")\n",
    "\n",
    "# Model complexity analysis\n",
    "print(\"\\n4. **Model Complexity Analysis:**\")\n",
    "print(\"   üîπ **Vanilla RNN**: Simplest architecture, prone to vanishing gradient\")\n",
    "print(\"   üîπ **LSTM**: More complex, better long-term dependencies, more parameters\")\n",
    "print(\"   üîπ **GRU**: Balanced complexity, fewer parameters than LSTM\")\n",
    "print(\"   üîπ **BiLSTM**: Most complex, processes sequences in both directions\")\n",
    "\n",
    "print(\"\\n5. **Training Efficiency:**\")\n",
    "print(\"   üî∏ **PyTorch**: More explicit control, good for research\")\n",
    "print(\"   üî∏ **TensorFlow**: Higher-level API, easier deployment\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if meeting_req_count < total_models:\n",
    "    print(\"\\nüöÄ **To achieve 70%+ accuracy for all models:**\")\n",
    "    print(\"   1. **Data Enhancement:**\")\n",
    "    print(\"      - Collect more training data\")\n",
    "    print(\"      - Apply data augmentation techniques\")\n",
    "    print(\"      - Balance class distribution\")\n",
    "    \n",
    "    print(\"\\n   2. **Preprocessing Improvements:**\")\n",
    "    print(\"      - Use more sophisticated text cleaning\")\n",
    "    print(\"      - Implement better tokenization\")\n",
    "    print(\"      - Try different text normalization methods\")\n",
    "    \n",
    "    print(\"\\n   3. **Model Architecture:**\")\n",
    "    print(\"      - Increase embedding dimensions (128 ‚Üí 256)\")\n",
    "    print(\"      - Add more LSTM/GRU layers\")\n",
    "    print(\"      - Experiment with attention mechanisms\")\n",
    "    print(\"      - Try bidirectional architectures\")\n",
    "    \n",
    "    print(\"\\n   4. **Training Optimization:**\")\n",
    "    print(\"      - Increase training epochs (25-30 ‚Üí 50-100)\")\n",
    "    print(\"      - Use learning rate scheduling\")\n",
    "    print(\"      - Implement early stopping with patience\")\n",
    "    print(\"      - Try different batch sizes\")\n",
    "    \n",
    "    print(\"\\n   5. **Advanced Techniques:**\")\n",
    "    print(\"      - Use pre-trained embeddings (Word2Vec, GloVe, FastText)\")\n",
    "    print(\"      - Implement transfer learning\")\n",
    "    print(\"      - Try ensemble methods\")\n",
    "    print(\"      - Use transformer-based models (BERT, RoBERTa)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüéâ **Excellent Results! All models meet the 70% requirement.**\")\n",
    "    print(\"\\nüåü **Further improvements could include:**\")\n",
    "    print(\"   - Fine-tuning hyperparameters\")\n",
    "    print(\"   - Ensemble model combinations\")\n",
    "    print(\"   - Cross-validation for robustness\")\n",
    "    print(\"   - Production deployment optimization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ **Models Implemented**: {total_models}\")\n",
    "print(f\"   - PyTorch Models: {len(pytorch_results)}\")\n",
    "print(f\"   - TensorFlow Models: {len(tf_results)}\")\n",
    "\n",
    "print(f\"\\nüìä **Best Overall Performance**:\")\n",
    "print(f\"   - Model: {best_overall[0]}\")\n",
    "print(f\"   - Accuracy: {best_overall[1]['accuracy']:.4f}\")\n",
    "print(f\"   - F1-Score: {best_overall[1]['f1_score']:.4f}\")\n",
    "\n",
    "requirement_status = \"‚úÖ MET\" if meeting_req_count == total_models else f\"‚ö†Ô∏è PARTIAL ({meeting_req_count}/{total_models})\"\n",
    "print(f\"\\nüéØ **70% Accuracy Requirement**: {requirement_status}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è **Architecture Ranking** (by average accuracy):\")\n",
    "arch_ranking = results_df.groupby('Architecture')['Accuracy'].mean().sort_values(ascending=False)\n",
    "for i, (arch, acc) in enumerate(arch_ranking.items(), 1):\n",
    "    print(f\"   {i}. {arch}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà **All Evaluation Metrics Successfully Implemented**:\")\n",
    "print(\"   ‚úÖ Accuracy\")\n",
    "print(\"   ‚úÖ Precision\")\n",
    "print(\"   ‚úÖ Recall\")\n",
    "print(\"   ‚úÖ F1-Score\")\n",
    "print(\"   ‚úÖ AUC-ROC\")\n",
    "print(\"   ‚úÖ Confusion Matrix\")\n",
    "print(\"   ‚úÖ Loss Visualization\")\n",
    "\n",
    "print(f\"\\nüî¨ **Mathematical Explanations**: ‚úÖ Complete\")\n",
    "print(f\"üé® **Visualizations**: ‚úÖ Complete\")\n",
    "print(f\"ü§ñ **Both Frameworks**: ‚úÖ PyTorch & TensorFlow\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì **PROJECT COMPLETED SUCCESSFULLY!**\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
