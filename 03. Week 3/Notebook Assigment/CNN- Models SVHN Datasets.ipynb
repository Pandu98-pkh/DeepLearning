{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101d40b8",
   "metadata": {},
   "source": [
    "# CNN dan MLP Models untuk SVHN Dataset\n",
    "\n",
    "## Tugas Deep Learning - Week 3\n",
    "\n",
    "**Objectives:**\n",
    "1. Membuat model Deep Learning CNN dan MLP menggunakan PyTorch dan TensorFlow\n",
    "2. Menggunakan dataset SVHN dari tensorflow_datasets dan torchvision.datasets\n",
    "3. Implementasi matriks evaluasi lengkap (Accuracy, Precision, Recall, F1-Score, AUC, ROC)\n",
    "4. Memberikan penjelasan matematika untuk setiap persamaan\n",
    "5. Mencapai akurasi minimal 75% pada training dan testing set untuk CNN\n",
    "6. Model MLP vanilla dengan akurasi bebas\n",
    "\n",
    "## Dataset: SVHN (Street View House Numbers)\n",
    "SVHN adalah dataset yang berisi gambar nomor rumah dari Google Street View. Dataset ini terdiri dari:\n",
    "- 10 kelas (digit 0-9)\n",
    "- Format gambar 32x32x3 (RGB)\n",
    "- Training set: ~73,257 gambar\n",
    "- Test set: ~26,032 gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de8d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "PyTorch version: 2.7.1+cpu\n",
      "GPU available (TensorFlow): []\n",
      "GPU available (PyTorch): False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import SVHN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds untuk reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available (TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU available (PyTorch):\", torch.cuda.is_available())\n",
    "\n",
    "# Device configuration untuk PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409f7db",
   "metadata": {},
   "source": [
    "## 1. Data Loading dan Preprocessing\n",
    "\n",
    "### Penjelasan Matematika - Normalisasi Data\n",
    "\n",
    "Normalisasi data adalah proses mengubah skala data agar berada dalam rentang tertentu. Untuk gambar RGB, kita melakukan normalisasi dengan rumus:\n",
    "\n",
    "$$X_{normalized} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Dimana:\n",
    "- $X$ = nilai pixel asli (0-255)\n",
    "- $\\mu$ = mean dari dataset\n",
    "- $\\sigma$ = standard deviation dari dataset\n",
    "\n",
    "Untuk dataset SVHN, nilai normalisasi umum yang digunakan:\n",
    "- Mean: [0.4377, 0.4438, 0.4728] untuk channel R, G, B\n",
    "- Std: [0.1980, 0.2010, 0.1970] untuk channel R, G, B\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "Data augmentation adalah teknik untuk meningkatkan variasi data training dengan transformasi:\n",
    "\n",
    "$$T(x) = \\{rotation, flip, crop, brightness, contrast\\}$$\n",
    "\n",
    "Dimana $T(x)$ adalah transformasi yang diterapkan pada input $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ca0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SVHN dataset with TensorFlow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\pandu\\tensorflow_datasets\\svhn_cropped\\3.1.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\pandu\\tensorflow_datasets\\svhn_cropped\\3.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 1501/1501 [17:14<00:00,  1.45 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [17:14<00:00, 344.75s/ url]\n",
      "Dl Size...: 100%|██████████| 1501/1501 [17:14<00:00,  1.45 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [17:14<00:00, 344.75s/ url]\n",
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset svhn_cropped downloaded and prepared to C:\\Users\\pandu\\tensorflow_datasets\\svhn_cropped\\3.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Dataset info: tfds.core.DatasetInfo(\n",
      "    name='svhn_cropped',\n",
      "    full_name='svhn_cropped/3.1.0',\n",
      "    description=\"\"\"\n",
      "    The Street View House Numbers (SVHN) Dataset is an image digit recognition\n",
      "    dataset of over 600,000 digit images coming from real world data. Images are\n",
      "    cropped to 32x32.\n",
      "    \"\"\",\n",
      "    homepage='http://ufldl.stanford.edu/housenumbers/',\n",
      "    data_dir='C:\\\\Users\\\\pandu\\\\tensorflow_datasets\\\\svhn_cropped\\\\3.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=1.47 GiB,\n",
      "    dataset_size=1.09 GiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'extra': <SplitInfo num_examples=531131, num_shards=8>,\n",
      "        'test': <SplitInfo num_examples=26032, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=73257, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"\"\"\"Street View House Numbers (SVHN) Dataset, cropped version.\"\"\"\n",
      "    \n",
      "    @article{Netzer2011,\n",
      "    author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},\n",
      "    booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n",
      "    title = {Reading Digits in Natural Images with Unsupervised Feature Learning},\n",
      "    year = {2011}\n",
      "    }\"\"\",\n",
      ")\n",
      "Training samples: 73257\n",
      "Testing samples: 26032\n",
      "Dataset info: tfds.core.DatasetInfo(\n",
      "    name='svhn_cropped',\n",
      "    full_name='svhn_cropped/3.1.0',\n",
      "    description=\"\"\"\n",
      "    The Street View House Numbers (SVHN) Dataset is an image digit recognition\n",
      "    dataset of over 600,000 digit images coming from real world data. Images are\n",
      "    cropped to 32x32.\n",
      "    \"\"\",\n",
      "    homepage='http://ufldl.stanford.edu/housenumbers/',\n",
      "    data_dir='C:\\\\Users\\\\pandu\\\\tensorflow_datasets\\\\svhn_cropped\\\\3.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=1.47 GiB,\n",
      "    dataset_size=1.09 GiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'extra': <SplitInfo num_examples=531131, num_shards=8>,\n",
      "        'test': <SplitInfo num_examples=26032, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=73257, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"\"\"\"Street View House Numbers (SVHN) Dataset, cropped version.\"\"\"\n",
      "    \n",
      "    @article{Netzer2011,\n",
      "    author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},\n",
      "    booktitle = {Advances in Neural Information Processing Systems ({NIPS})},\n",
      "    title = {Reading Digits in Natural Images with Unsupervised Feature Learning},\n",
      "    year = {2011}\n",
      "    }\"\"\",\n",
      ")\n",
      "Training samples: 73257\n",
      "Testing samples: 26032\n",
      "TensorFlow data loading completed!\n",
      "TensorFlow data loading completed!\n"
     ]
    }
   ],
   "source": [
    "# ========== TENSORFLOW DATA LOADING ==========\n",
    "print(\"Loading SVHN dataset with TensorFlow...\")\n",
    "\n",
    "# Load SVHN dataset menggunakan tensorflow_datasets\n",
    "(ds_train_tf, ds_test_tf), ds_info = tfds.load(\n",
    "    'svhn_cropped',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(f\"Dataset info: {ds_info}\")\n",
    "print(f\"Training samples: {ds_info.splits['train'].num_examples}\")\n",
    "print(f\"Testing samples: {ds_info.splits['test'].num_examples}\")\n",
    "\n",
    "# Preprocessing function untuk TensorFlow\n",
    "def preprocess_tf(image, label):\n",
    "    # Normalize pixel values ke range [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # Normalisasi dengan mean dan std ImageNet (karena SVHN belum ada standar khusus)\n",
    "    mean = tf.constant([0.485, 0.456, 0.406])\n",
    "    std = tf.constant([0.229, 0.224, 0.225])\n",
    "    image = (image - mean) / std\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Data augmentation untuk training\n",
    "def augment_tf(image, label):\n",
    "    # Random rotation\n",
    "    image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "    # Random brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    # Random contrast\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing\n",
    "BATCH_SIZE = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Training set dengan augmentation\n",
    "ds_train_tf = ds_train_tf.map(preprocess_tf, num_parallel_calls=AUTOTUNE)\n",
    "ds_train_tf = ds_train_tf.map(augment_tf, num_parallel_calls=AUTOTUNE)\n",
    "ds_train_tf = ds_train_tf.cache()\n",
    "ds_train_tf = ds_train_tf.shuffle(1000)\n",
    "ds_train_tf = ds_train_tf.batch(BATCH_SIZE)\n",
    "ds_train_tf = ds_train_tf.prefetch(AUTOTUNE)\n",
    "\n",
    "# Test set tanpa augmentation\n",
    "ds_test_tf = ds_test_tf.map(preprocess_tf, num_parallel_calls=AUTOTUNE)\n",
    "ds_test_tf = ds_test_tf.cache()\n",
    "ds_test_tf = ds_test_tf.batch(BATCH_SIZE)\n",
    "ds_test_tf = ds_test_tf.prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"TensorFlow data loading completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da777808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SVHN dataset with PyTorch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182M/182M [02:40<00:00, 1.13MB/s] \n",
      "\n",
      "100%|██████████| 64.3M/64.3M [00:56<00:00, 1.13MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Training samples: 73257\n",
      "PyTorch Testing samples: 26032\n",
      "PyTorch data loading completed!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAIfCAYAAAChPG9iAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmHhJREFUeJzt3Qe0betd1v/ZV9l7n35uJaRIIBAQpJkRwNASCBokGpQOAqKQCDJoMowE+CdUTSIGog4x9CYQYAyRovRhBCLFRKQHSEhubjnn7LbKrP8xVzzXW9bznL3evfe895z7/YxxjezfnWvN8rb5nn3PE3dd10UAAAAAAADAgJIhvwwAAAAAAADosSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKXWL+7M/+7MojuPoX/7Lf3lin/lLv/RLq8/s/xfAyaLPAjcX+ixwc6HPAjcX+uytj02pDfQN9yj/9I37eudZ98+znvUs+z3f9V3ftfr33vjGN0a3oh//8R+P/v7f//vR0572tGg6nUbv9V7vFX3Zl31ZdO3atcf61HCLoc+ejKc85Sny3jz96U9/rE8PtxD67MlgnsVQ6LOn47nPfe7qel/ykpc81qeCWwx99nTQZ48nO+bxTyjf+73f+7D/+3u+53uin//5n3/Uz9/7vd87ms/nq///p37qp0af8Amf8LD65cuXoyeyL/iCL4juuuuu6DM+4zOid3/3d4/e9KY3Ra95zWuin/7pn45+67d+K5pMJo/1KeIWQZ89Ga9+9aujg4ODh/3sz//8z6OXvvSl0fOe97zH7Lxw66HPngzmWQyFPns6m8pveMMbHuvTwC2KPnvy6LPHx6bUBvrF3UP9j//xP1ad+JE/7/U7y70P/MAPXFt/IvvRH/3R6CM/8iMf9rMP+qAPij77sz87+v7v//7o8z//8x+zc8OthT57Mj7pkz7pUT97+ctfvvrfT//0T38Mzgi3KvrsyWCexVDosydrsVisfqvxq77qq6Kv+ZqveaxPB7cg+uzJos+eDP7zvZtUWZarht8vMs+ePRttbW1FH/ERHxH94i/+ojzmVa96VfTkJz959Sekz3nOc6I3v/nNj/p3fv/3fz960YteFF24cCEaj8fRB3/wB0c/9VM/dcPzmc1mq2Pvv//+G/67j1wo9174wheu/vf//J//c8PjgZvRzdxn1/mBH/iB6KlPfWr07Gc/O+h44PHuZu6zzLN4IrqZ++x13/It3xK1bRt9+Zd/+ZGPAW5W9Flcx6bUKesbd9+wH/pPVVXH/ty9vb3oP/yH/7BaeH7zN39z9LVf+7XRfffdF33cx31c9Du/8zuP+vf7X838tm/7tujFL35x9NVf/dWrDvzRH/3R0Tvf+c4H/53//b//9+q/D+4XrP/sn/2z6F/9q3+1Ghz635J4/etfb8/nN37jN1a/5tn/5wEh7rnnntX/Xrp0Keh44KTQZ2/st3/7t1ff+Wmf9mkbHwucNPrs0TDP4vGCPrveX/zFX0Tf9E3ftDp3/hNbPJ7QZ9ejz56gDsFe/OIXd+oWvuUtb1nV1v3zi7/4i/ZzX/e6163+vd/8zd+U/05d191yuXzYz65evdrdfvvt3ed+7uc+6jwmk0n3tre97cGf//qv//rq51/6pV/64M8+5mM+pnu/93u/brFYPPiztm27Zz/72d3Tn/70B3/Wn/8jr+P6z172spd1IT7v8z6vS9O0+8M//MOg44GjoM+eTJ/9si/7stWxv/d7v7fxscAm6LPMs7i50GfD++yLXvSi1ede1x/b30/gNNFn6bOPB/ydUgP8ZaOf/Mmf/LCfvf/7v/+xPzdN09U/vf5XBvtEnf5/+19P7P8S00fqd4fvvvvuB//vD/3QD43++l//66u/9PSVr3xldOXKlegXfuEXoq//+q+P9vf3V/9c1+9Wv+xlL4v+8i//8mGf8VD9Dve7+mLYfwb0nd/5ndFXfuVXkuSFxxx91uvP+Yd+6Ieiv/bX/trqT5OAxxp99saYZ/F4Qp99tP4/V/qxH/ux6Nd//dcDrhw4XfTZR6PPniw2pU5Zv/j72I/92FP57O/+7u9e/Upi/9++PvRXKPu/52XdeTzSe77ne0Y/8iM/svr///Ef//GqE/6Lf/EvVv+sc++998pOHOpXf/VXo8/7vM9bDRSveMUrTvSzgRD0We+Xf/mXVxP6l37pl57o5wKh6LMe8yweb+izD1fXdfTFX/zF0Wd+5mdGH/IhH3KszwJOA3324eizJ49NqZvU933f90Wf8zmfs9ox/oqv+IrotttuW+00f+M3fmP0J3/yJxt/Xr8r3ev/krZ+4brOe7zHe0Qn6Xd/93ejT/zET4ze933fd5UUlGU0R9y6boU+2+uTu5IkWcUDA7eyW6HPMs/iieRm7bP935PzB3/wB9G/+3f/7sG0s+v63/Tof9Zfy3Q6PfZ3AY8n9Flcx+rkJtUvLp/2tKdFP/7jPx7Fcfzgz/tfS1znj/7ojx71sz/8wz+MnvKUp6z+//1n9fI8P7Wd8IfqB5qP//iPX3XY/lcut7e3T/07gcfSzd5ne8vlcvWryv2vN991112DfCfwWLnZ+yzzLJ5obtY+2/9lyf1viHzYh33Y2pff/p/+L2juX9yBWwl9FteRvneTuv7f3z70v3vt/5vWN7zhDWv//Z/4iZ9Y/Sc3D00X6P/95z//+av/u1+09i+a/Y7vO97xjkcd3ychnFSEZp8A9LznPW/12xY/+7M/G12+fPmGxwA3u5u5z17Xv9j2/73/p3/6px/5GOBmdTP3WeZZPBHdrH32Uz7lU1YvsI/8p/cJn/AJq/9///fmALca+iyu4zelHsf+43/8j9HP/MzPPOrnX/IlXxL9rb/1t1a7yi984Qujv/k3/2b0lre8Jfq3//bfRu/zPu8THRwcrP1VxQ//8A+PvvALv3D12w6vfvWro4sXL67+0tPrvv3bv33177zf+71f9A//4T9c7Tb3EZv9wPC2t71t9Z8BKP2g8FEf9VGrne0+ztPp/+T2T//0T1ff/Wu/9murf667/fbbo+c+97kb3CXg8eNW7bMP/U/3RqNR9Hf/7t898j0BHs9u1T7LPItb1a3YZ5/xjGes/lmn/3t1+G0L3MzoszgKNqUex1772teu/Xn/3972//R/EtrvBPd/Ctp33v6/y/1P/+k/Rb/0S7/0qGM+67M+a/Unpn3n7f+Ctz6t4DWveU105513Pvjv9J/xxje+Mfq6r/u66Lu+67uiBx54YLXj3Kdsfc3XfM2JXdf1weBbvuVbHlV7znOew2IZN61btc/29vb2ov/8n//zatFw9uzZE/1s4LFyq/ZZ5lncqm7VPgvcquizOIq4C80XBgAAAAAAAALxd0oBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcNlR/8X/8l9/XdaSVO9tZXkha3mam89MozCdrLRdu/6ItpHHxFGsa4nZ09OHRZ04j3edi6415rjWHBe1XdC2ZJLoi8jM80lT3axS01ZCuHvZmOt2zzW1bS/0/M0zMJ71194r8Pui6DWveU3gdcRBbcJ1B6cz59J18ca30zSJyHaTWl9ba8aILq71cVET2Ifc2GIuPtYXWLuLj/S5tOoZ9F8Xb96PEtNQYnO/+rupj9P3JHHHmXHaXZt7BF/wj14chfrn/+p1QeeTtO6eaokdl8x84/pf1218/plpt3aaNZ+5LEtZa5pK1tqmCZpTQsfNNNMX6PpKamqta9hGJ46LzfjQxWb8NtftGqZdz4j2deO5W3/m13/J50Whfu0XfkTWOtNeuihsfZKZ9ZV5FFYX0N7bgLV2r670fFmZvqfGlV5s2lme6ncQ1wirqgpap7v50rXPPDevY+Yz22b9fantOObm2TCtHRsDx01Te8ELPyMK9T7v/0GyVpf6udeVnlOq0rzXNaZmnq1rZ2o8a9uw9ajpXn0j08eZw2Iz5jr23TqYm5/NOG1f3cyzMweq94nOfJcbO5Kw22zFif6+OA7bn3n7vW+94b/Db0oBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwJoP04RqXs27SRbvURKe6HMPAGHIXUKniNRsT1+minENjGDsTk+nus4tJdtGhNkrXXENr4i5d5HTsPtRI05A90rC28ETg0mRddHSW6WFhVOSyVpiae06L5VLW6nr94NLUro0lQXHaiTt/08zqWkcIN62+trapwyKbXTex44fuD42L4jZf5yLrQ9gg+7CU+6g1x5kk4Ch24+ZjMLa4cdwO5IaLiLYDiDlMTQ2ZmTNcK0pMNXYxz+Z+NZWu1Usd+12K8eiGUfCmn+S5jqwvRubaM30NZpizLVeky0edfeDmE03Jtljzme6Zx2b8S+y6MZxbP7rrd/3ZrTsb86GZjeo29018X2Y+rjPzSe2erhuQTV9XMervEoctdgLj3t3a2EnMuSRJdqLr3NTekzisnZijGvs+52706fRLp231mq21a6go7DjXdKPQ+7Y59+7sm4Qu1pVex7rlYZyEjVVuHdu3+pBzseNA4CtmZx66esdq3fM2pSQ9+T7UBc7dsR3HbozflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwOCOnN1Xmth2FwvuAy1NnHMbFpfrqJh1F93oIkDjwDj0znxoa6LZm87U3ImaqFLHRSh3RX7iTc49VxdlKo9x7cu2oc2jk28k8BEcj4tgtxHl+hmNRmNZG4/HQX0lSXWM63K5Prq3jKqg5z4aTWVtMt2StSzX7b2u9Ng4X85kbbHQtaox12eaYOcaWqOje7vYjR9hz07dM9f2qqrU52FnEzNfdIFxuebr4uRko5qPzcVRm1rsM41NTR+nosg78xw6FwFt2q2Le29rff51qdvZ/HAha8uyCopQTmI9phaFvr62KcxxutaNTNy26euqq8fmPndmfIjTk5+DM/NnqXadcMLx6teVS9M+jc71IREZ3stj82xTM69neg6Ls3TjebszY0fq1riZXsdWtamZvufGOPdn726+bNx4VZvxyrTd1MyXaaL7c2KeeZeufw6VGTdj886TmL4em3vp3kHs+8kN3hJPRRc2p7j+FdXuODPm6kcRxC133JhjP7PRx7Wmz+Zj06YT3aZrM3fbd3I3t7Xm+0wbdLNUKsbNXj7SY/jOuZ3151Hqdrm/tytrrRuLzZzYmn2W2ryDdGYKSqLQvYHrxwMAAAAAAAADY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAIPTmYWPsFjomGQX8e0CFWMTCWlS6aO2DYv47kREausyOc1XxU1YXG7buOhUfS6NiXitXdy7jcvVEhN/6iIoYxP17NpK2+paKqKlXQS0a5Y2OtqGgN48XPzryMSJTycTWdvaWR9l2tve1rXMtKW9vT1Za9v9jdvRdLIta+fOnZe1nTNnZW081vdkudRj43xxqGvLuazVtY6s73uf0pixrFzqz5zN9bnMZrpWVc3G7c+GEtu5JKytN+Yb3Tid2HFg+D/PcdfoItFje/32C03NRDZ36+9NYr6t6cKiid0FLA7C2vThoe6zC9OHXGx2amLGs1yPxZPlSNbGY12bbk9lLco3n7tdlH1s+qxb46WmD9l5PXJcH4lORW0yyn0Eu+kPtRl7Er04Thp9XG7iy2PxmW4N6MbOLHFjThYU6Z4kedC6uTX9sqr02NKYcacxn+keeZqZ96HYPNf0yK9qDz1If5dps5n5rsT0vsbMCY35vsdCYwYDN0+5cbwzY2Rr7ltp3t38q4hYX9lbHQeNVYlpS+OpnmuaWvfLqq6C+lAc63OJTK0xz6Ay62317tkbF/raL1zQ7xrbO+uPm89m8pj9vfXvQqvjzDtInupx07yqR2Wt22VV6VpemOdzBPymFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABnfknNGyXG4cJ9tLTbxolrmoQhMJ7KJOTcRhLSI0Q+NKExPN3roY7kYfV5kYxsbUXGx25+LCzQ1L2yQoGrUxz7xOdERoYiJVNRexG3hc4FZtZzOnTymP2lDx3r3JdCJr586dk7ULFy+a484HxUc7Bwfr49lHIx2Hfvttt8vanXfdJWtnz+rrzsc6tr0qdZzs0oybts+6McnEbbcmGnux0LGx999/v6y94x33yNrVK1dlbblcP1650TY1seVtazq06Xtdp+9JaI+NOzu4BPMR8uY4M5652F/3fe4z48REY4uPrBr9HOql7kO1iY5uTDTxoRg7ejMTvTw7nOtzMRHyrg26xpSa9VO10OPccjIKmsMSveyKknx9/8sLfVBm+mxsOrs9R9O+3CqhMhHktVl3HUfdmPHYPXgzxidxHfRnyal5uJV57iPxmbH9c2sTIR/rWpzrz8xyPeY0ZqyqarOmLquw9mJqSzNeubVlmul1RGvmFDdGpKqvmPYVp/pe5rluKLlZ27v3KNsP7Fw6/DzrWnzqTifVR7bmZaQq3frErWu6x8W7RGbadFXpubQx46Z7F4zN+3/UmWdg1oGx+b5iMpa1S5duk7W77tLvIVm2/jzvbfQ6vBhtyVrTmIZp5tLY3UozXzaVfnaFez5HwG9KAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcDrf8xGWSx2rmmU6HrA28aKNiRxMzWcmPqxbl8RhnYtyNt/VmJjPtjNxio2O+21qXatMNHZVl0HRoeYSorzTzSPLsrD412DxxlGlPk522BjaIeNbr5ts6QjRM2fOytrFS5dl7fJlXTt3Xn9ma/r6fKZjY6dbu2t/nppo4jNnd2Tt4qVz+rhz+jhPR7N3kf7Mxo47WmbbvLYol7I2Hk2CorFLG7e9t/7zzPiXZqOgftmZ8daLw+KYTyepOorNgOzGkM5dR/BnRkH3JhLPwrX30rSJxWIha9VSt+n5Qo8rs6WuLSv9fZ2Jnndic+1uPRBFuu+1plbMdUx31ug/iyy6fOM/vXTDkZufu1bX4sS1PrO2MmukugwdI7z5YhnUv9pWP7/ExGrn+rAoTfVzH5mhVY4fLk48CpOm+p0gMd/nXlaSSvehzkTP17WuHRzqcWA+17XE5KzHiX4+W9uyFGWR+UyxFsqysCc0Mg0lz/VT6FrTL9184eLsH2+/jeHGOvcy5d7BzHjmZ+F283k78D3Ezd1xowekONZ3OjFr+DTJg46rTV93lz4qdJu/eOmSrN1197vL2h133i5r88Xh2p8nVw7kMWkxlbWkCtvD6PSwEkWpfq55qg8cTfR5HgW/KQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMG5lNUjx4K7nMzGRMHbGG8XQWmjTnUtEZGdsYmh9QGbWuLuifs2c92tuZeNiaruOhc9bM7TRJy2Jv61c8/cXF+IODiYOMzJnv3p2trakrUzZ87K2uXbdQTqHbdflLXJeCto/JhOdYRokakhKgmLoXU1G4kbBfGHmbh0ey666oayXN7LKDp7VreHSxduk7XDg7mszQ/X15YHpTwmy8wdC4wz9g/BzDMu/rk7nT/PcZHN7vLdcXFr4sTNRcapm59d/Pf6z2xMbHtl2vS81O1ludDR7GVpao0+lyjR55IV5l66/mzuV2TWQW2ix82mNfdlOZO12rQH1VlS006S2LS9wPHILEuizqx1WlOrllV0GnZ318d79zoXx23WZVmmI9EnY31zikK369qMESPVduM0aO3o5rYiL4Li3p3O9IUuWsrastRtYm9vX9YOxdzWyzN9fVmmo+erSreH8cT0v3R9W0lTs0YyC4XReCJreZ4Hzl2mz4Yuro4hNW2361ytC3snCrw3nX1bDOH6rDlH84jqSI859smayaFL9LjTmePyXNeKVPe98xf1O8/d73a3rF2+/Q5Zm+7syNrhYv2YVJvxdlnpu7mouqA2lLb6fqUj/V52drota+fOn4+Og9+UAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4I6cv+ojLcOiul0cdRIYZ5qaWixiQGPzXS7S12V0t42OYWxN/HWb6uPi2sR3R0Hp3Tay056n+dDWRpy62FFzr9PNY9v9XdHcabho1Meb8ZaO9BxNx7I2mejjEhN3HPok8twNQ+nG8fLzQx0Fv7+vo5yTxFxbqqNaWxMvn5gG2tm4dDPGmVh6p3AxuyZutjGx2c1SX3vUdBv/SUhsO5+uuTvSmdYXOq+18ekMBP58wmKsG3P9iXkasYlejhI3jq9vE6WZMxaljnSfm9qy1H29afRx/V1RirEeB0aFjkRPTV9vat1POnNfGtMfGrPGWC71fYlr1wPXf59rCq2IpO8lje6Zdhwwtcpcd1Xqe7k07eg47r9yzVS7oFpR6DZomlJUjPXcfcbc8SRfH5dejPQ6QSyn/y9dzEc6mj0186xr71FZB7WXhYhm7+3vH+rawUzWxoVbW+ko9UbMlysuuj1Zv35KTL/MzJprNJ4EtUv3fGqzRoqT9sTfLW/EvSe25l63XXPi47h7J3JzsHoX6exLipu33XHufum+3prn57pzW+uxepTr/pVv6bHl7NmzsvbUpz5N1m6/605Z25psyVplruHa7sHan++Z95O5GeOW5tHZdZxZOW9vnZG1O8w9ue2226Lj4DelAAAAAAAAMDg2pQAAAAAAADA4NqUAAAAAAAAwODalAAAAAAAAMDg2pQAAAAAAAPD4Td9zSXk2Rc8l5ZmkDVfLTC02tUQkAaStSdaSFZ+a4BK5bB6LS7AwsSvub9iP49AEi/jEk+1O3tDnEZam81gYj3RSSmFSpPJCDwtppvtKbRKtXPpUFm+e0LQ/06k59937gKwtF+Y8TOpWYtqZS/cxgXdRNtL3eWdbp3pcOKdTMba2dbpPaxILr13VCVLXru3J2mK+3DgRrkj1fXZDlUtysaky5rjY1Vwqzqn19c2T0VbM+O+Sf2ozw7nkSBdDWlfr0xqX4ue9RaXb0bLRx5Ui6a+XZXqMG5u+Nxmb9KyxSQ4z98utBxpXq/WzWyz0falc0pxJS0pFEnGem+S2zCQs5foZNK0+/06Xoro2KVeB66fjuLKrx0fHrY3HYzP2pLoN7tQm9dmlcIqktsIkybnzd2vjIjfzrEuwNGldtUmPnc8WQQl7e3u6dnioU7JaHYAYLZemz5p23bmUVPHsMnOfM9MvM5HEuKqZtWFk0sH9e5SLYTudedYn2br3uvbEa51J9HOBeDJ9z9xP+0Zn+nNk1uiteXwuHj1Ou5BwX5sqfuHSRVm7+867Ze3ynXfpz7x4SZ+MOc9r969P2OvdL9bUV3Z35TGVabNZ4RJN9XFb2/pe3n7XHbJ2513vJmtnz56LjoPflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwOBMvufDxS7a0dRcbKyrxYG11MVaqvNITW670Zm4Uhe/G2UmOjTTjyRzNRELexz2+ZgIchdP7kJJ48QFlm4e3/p44tLVT0tq7qftX/b5OWGRwMuFji9fLNbHeB8ezOQxBwc6AvrK1dD4bvMAzfjhjts+uy1rzSV9x4rMRGqnurac6/tybU/H1x7s69pirp9dU20WZbxi4mtd/G5kxoHYjRF2+DDFsCnj9MYzG3HtosbNHBY4DjTi+2qT6d647Ou4DWouaa7PfzTW8+W2iUmeTib6+8zJdObaW3Ptrn91kR4DFwvd152qWj/eLsv1P++lhYkLN3HnjZsV3HGNabOmrde1yy4Pt1jqZ5SaNVuR6TaYpFlQPHvT6jZfVfq+1aJWm0eUpWHrcHttZk1dLXUb3N8/lLX77n1A1u695z5Zu3p1d+N+0kszHc9eVroNLsvKrJ/0fcnz9XP+eKTHqqzIgt6HzDAWNa4761IUu3elU/q1CTfdtGZ8cfOzG+vcHOx05kRbcS5uCZEk+vPiSI9HsRlzlou5Ps6Mf5OJnmcnU912L1y6JGuXL9+ma3feJWtnz52XtXwylrXFfClrB6a2e7h+vNo91HN6at7xJ2ZdsrWlz//C+XOydscdt8vaztkdWYsC3uMfdvixjgYAAAAAAAACsCkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGZbNaHS1zEq6uZWOnURI9mLmbdfaY5F5c8GnSQiQVvbCyiiRw1uaqNqbWmFrskZJMfmrjI30xffG4if91zjc0eaRwQM+nizkMFpqRHXXu8mMwQLra4bsJqrYlszhLTZxPdJkoTN16KGOjFsgo6xzjW8d21y7828rGOgJ5MdC3pdGPKE31clo2D4rZdrPtsX0fIH+zPgyJx1ZgUuzHaPALbg0xXdzHO7gs784XD92Z/HW6sc/NzZKPbzXhs4q+jbv04UJgY8qrSccddpOOOO7dOMPPzeOLilXX/mk51v4xj3SpaE+3dmJz12tQ6M182rs2bcacR85RJSY9a21HcesxFwZvjUrMOMv3Z3cvjGI11tHme63a2vbMta+OxbvMjU0uyQtZqs2AtxTOszbO1831u1nnmuLrW83pZ6rn7YH99xHrvypVdXbu2J2uzmZ733Azg1uK1GQfK2qzJXH8WfSwrdNvLcz0Wu9mysWtDt35yc9djwKwR7QmZ52DXBObexJ2pJeZk1PrKvqSYOd1cgGt+iXkXdPNGmuqx6uy5C7J2xx13yNq5ixdl7cy5s7JWmHV61ep542A207VDXVuqd7PY7ImY/rxz9oysXb6s78nli+dl7az5TNdY5nbcvDF+UwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAIPTuaAbxB3bWmJqUWDNfKbNtQzgzsNJzTk2NoZbR0KmWVjNxSS7rE8XCZ6a6Ep3DYmJA07N9yU25jQkCn5YAad/bHMTVzpf6NjOxXwha822jgTOC/NsbVS3fk511Wwcuxy7/uWixjsTyWzimjPT94piR9YmIxM9P9a1iallcbbxveztHuzL2rVdXZvPlrJWiAhb9fNeVc3DYqVdXzeRy6GB1I+joeX/Mv3BDD6piZxOkibsvuXr577xWD/3rtNtejzS/aurdax0Z85xVOh7Mhrrz8wS3b9a0yi6qAlrS50+z64zsfStOa41axNxLp1ZB3WmfXVJWK0xf17qenPrjrOR9eGe9KQny1ox1lHjOzs6cjvL86A/Sx6buSEv9Lmk2fo2nxX68wozf+VmDRh3ui/UTSlrs0Ula0tTqys3N5ixMTXjlXsvMGOEO86NH22ra40Y59wxriu4cdN2ITsnuvcoN26ejqYJG4+74LXE427B8Gid7rORfd/T7X062ZK1O+68Q9buere7Ze3shbNBexGlGVsK07CbWtdmC/2udLjUNdU1R2a+2NrR9/LseT2X3HbHZVm7cE4fl6b6Xu7t6XeCK9euRsfBb0oBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwOstxg6jF0JrjYkk7E3Xa2cDgAIk5f3NtNjk0MB40CbzPaWIitc25pKmJy81NRKj5vsR8ZmIiolNTk1zsbWCmu7tfjzeHcx1Jupjp2ny+lLXSxKOaJHXbKw8PTKzqwWztz5eljncdTyaylhcult6cZaKf+3RLf9/5czq+9sKFc/ozp1NZ6xp9nof7c1nbu6ZjXA92D2StXFaBIcjxxuO3EwdHTptx2rbMx1dft3OpzbGudcmNkbEec2PzMOJ4/YdmZuyfjHW/bBpz3Y1ewmQm0jgz83pqIt1b05Ya0y/rykROVzqevDJx9k2rz6VuAttut/4zY3PdYWHvfSy2a0Mmlt58amOivetT6s5PevJTZW08HsvazpkzQX9evKz1eBybPlvkuo8V4/VzWD4ab3xMLzNjVbU064tS94XFTM9tcxPN3pr5Js30PRmZ61NzWy/Ls6C1sZ2n3PDebd6/Wv+CEihsDn4sdLVuZ5YZ47s27DNNl/UL5wG5dru9rfvJzjk9xl267YI+7syWrKWpPpdlWQWNEdlI12LzfXPzHjJf6HFO9c2JeZe4ePGirN122yVZO3tuR9aKiX5pqyp9bct6EfQMjoLflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwOB0dukGcdQ2qtroTLRvFxhnGho3rsQmdjk2sdLuPPw5Bp5/4PNxjy5O9J5laiJCUxP9nbrPNMfFpqa/Kwp6Bi7a+2ayt3sga1uTfVlr79afmZm4dBv/bWouXjkfjdb+fGLid9PMxS7rs8xNlPN0qqNan/pXniJrd9x+u6xdunwp6M8L7r/vAVm7553vlLV33nOvrO3t6bbSNPpeJ6Y/5yKCfDTSMdztXMfo+sHKxcuHxnCbeeaUEq7dWO3OJzbX35q+4mLD4zYsxjoRfaxITTR7bNqEH8hlKTfjQFvr41pz3Z2ZG2oTM75c1LJWmVjp5VzHK9fmGpw01uNcmq5/Dkmmn0+cmmWk6XuxOQ8XQW6GHLtmsePHMTzpSU+WtaLQkdvbOzqquzXjkosat2O1uf7JeP08m2XFxm3lRmvj6lDHic9n+tr29meydnCga1Wl+15iBrLJeBLUrot8HHTPEjdPma6uxis7jnWDdxPDre1P9n3uKP0kNufjR1wz1kW6DUadee+JzXmqtuuerRlXXKk1H5oWuk3fdsedsvakJ+t1cybWjr2r167I2jvu1Wvc6XRb1lLzfWmux8DZfC5r85kek2Ixv50/d1Ye88z3eS9Zu+2Oy7I2mawf23tdZ9YspR6Ls0zP3We2z0fHwW9KAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcCbL9+hR1a7mciZbE11pkpdt/HUY/XmdibaNWxff3YVFjbusViN1Gd0mitvFgKapiWU23+di4mWM6Y1OJoR5Pj5qNrT2+NKaTODKxJe3pvM15jNdBHFjEnHLcvPYcxvp67qCiZcfjXQs7JlzOk720uVzsnb+go79LnLdv3b3DmTtgSv3y9o73v52Wbt69WpQbHZu4nLdeFWKSNm61pHgZsi5ATcWtzfNOBDb8b8JyvFOzBwWhc5TZmzNxLkk5uGmZs5o3ThuFhG5+cxFW8pabfqCm1FcH1ou9Bi3XJRBx5Wl/r40MRHX7jkU6+Ov00LHSieZ+y5dy81xSaqXpl2rzz9L9f0yzeFYJuOJrBXifvZGk7GsxSZePsv1s2jMvO7GrCxLNu6znekNTdUFtffZoY5RP9zXc+L8cB50T1z73JrqWhzr+5KbZ56b73O/I9C6kacNmaJO/n0ujs2asnXvBOZEA9+HbqTrzDW66TL0CxOzsOmaoMuXp2meXxPYxvJcj8e333GnrN122x2yNp2ekbXdvT1Zu/e+a7J25f5dWUtvM/ONeWlwa7JyodeyBwd6vGrEO9b2ln7POHfhoqxNJ3oOiky/XC71WNxUev03Hk1lLc/NuRwBvykFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDB6ZzHIZikbpfi3bmtNJdmKgI9XfR1bOKobWC4iZFsWhe1HRhfbq47MXuPLv7VRke77HYTr+m5ePKQMNbTiJM152jiyR8LsXkOicvHdhHyLgbaHOVbtYuzTzdut12rzyQ2fWE00hHd58/tyNqZHRePqr9vttCRsbt7OvZ2b0/H3u4f7MvacrmUtSTRU0GWpUHjVVWvj2evGx1ln2VmSgqNcT6NaGwXY30MceLisV2/1MfFbswy1+Gu336maBONOf3MjVUurtmcv11DmEZR17p9liZC2dZKPSY1tT7Rxjzz1IzT6UjHX2ep6eviuGI0kseMxrqWF7oWmTEnjvWYU5jxaFQUsjY113AcV69ckbXC3JvKzFOjsY7VzjJ9jcVY14JmdTOpVwvd3qtK1w4O9bx3cHgoa4eHM1mbL5ZBa4+RaRN5Xmy8LlkxbTczn+nXzfbFZv0R7p3AjB1ubehnPXeO7inoRnZaK+rGnE7b6fNpTZ9tzfq/MRdSmr5Su+/rNh9zt3b0Onbn7FlZO3PunKw9/T3fR9bykR7H/vyt75S1t/3522TtypUHZG1nZ0vWbrtwhz5urO/LtQO9Fl/O9JjUNvrZ1eX6tfE7336PPObN6ZtkbWfLzW2m8Zn3eLf2cHsYbs47Cn5TCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAgzP520ePCY1dvLyLCbVbYqcVBroZFx3trs3Fg3auZs8mLMbVnUuaushp84BMpOzm4bXv0pjc1NRGym5+3f5OPz7a3nHleR4Ud5xlelhIM9dewrp6ZyJ4VaxxbPve5m2lNxrpe3LmjI7LnUzGstaZdru/vy9ru9euytpiMQ9qu6mJf41NfrSLlo7NQy+S9e2vs62hPfE/QnHXFt7XQ0c5zwyrUWeuw05T5r7Fvmea73Px1+trSeg5diaivNPtZbHUMfHlUsdwz2eLoFj6qqyD7nNs4uWT1IzFpg0Wph9lZl4YjdbX8kIfU5gI8jTNg7p6a4qxiapOTQeaTI4XVa385dvfIWvjsZ4bdg71OH7mnJ5vdnbOBn2fm/NVV7ex5t36WPPecqb73vxAX/fMRKzP5/q4qtLnEse67+W5bhPTran5TN1nq7oNmoMTN6e4AV6UOvN57p3NXVvwrKc/0g4Enenrp6U1815rnkNr5iK3JG3d+irTN67I1vfnrZ0z8piLly7L2oWLF2Xt7LlLsjbZ2pK12aGeZ++994qsXXlgV9baRt+wnS197TvTHVnLzLxRmDXu1MwpZ7e3ZW13/3Dtz5czPcb96R/9sazl5r3M6RK9vmhMe+7MvsH2WX2fj4LflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwOB0HuAjJCZCNDU1e5yJak1cTLI5zmZqi1zO0FBwG5l9KlHjLrbdxCSbuEj3DGx8rXmu4bqgSFV5jIm0PM5TV1yztKdySvJCd+8iD6vlpl+6waQyefDVUkewN42OWQ9pm0WhY7G3TZTu2bM6hnsymcjazMRY7+7q2NurV3Vc7mKhP9M5jT7rx51s8zjtSkeJRy7G2kVm277uxlsXm33y48fqc13ydxz4bJOwL2xbHQff6VIUi3uqfr76PHPdTW1i6U3tUMQu3yhe/vBQx9KXZRn0gFITvRy7+dnEUedunDafmY/0GDiarK8VRR50jq53dWZS7xrXEXSpjXR7yMx5Hsef/ulbZG1s5obzFy7I2gUTDX7psr7G8+fOy9rWlh53UxHx3ZqO3lT6+ZVlJWuzhR7j5zO9FliavteatpQXOrZ9NB7L2niio+5dy+7M9bkxsDWDYGOur1a11nUU8+7l4uXdPBO6xjXvGZ27hlOStOZ8wl7P7AokTfR4nJoxfmt7e+3Pz128JI+5/Y7bZe3S5cuyNt3akbW20n29MmvVzDSlM2fXX1svz/VcdOHi+aD3ocjMG6Oxbg+XbzsX9P583/3r1/cP3P+APGbfvC/UpoU1plabBi3Hlf6ebOl5beecfo86Cn5TCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg3MZiQ8TmyhQF0edJDpyMDY1m17uPtNElHfiuLh1UdUnH0nqPjI4AdVct7ldUWJikpPY1cKeqwtH7Wym6vrjOvd5JtIylLklNwrAjoamYp5XNRP7a9PlT2Eb28UdK1mmh67RWEfGnjunI1wvXrwoa2fOng2K9K2WB7K2d21P1q5euyZr85mOnD48DIujtmO4jVI3UbQiZTeOu6DzcN0rDu1f7jPtcHRa/Tnw3hipGY9bE4Xcdm3Q9auzdHesqmtZK03E+mKhI+T39vdl7fBA98v5fB4UUZ5lehyIcjMWu+Ev02NZXujI6WKio+7zkT7P0XR9bVzoz8sCx4e6CWtfsXkGrs/m4yMvdzeyu6vb2WJZylpr5j0fIa+vI01Ne8n1cy9ErdXDQ9SY51fbmHjdn5dzfb/KpR4jqkrfscL0kzTV9zIx99m1T1erGzPOlebaS3M/xb32/csOOrISm99j6CL3mW6MaAPfJU6He6ewfbYLu/o41e3Tjbtb2ztrf37+3Hl5zAWzxr148ZI+j9FE1hamP7e1vimXL+vzPH/2jKxNt7dk7eLFC7K2tTWStSjW/bIwY+o5814wLvQ9y8Qzr5d6PbP7wL2yVlX6/M0QHpVuw8H0ven2VNcKc5+PgN+UAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4I6ckZsmOrYyMbU4TgJrcVjNZHyr72sTFz9sYm9d5KqJWnS1yNWMJPSeuEz05BQ+09bMPVM/D7xfTwRx3AXWXLSvDhjtzB53qxNLo9REiqtaluuo3AsXdEzrnXfcJmu3364jcbe3dAyt4+LlD2aHsjabzWStKl3Iq3mupj8naWC/DOp++qDERUDbMc6IT348cnPQcXTuGdnvdDHWJi7YRIO3pubiv7tOzLPmGBVr3pvNdV842NO1w33dv+am75XmXPLMRMjrZVBkUqXtcVmmn3kx0gduTfT4mI31yYzH6+OcC3PdbhAoGz1Wta2ZSzozxpkxJ06SoHXqceR5HjQ+Hs50/Hdybc98n362o7GOIZ9O9RwWR+vvjRuO3SNqKtPXSz0elUvd98pFHbQWd9yI2pq2W9W6tljo5+q4qXRq1gPq+6pa38vWvNdEZtwP/RWHxH2dufDTWt13gZ/sp+DN58SVJuy9eyzGga3JVB4zMeNDkRf6u0Yjc9woaN3v1lCueZ49d07WLlw6L2uJeeeZL/ZlbbHQ/agz40BiLiIXc1/SuO/StcbNs11g+5rotnL+zI6sXTh7JjoOflMKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDy478L2Y6OjCJXQzvyUdnxybI1cWeqwDY2MWV2jNpg2ouhtvVbHaquWyb9v440rnsyiBhFx4nJx9E69vl6UhcPLZpFPZck8B4ZZMfvVwuZa2qyrU/n27p2NszJq700uXLunbpoqyl5p4sSh3VWpsY19Ic1zS6DWapHrbTNAl6rm5MbRqd69uZ8Uq1vzRNg2LibR6zLcWBfy7ThUVDH0NnxnhXc+dTm7jgzuVRmwjlJOAGuDa9XOo2tn84l7WDg0NZW8x0NHtlIutTs54ZmWjs7e1tWStGOm7bfF1UFPq40WRsjtN9LMv1+FFk2cbjipu3086MK+Y4twxy/TkT57+qmWs4jgvndAx5bWLBy8rNG7pfzhd6vpwtdJtfmuPGItY9NmN1Y66tquqg85jP50HXvSzXrxN65UTfy7x0c9si6Pv29/WY1Jr3gsrMs+Ox7utzMT4ud8qg55Plua5FYdzS3q223ThwPG4s0PfNn+0pjC9ujBQ3Z2HGgL3dvaD+nCW6TSxm86B5PU31Z6aFbmmdeZdoAvvXwryDXLum79ne1WuyVprnsH91/WcemudjXueiPDPzs5kvR2O9Zjl34YKs3XH5kq6Z96ij4DelAAAAAAAAMDg2pQAAAAAAADA4NqUAAAAAAAAwODalAAAAAAAAMDg2pQAAAAAAADA4NqUAAAAAAAAwuOxk9q905KBJmQyOv+5cLGdAhKj7rtbVXCSuifRtah1N2Zj4bh9HaiK6TVykOy5xOa5p2PMJjTXH5lyMd2qinnMTq51E+jjHxXHH5sHHyfpabmLNp9MtWdvenupzzPR51CbO/mD/QNYeeOABWStNDG0Sh/VZF5eeJmlg3zOx7m64EoOxHb/tH5O46z75P3uJfVh1dBpcfG9j8rHd9buo5yjugtpLYvKJm2b991XLSh4zn+v45NmBrh3uz4Nins1wFI2mOn59urMja9tndLzyaFzIWmbG4syMc5kZp12HLuzYEsA0PtdO3Bxk1wmmP7te6ZYzx3HnXXfI2nxhosb3DsKuQ/SvXlOatWWlB+taDORubq4q/V3LRSlr87m+JzMzDswXuq+Xpf6+xVz359T0hTItg57r3v6+rLVmDK9qPT6Ox/oaDmeHa3++MOdYmbaQm5pbXyRmLnEvZuaWRGYYOCb3/jKs1IyRrTmb+Xx9f7h27Zo85tD0ocLMJ22rn/vVK7v6OLM+3NrR6/Stne2geW/rrP7MqtL96+pVfQ3333OvrF25V6/v61KPZZUY5+azg7DNGteGxDtUb2ui1yXnzuh7ef6MXged29LHHQW/KQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMHZlMGjRvR2Jgq0M3mfKoa2F8eBEfI2n3h9rTOR2XWtz7EytdrEqrpoyqbRMbuOi17uOrP3aNPZzX1OTDx5oq+9M7nM7hoS8ex8lH2o0A81/eC08qiNLNOR26ORjgJNTQz5adzuYpTLWl6sP8/JZCqP2TZxsu44t0VfmYjrK1euyNre7m7QOODFpxDBXgeN4W1bb36aiT6P2EXbuph4V4vNWBUYAO3mp+Nw86w7VRcd3dox1/VoXXOnWYtY+tlMx1Hv7a2PNe8dHOho5dmh7peVyaPeHo9krSgmsjYe6fFjVOjY9pEZ49w47edE01ca/YAa11fq9TU7fZmu4NZjsTkwid3SVH9m0+i+vlyGjrfezpnzspZlus2XtVt31kHPPXLjkjuu2/x+VqXuezMTPT8TUfa9QzNGzGYzWStL/WxHo3HQu4RbXLprONjbl7XWzKVNra9hx8Ssz8W5LBcmkr7Wz65pzNowNe8ZgVOinfJc8Rha9+5px6Wwrte5sdq8f5bL+cbPcG9/Tx4TmzVgYtZlXasv/PBQt7NRrufSzvSv7bNn9WeO9NyduL0B05QWc90f7nnHvbL2wD26Vpv+Vy8XG/fLxEyJnWmYWaKfwbTQ65IzE32fx5k+mYUZ/46C35QCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDgXO7ukaM5Xc3FeHcmCtPFkLuYWhdxHYtYS/d59jxqHStamXhXFwVfm+McH/eeBUXiunvparWJbM50AmVkmoPcP01TF2ke6jQ+83SibZ0819G+eWFif22MdyCTKZ6ZmOEiX9+uCxNlWphri000b7nQfe/wQMdR7+7tytpisZS1xsQSuzHVJXu7uHCXfu3GwLrR96VpdXR5LMaIxGVH23mmDcyVNseZnOAkTsOixI8hdJ5159OaOGcn6fT1t6a9lMv17Xo+1+1ofmji5We6Nl9WQX/iFk/0+JFnOgo5z/TYkiRm3InC2pJrgy5evjP55G1rauJUzBAdRaadROb83bgfRW1QhPpiadrRXEd0H8e4GMtaZ+a98VgfV5o1YmbiuFO3/pUVPbbYfm7Ocb7U896i1LWyWgatm6tKP/eFiWa3TL+cz+e6NpsF9Vm3RnRtdynutbtfTaXXHq1bl7Rp4BK3DTzu8UbfG9vDYjeeuXdk/ZFdvX7t1Zq+4MajNjHvkGbOqGt33VlQX1Dr/t5koufnSa5rXW1upnuXn+v7uZzpftmZfpSItlJkaVAf6sxqJzNr3KjR6/dypse4w2vX9Ece7EfHwW9KAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAODxm77nIxLCEvZc+pRLyXI6l3Ij/sZ7l+ZSm3NsTCpVXYYlhdQiUeFG3DX41Kagr/MpbK7YuBQ2k9TQhqSNmSSawbdjTyety0nN/XQak8jQtC5RQitNqmRrPlM13cr0k8PDA1m7dlWnc8SJ7kP7+zpRYmFSKkrT113ineuXbmiMTdJGa5LyQpMAXR9LxYmagKioM+evxu9VzaXvWTaTKrAWzo5Z/sCgcVUlJK5qnUlYNUE2pWjXy0UZlL43N+l7danb7Xg0CkqMM8NftFi4+VmPA8ul6ScmcCcx6bKtaRGJmeASk4iUJutrxUgnCxYjnWSY5C75151H2DjWNfqeLMuwtdWNuLHzNGqxebZuXvRz0XjzNZlb95tkLZtAZ8Yx1wbduOnOxa3F3ZRSmfV96wYQN/eZ++nG/jhkbT/8clQSQ87/dUopt2atZ7/RhSe6tYtpSyb0zrbBJmidF/Y+3pn5MjFptYVJyjtz4aysXbx8QdbOnTsja9Ox/r7FgZ6fG5MM7NL3qqW+13my+btZ7NKXzXtSG+ta5taq5r1sYVL06oW+l7Fpf0fBb0oBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwG2TGh8Vj22jY1sVT6lriYrNtlnockPNpYntNRGNjYhFrE9tbVTqi0YWVuth2x8fL61ptoqojE8vsYt1rc68zkUjati5qO+w8wsPe46Do9dPS1k1QzOn+3qGspSbGOzGNaXf3QJ9L7WKZ1z+NslzKY/Z2d2XN9fXOtL+DfX3+e/t7srZcLvSZBMZ+qzjZ1XEmX7gq9bnUjb6frke4PqZq7pjaxYW7nhkHHmd1g/95TmdjvMMyvl2bcPNl6+KoTdut6vXPoqxMNLv5vNacSGduWGPmhrLSc/DcjI2uTczn+vsSNxelJko8DmyD5r4kZvwoRuO1P59Op/KYaadrY3OOWW7GsdhEkJu5NE5yfZyZu47jcKbny7mJzp7PdK02a0unLvX6sSzLjdfiSaG/K0v1Myoyfa9Hhf7Qrck0aDiuzLW5tXHoO4hbz7l27cbwIs+DPlM9hyw1a4hM18x04acgx76WmXM5pWVzHLwmCF2fpEHX6F7rOnEumbnZSWrGR3dLYn1cLuaM3tkL52Tt8uWLsnbmzLasJZGeu3evPaBrV3RtfrAva7F5Xx+bca7I3cv1+vG9bcxYpT8tMkuIKDFrq25h5ovOzGtu3Cz1vHYU/KYUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGd+SMXBcdHVrzQiM7tVhFZdrcUV3rAuNkaxMx6WruVrroaHdc05j40MREyja61pnjOpddGcDHQwdGqJujulNol6dld1fHnCYmotb12YO9A/2FJoJ9f1/Hi85mS1mrRZxpWepjru3vytqinMla0+h41IWJnJ4f6murTPS8i6oObddJHBZ/7Wp2BHdjf7q+FmfmE/UjsBpzL2MTppu5McJEVZ8WN77I+ev/VuVn+gcoK625p03t5rf1fbY2bcyNqqmJXY7s/GWuzZxLuTDjUVVuHPO8KiUm0NlEsCd2/eTWXfpD02Ika1MRSZ2kJpI+K2QtEXH1vdasE/LcRKgneVDkeZHr6z6Oe+55h6yVpr0czGZB9831hyQz97Rwse7ra8VIP9vJVEfB75zZkbXbLt8ma6Ncf99ysZC1yszPbk29XOq+fnioo80PzZyfpmb9KytRNJ5OZW06mujjRuufw6jQz6co9H3OzTPIMjNYdXr8a83vPzw2vxmhz9UOyKdguVwErUHUOj0242rirtuMOVGia8VEt5ez5/U4cOHSWVnb2tJjdb3U4+bhnu6XD9x7r6ztX7sia615L8jFGreXpGZ/oF3/ma34ea9rXZvVzFInqiPd9mLx7rU6rjF7GJVpz0fAb0oBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwJmv56DHxLqraJhpHYVHIoXHpOqpVH9PYSE5ZWoWgyorJaGxMDGPw8zG1pjHRwzbSUofbpiZKvGtMLekCWqprl5vHqa7OozMZmjeRg4NZ0HNoljru89poVx9X6bY7n+uY0GW5eX9Ymv4V7etrm830c69N9GtjIlCjJqy9uHjozvQhpzVtt3P3zJ2NGecSExUcy7hce+VBtTgOO393R9xw5D7zWDozLsVhpxOHfl8bNoep55Saubko9FKk63Q8dJS1QdHsJsnZ9pNlWW8c87wS689MzMnEgeuP0UjHy6d2aFn/fZ2ZS81UElWmGJtxMzYnmZoWnee5rI3Hk+g03PPOe4L6SW3WetNt/fwSM9ZlmR6PR4WObh8V6+9bket+ORnrfnlmR0fBm+D56MyOvu5lWcpaXeq+V5nawcGBrD3wwJWwP843/bk18/rI3M9ipNt1Jp5Rmus7nZl5OzUPyI05bsni5pKwN57j6cx4bI9zz91+ZBNUc+vATvSkpDXf5eYa9/zMObr2cv7ctqxdvnRB1qYTPe7sX7kqa29/61/K2n1vv1/WDnb1OFAt57LWNmZMqsxaIVqu/Xls7rN9Pm4u7ZqgJmvXxub9366bj4DflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwOCyI/+Lmf5Xk8REL5u8yDSLTzyWNCwaOw6Ka7aR06bmYnvb1gXmRid+Lq7mnmviHsKp5aVvKj6F2jHzLgdUmihkl4jbmRhvFyVcmcjm5VKfS1HoqO5EtM+m1nGri5mJxDV5pa2L0jXRvG6Ms6OO7UPmVEyUeGdyY93Xpakb3038q5lB1Pe5c3RRzV1w3ws8zmXwntIY15nPdZHbiTkdN1b7e2rakjlK9YfRWMeat80kKA49NV02NvNXZ/p6XeuxKqp0+2wrPQ7Esa7lZj3gxgi77spchLyu5UWx8edF7j6bhtJ25l6aCHn3fVmhz3PsOskx7O7uylpi5oY8X3+ve6kZWHNzjcVYf+ZopGtZnm28PiwKfY7TbdOfC/2ZdTmVtbbSfbaqdJ9dVnpdkj+g7+XBTMfEp4dh6/vGtHn3ruHm7k58ZmMi6StzT8qle+dx16a/r2nqoPVm5ya96GIUzoy55nc17HMw39Z0uu26sbWLzDgozsWPcvosm0ifo+nq0d13XJK1PNHnf+873iprfzmfydrs2p6s7V/Vtdmu/szFwVzWWtNXonYpS12r23wUrR+vMjO3OXFs2qw7zoxHkVkjJeZTzfB3JPymFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABmeCHh9uOh0HfYGLZU5NTLI7znERmrWIHnWx2C62ODXZh6OJvl9ppm97U5uMayM1cceJifXNzLm4+NfMRMi7WpK7z3RZkpvHObtY6cTcLx8g784jNLL+dMTmXFsTtVuZKOHaxP7WNoJY15JE14p0tHm0rYky7Zp242jlG0Wz56a9d4GR7i4KuTLxynHchUXIu7j02MUSB9xP2026gWuncS7hgj/VtBf/meY482zN1B3l+fp2Nh7r6Oss6YLG8bR16wQ3/ukxonZx6aXu66UZ4+z4kev7UphaZPpzaa6hGKkxNYqy0frvS828nWQmjto+nrC5NDVtxd3nbFREQ3PrjML0h0I8h17u1mx2XaPbfFOvb7u1eURuvnTPKMn1+RemTbeFW9ub+1Xq2nKho+CLwpxnoc+zasz614xlmVmnJ3ZpvP4zu8is1Zr1kfS9qtLn0Zi1R2tqbs3i3tna9nTm2TzXY2Bnv1N3iMZcR7Mog+ZSJ45Ff3ann5h3InNc2i1krZ7tydq1e/V179+r29nh/qGszXb3Za0tTTurzFrHvDOkZgzMzTtrl5o1jfjM1D07szWQuJM0v3vktlmSwPVvFtqgH/xeAAAAAAAAYGBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcDq79BHG03HQF5jU2FPioqXXZyq2LibeRRNnOvpwYmrR2MQ3ht4vF8Hr0t5NJmRi8pzdcbH5wtR9povEddmVQW0hCjp/Fwnraj5m9nRkJjo6M/czdfe609eYmDZoTiXoGmy8dWOiXwcekDrb0MLO041XTm4irjMTbdu6GOTOXUOz8ZgaLuwzfeiti55vH2d/TuRitd25mmt0sb+5y4pf386SVK8h2irbeN7upe1Jzxn9+KG/r6pM7Hmlj4tjfb/yvDA1HWcfmflysdAR3lmhP3M0Xh+Vnhf6HBOz1jHTvdWZcaUx7cHc5ihNjhdVrdxx2yV9PmZcdc9hPNWR9YmZS8tKP/eDQx3d3rXr23Vh2l9r+klTVbIWm+cXufnEjEedmRNdv2zNPJvl+tmNJqOgJWJjxuLM3OvRSNcyEQcfm+/qav18ahMvH5uaGzddn3V9/bQkJra+M+vYyKzj3Zo0Nu2sM+tV+0KYbD7vxWZ9YYYq25b2rz4ga9Voor/PbD8s50v9mQtdi83Q4q49MZ3WrYPsGjHkdb0L3gAwNbe/kYS1Z3e/jtmd+U0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAM7shB7amJH7ZccnRgEmZrYhhd/LWKUm8CYxgTkz+cJCYy0eYkuyBJJzCH0Z2nORfXHmxcZOBxSmsiWlMTX/tEkJjsaBePnZiaiza3edx1HHRYmq0vNq2LJDUxrXFYlKmLLW7NgS4K2Tf3wEh01+bdsONioM1hkYkzbtUgboeqsD7bucNse9CHmSb2GAmNC3Y33I2fyYnWskyPK+2o1mdY6/aeRPozU/N9oeNAY+Lla3OervNlWaFrJgreRX+XjYnNdmO/uGepyQs/jUh395lNU4d9Zno60fNbO9tB40uS6eeXFy6fPQq6N+VyIWuZOFF7r8V6ute6Oapx/cQ8I7PWa0176cx5RuZ9wY1xeabXVm1uxivzfVlu1mt23ax0YesZt2Yx3+bevewUFDqtHYc5n8ScT+AyI+pMP7LvKXY5t75N+LdLt85Lgmptra+t7kpda/VxTWnul5tL8yRoQef3AMz60TbezRt27BqY7ZdOEniO+rjkFOb8G38rAAAAAAAAcErYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4OLuNPJ8AQAAAAAAAIPflAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JR6Avjar/3aKI7j6P777z+xz/ycz/mc6ClPecqJfR6A/4c+C9xc6LPAzYU+C9xc6LO3NjalNvRd3/Vdqw5x/Z/xeBy953u+Z/SSl7wkeuc737n6d/rG/dB/R/3Tf5bykR/5kdH7vu/7RrcqdY/+8T/+x4/1qeEWQ589vgceeCD61m/91uhv/I2/EV2+fDk6d+5c9KxnPSv64R/+4cf61HALos+ejL5/fsZnfEb09Kc/fXUv+usFTgN99uT81E/9VPSBH/iBq3v47u/+7tHLXvayqK7rx/q0cIuhz56c/f396Cu/8iujpz71qdFoNIruvvvu6EUvelE0m80e61O7qWSP9QncrL7+679+1fgWi0X0a7/2a9FrX/va6Kd/+qejN7/5zdGrX/3q6ODg4MF/t//5D/7gD0avetWrokuXLj3482c/+9nRE9kHfMAHRF/2ZV/2sJ/1AyJwGuiz4d7whjdE//yf//PoEz7hE6KXvvSlUZZl0Y/92I9Fn/IpnxL93u/9XvR1X/d1j/Up4hZEnz2e/n79z//5P6MP+ZAPWW0sA6eNPns8/+W//Jfokz7pk1Yv8v/m3/yb6E1velP08pe/PLr33ntX9xI4afTZ49nd3Y2e85znRG9729uiL/iCL4je4z3eI7rvvvuiX/3VX42Wy2U0nU4f61O8abApFej5z39+9MEf/MGr///nf/7nRxcvXoxe+cpXRj/5kz8ZfeqnfurD/t177rln1Yn7iYZfEfx/+p3k/k9xgSHQZ8M985nPjP7oj/4oevKTn/zgz77oi74o+tiP/djom7/5m1d/QrS1tfWYniNuPfTZ4/ne7/3e1TybJMkt/yfVeHygzx7Pl3/5l0d/9a/+1ejnfu7nVn/40ztz5kz0Dd/wDdGXfMmXRM94xjMe61PELYY+ezxf/dVfHf35n/959Fu/9Vurzb3rvuqrvuoxPa+bEf/53gn56I/+6NX/vuUtbxnsO//X//pfq/8W9mlPe9rq1y7vuOOO6HM/93Pln4j2/w3u3/t7f281wfWDTj/B9Tvjj/R93/d90Qd90AdFk8kkunDhwuq3Id761rfe8Hze8Y53RL//+78fVVV15GsoyzI6PDw88r8PnBT67NH7bD/RPnRDqtf/yna/MOn/JOhP//RPb/hdwHHRZzebZ5/0pCetNqSAxwp99uh9tv+t4/6f/rctrm9IXf8DoK7roh/90R890vUDx0GfPXqfvXbtWvS6171u1Wf7dXL/TtuviRGG1coJ+ZM/+ZPV//adYyg///M/v3oZ/Af/4B+sfs2372w/9EM/tPpPbPoJ7JH6Dtx32m/8xm9c/Tvf9m3ftupID/WKV7wi+qzP+qzV30HR75T/03/6T6P/9t/+2+rvkuk73412i9/7vd87+su//Msjnf8v/MIvrH6tcXt7e7Xj/q//9b/e8A4A4eizm/fZR+r/1Kz30F/jBk4Lffb4fRYYEn326H32t3/7t1f/e/23Vq676667ond7t3d7sA6cJvrs0fts/5879ufR/yd7/d8h1b/T9htgH/ZhHxb9zu/8TuDdeALrsJHXve51fe/o/ut//a/dfffd1731rW/tfuiHfqi7ePFiN5lMure97W2POuZbv/VbV8e85S1vOfL3POc5z+me+cxn2n9nNps96mc/+IM/uPquX/mVX3nwZy972ctWP/vET/zEh/27X/RFX7T6+e/+7u+u/u8/+7M/69I07V7xilc87N9705ve1GVZ9rCff/Znf3b35Cc/+WH/Xv+zo17nC17wgu6bv/mbu5/4iZ/ovvM7v7P7iI/4iNWxX/mVX3nDY4FN0GdPps8+0gMPPNDddtttq74LnCT67Mn32f46++sFTgN99vh99vr9+Iu/+ItH1T7kQz6ke9aznmWPBzZBnz1+n33lK1+5+vf6e/ahH/qh3fd///d33/Ed39Hdfvvt3fnz57u3v/3t9ng8HL8pFaj/u1T6FKr+1+P7Hd3+t31e//rXr/7+hqH0u7HX9Tu1/a8z9olYvf6/bX2kF7/4xQ/7v//JP/knD/7Fdb0f//Efj9q2Xe1A9591/Z/+1yj7neZf/MVftOfTpy/0O9pH+e+M+3SR/u+h+dt/+2+vfkXzl3/5l6OP+7iPW+1m939ZHHDS6LPH67MP1X/np3/6p6/+tKn/Uy3gNNBnT67PAkOgz4b32fl8vvrfPr3rkfr/pOl6HThJ9NnwPnv9L4Hv/zqL/rewPu3TPi36wi/8wugnfuInoqtXr0bf/u3ffuR7AP6i82B9Q+uT4vr/7vv222+P3uu93mvwv7vhypUrq9Sr/lcc+2SOR6YBPFLfER/qr/yVv7I65z/7sz9b/d/9X2Tcd8JH/nvX5XkenZa+Q3/pl35p9LM/+7PRL/3SL/EXoOPE0WdPTr8A+Jmf+Znoe77ne6L3f//3P5XvAOizwM2FPnv8F/N1fydN/6L+0Bd34KTQZ8Nd75MveMELVpt51/Ubav3fMfXf//t/P5HveaJgUyrQh37ohz7qv/seWr8D3Df4r/iKr4g+4AM+YNUh+p3hj//4j1/971E2gh6qP6b/WR9Jm6bpo/79h3a409Dv0l8fnICTRp89Gf3C4Tu+4zuib/qmb4o+8zM/88Q/H7iOPgvcXOiz4e68884H/5Ll6+vh6/qf9fcWOGn02XD93/fW6zfzHum2225b/bYUjo5NqZtU39D7XxXsXxC/5mu+5sGf97vDSl97aFzlH//xH6867vVfT+x3mvud5f7f6XfNh3Y9wav/NVLgVnMr9Nn+T9S+9mu/dvUXRhJ3i1vdrdBngSeSm7nP9i/jvTe+8Y0P24B6+9vfvvprLR75FzkDt4Kbuc/2yX69dX8het9vn/GMZ5zad9+K+DulblLXd34fmUrw6le/Wh7zyP+29frfBfP85z9/9b9/5+/8ndXn9gPDIz+3/79VNOemEZr9b0I1TfOwn/XH9L95URRF9FEf9VH2eOBmdDP32d4P//APR1/8xV+8+ruk+r/7DbjV3ex9FniiuZn77DOf+czVS+y///f//mFr5Ne+9rWr3/ro072AW83N3Gf7/9Sx/yssfvInf3L1d1Zd93M/93PRW9/61ui5z32uPR4Px29KPY7dd9990ctf/vJH/bzf+e1fDPtYy2/5lm9ZdZr+L6TrO8Fb3vIW+Xl97RM/8RNXvw75hje8Ifq+7/u+1V/Kdv3vhOl3lvvv66Mw+/8u95M+6ZOinZ2d1XH9X3rX/ynNl3/5l8vP74/77u/+7tW/7/5yuP4vOe+/p59g+2vpN6l+4Ad+IHrzm98cfcM3fMPqL6IDbka3ap/9jd/4jVW0bh8R/DEf8zHR93//9z+s/uxnPzt62tOedsS7BDx+3Kp9tvcrv/Irq3+uX+fh4eGD19pfV/8PcLO5lfvst37rt67O5XnPe97qL53u18Wvec1ros///M9fRdQDN6Nbuc++6lWvWm0+ffiHf3j0j/7RP1r9HVj9H9z2v6HV/6Xn2MAj0vhwxAjN3/zN3zzyMaERmv0x6/75mI/5mNW/08d1vvCFL+zOnTvXnT17tvvkT/7kVfxk/+/0sZmPjND8vd/7ve5FL3pRt7Ozs4qqfMlLXtLN5/NHffeP/diPdR/+4R/ebW1trf55xjOe0b34xS/u/uAP/uBEIjTf+MY3di94wQu6u+++uyuKotve3l5934/8yI8c+f4AR0WfPX6fvX4P1T99HTgp9Nnj99mHntO6fx567sBx0WdPps/2Xv/613cf8AEf0I1Go+7d3u3dupe+9KVdWZZHvkfAUdBnT67P/vzP/3z3rGc9qxuPx92FCxe6z/zMz+ze8Y53HPke4V3i/v/ZZBMLAAAAAAAAOC7+TikAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMLjvqv/ie//gjZC2OY3OkriWu1ulaHOu9NHcmSRdylKudgm7YfcKuO43P1B9qHusN2pH8RHMerTlK15quCTvHuA06rikrWfuD//jfo1Bf/6p/r78z1vetNffUNZikTcKek3sW8jg3PuhhLUlzfR6ta0thHSXRlxa5ptSZ+1W3taxVja7VphaZ9pCNRvq4ZPOxMzaDgL3PtanpLhslrs26thKnQQ/vFf/sc6NQr/3uH9HF1MyXue57SaGvI0l1rTVtojJtcFmVa3/e1PqY1Pz5WJ4VsjZOddvMEv1su1p3zOViKWttGThGmHuZJKZfpu4z9fXFnb5nnRk7Q0a51ozfZriN0ix03aWvuzFjS2PG95f8vb8Zhfr/XvndQW2iMXODO9d8pPtsmusPTVI3Ga3/vjjWDzCOdBtzCz23vqprvRaq2ypoLm3ccqZ17Uxfe2oadmKWQUlsrt30IzfBJeLZJeY9KVo/RN9oGRvFdt017LvSS7/ks4KP/cnX/3T0uHmtM329dXNKu/5BdabBt+KYd31XG3acqTlJwNrxhjXT5hPTPhPTadMo7DjXH9RxsTsPN7C479JH2eNc26tdGzOriL/9wudFN8JvSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHA6W3eAuE+TGmvDUd1O2pChpCbl2d6v0Hj50GdgEkftvXTPx12DO097X8y5+BaxORdtm5oo+NZlz7vvs9HDpyMfm7h0E//aBQWDH+cRmZh1cdt8FzLtLzLP1iXNmnN0ybZpYAyt6+ougr1qdfusGh0937iHl7ooWl1SRRdJH7txpTX3q3E3rA06zrUxdw2nxbXB0Fqaho3VjZk51LzYNfo51GZcdefRmmh2N7kluR4HChN1X3Umsr6qgwZH15/doOTGCL9WcOPj5p+XmvEhzfRxmXk+bg6y04yJqr7RCiPUaKTbUm3afGKe7STWS/O00N+X5vr7ssyMg3IccK8IZi7t9MNtGt3X68x8ZqtrXRzWXjpzDVGna3Fi7kvs1lZunjJjoD4qSsS1J7F+Bq17KTDtMrFjx2nMicPPs6fC3O/WjUtmEdKKWtvpdtSadX9rxqrWrKEaU3NjbmrfiQJrps13ZqHemfHKfZ87LnbnIp5dbJ5P5M7RrtXC+vPJvnEfHb8pBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbm814fpTOSgijfsJS4vPTihNyyW1Mckr5eYc3Sf1nWB0akmJrMqS32Yiw6Nw66vMUUXl+64tpLnOt47FVHBLjo6dnuuJnrTNhMTE2xDNE17OK294dF4rE/HRY8Gdkz3bIfkTqNxqe023lUflpp+kiUmLj3VbSmREd3+XBrz7FytavWNqZpK1kIC2N1c4npJavpe0po+ZCKLu9Lcr9pFJJ9OWy/NGJ+YoScxbTc103ya61pi2m5inqGumTnKPaPAyOnYxMS7TpSYG90mS1lrujpsUDLnaZ+BOc84NjU3zoke6NZxrl26sTEx86WLSe9aHXkemVhzWzuGrR09z5aViWevzFoo0WuhONPPIk31NaaZriWZahP64XZmPG5NG2tTXavNYtV9pmnudr5x1+fWeq1Zz7WuXXe6PdRmDnbvDInoK3b9G/gOkpjPdPHyjzddYOD9aYwgbi3emrGuFeNZ3Zhjat3GGnecmYMbM666dwL7ri4r4e/4bi6N3TrdzX12jeTm7mTjz2tMza5nTJ+1eyLu+9ykH7DP8rCPPtbRAAAAAAAAQAA2pQAAAAAAADA4NqUAAAAAAAAwODalAAAAAAAAMDg2pQAAAAAAADA4NqUAAAAAAAAwOJ0HvQEbK2j4aHCzX2biWMtSx5dXy+VG0Zq9zMTQZqmJ2jY1l1TdmMjOg8O5rLUmerMxQaZNre9Xa6JKbYRmoeOMs0zfl63pVH9mtj6CMk1NNKWJDu1M/GltnoFL9k5lrHIfuVwERbEeR1EUQVHqvmaOcjfnhONyYxO32jQuTlw/29TEQ/tYWFkKjlL3UfBuTEqCosQbE1V9uJiHRberY0w7ScyckJnc78w8u9jkOLe5GQeWulbVpxMvv3dtT9aS1ETBF/q552M9DsRm/MxGZvwwk5iaF9tcf56bg5PEzKVmjG9MjHVrxtxqqceI+bKUtbqpg8ZUN7YUZpBIzf3M4pE+ExP5rtZWbhzLTAdrG32/mqgKez6Vvs9VpY9rTqnPbu3odUtR6WusF/o6zDTlg+nNOG7j5et44/7VmbVxF4WtaVIzr6dm/MsL06bt64nuX52p1a1Zbzf6C81hUWrmRbMUjzr5oe5+6TE1MdedmM90tVCnsaZ8bLQbr3FXNdvOmo3fX9z7ZdOEHhd2/m5s8S3JrcXNGilwTd2mYcelZr2qjkvN53Wm5vZgWjOmRqaWmLVh6vZ8zHUfBb8pBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwelc0Ecx8Y0uHtB+ookjNFGFVanjKZeLpayV88XanzdLHdubmXPMMxOr6mIYbcynjtdcLHS8sgs7dnHvLsa6NcdFJiazsLHEuaxNJuOANmYiY13Ero2adW395Pd4u2NGaCp5ru+1vW/BhovvjU3/qivTGxrXn/X9ynPd1zMTQ+siuptGjzuNi+82UfcuZjdyEa/mGranE30uZuSpxDVUZszpzPhnEnGjzFxbkZhxuouD5pnKzBnH8ba3vl3W0kLfgGysr3FkxtUnjXStKMZBccGZOCwfFUGR5y6i3D13F0fdVnouLU1tsdTri6qqg6Kjc3N9bgiPXcS1mVPc1NeJMTzuwsbiyCwhmroNWpcs52VQn21NeziOtND3OnZx4ua+Lee6nXV1E7ZmM/OG7mMutt3Fgocdl5ilkBnGozx3CzOzDjTnYqYiu4aPY31gZ+5LFunO3pmL78Q868a/PM3DouzNGjf0PbALXqeHc0OWk5i21Lr1rxt7zDW6MUvVWtNwGzOuNrVe09S1Pq4247hrg24ccC3JtTM3z7ZJ2HFplwYd15lJMxXHxXYA7ILmGftaaud18xTMc03dYuEI+E0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMzoSsbhD7645zEaImjtsdV5vo5XJRbhyzW80X8pjERh+aiPXQSGbzfS5O1sVDOzaWMw6N/A39PndgSMxk4Pmbw1yEsK3ZRNiwKN0bSU1cun9KYZLEZSifLD+umIhaM+JNRiLLPoqi8VjH2WcmjrVtTbT5Yq5rJnq+MZ9pLj1KTS3JdFspTC59E5l7Xa1v9HXjYo5NZLGLY3bR65l+dqmJ2k5NPG+eu74V7mDvUNaS3LT5mRnPsn1ZKwrd5iMZEx9F2+fO6M+crL/fiR2PtM60l9q0l6rUfWh2qOf8B65ekbX9Pd1nIzOOT7emspbZ9qlrRW5q5jNbcz+rxfpY8HKh79fhoY4SL5cz/V2lvpdNoz/zcF8fV5b1oHNer23boHvdmOMqN8Z3jTkb/X2JjVJfPw4miR77Y/f64Nb25l3CDRGpW1+0YfekM8e5Z9ead5CoTYLmlCw199M8u048uy7W9ytJ86Dn49bvN5MkCbsO83oWJW594j40+L1H1cx4ZMacumnCarWudebFJzPvyK4Ndq4/m+/z72Bd0PeFv2PGG79zx4G1G7xYhx3n3pHdS8gR8JtSAAAAAAAAGBybUgAAAAAAABgcm1IAAAAAAAAYHJtSAAAAAAAAGBybUgAAAAAAABgcm1IAAAAAAAAYnMkg3SBy0EQmxoHRsC7y3eUR2shLEeNaLXX8cGTiLlMTaZnFeVgKo0mfzEcjWTO3OcpMDK2LeO3c8zER8pmJkM8K3eTizMSAJgHxkzZGNzAK00aHaq37yFPaGnbxwzbaN/B8XH8+aYk5ydiMAZ0Z8YpMt9si1RHr5jZHrWkvS9Np28ZF9+o46ti1QhOp7a4hKoqgdpSIWmJOsTL9uXMR3SZOu3OBzG7ANeNRcvSpcyN5psd412WrZSlrs4OZrN2TvlPWUtcfJmNZG00m64/J1/+854aOutLzcznTtaW5J/sHB7L2znsekLWD/bmsFbm+J3mma9kZfZ/HhW4PE1PLct1nGxNnX87XX9/Bwb48Zn8/rFaWS1mLzThwuK/bc1Xq9VpySvNT1YbFpS+X+voXpW7Xbs3jxvE80u0sF309c/OemUzNLbHvEqlblnS63ba1vl9tp2uV6wtlvfG7xI3uixsD84lZU6fmwcabrz1a11Dsu96tIU7CxoLE3VOzzIgS3SFi85mxWSzFcu3i3kTM+0trrs2svRpz4a4lJal+Bqlp7278cE3XHZeY9uDeozL3HmzWpOozE3Metmb2IiJ7nBtww/qIG3eOgt+UAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4I6ca21TQl2mvYk9jxMXJ64/M4vDajJd00R5diba1uVdxnlYbKWT5yaC10SHJia20iXBd3EbFJeeFzp6OC30NybmMzsRZap+3rO32bQ9m6hqPtVGYZoY5/aUtoaTxHRvGSfrb5yNEA0UEuPqxgcXJ95ULtpW10oT0R2Z576c6/jyg8M9WZsv9HF1o88l1V0vSgsTVT3VkfVbsfnQfPNnl2b6PHJzL81jjWI7kmlt42K/9RfWlZsYwt11x52yVprY8z3Tlsr9Utbm84Ws7e8fytpsNpe10Wiy8XiUmAjoujR9dqGfUbXQz3YxX+raTN+vcqFrmeknSasHuTzW92VkOnRhIqfd8O6i7ueH65/r7p5uC/u7+7K2u3dN1pZL3fZyc23LZRUUXR6bteGxmGdbmz47N9c/M+0zNhN07tZXacA869bobt1v5tLYrpPMR+pSVJd6PK7NGF+VZdA4UDf6+8wyNsrdVRT6Xme5e67rj2vcXJqatbZd49nFWnSz6EJj6+1xYbXWvRs0m7frqtbt1tfcdzVB9zJJkqA5P8vNXGramfs+1zzteQbW1JjqjkvdPTFzoltbxeYc/Uuy1rRuND4eflMKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAI/f9D2bsBenYX/jvUktcX/DvksfSVzan7oGl+pkPi81fxt+bv42fPc37LtUuNFopI8TCRyrjzTJHa1JhehchI+5hHTkEvZciohJMkjWP6TWpLy0pp3ENn0uCbpfrtS4sIJTSi1xyRA3iBg80aS8Gx3nIyA2T9+LzHWnqUt307VFqZOSFnOdRLa7uytre9d0MtV8eSBrdasTnTLT97KRTjTZObsja+PpVH9mVMhaLsad1CSrZFW1+fi9SjwyCaMm7a91CXsm5XBZ6nSs47jttttk7WCm28Sy1u0z2jWpkkt9HUtTq02CW6tKZr53yWiJq7k5w4wrJkwoaip9bZFJ+Up1V4jGmW7zW2Zen4okw94o14mZkbm+Q5McuRCpigd7OmHv2jWd/FiZ/lzk+oZNJvraRhMzhgcm5x7HfKHH/6tX9Bj/wAMmmdAkR0639Hh89owex7NE3+8iW98GJ2P9HBKz7q8Wuo3NZzrJcXGgx7HlUt/nhUm5bc18uVjoMW62r8/FrTuLXPfnnS39mefO6meeXzova+Od6cZj49wkErrUPteF7PvC44wbl5zUvGe5JPDOpJU1ZjJyqc+Lxfq2NDfrUXfdjUkuda8Lbt3v3pETsy5z6Xu5O868W4e+DzWtmUxtkujm81Ts9g3MGsLtN8SmzbpHbhP2EndtxxsH+E0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMTucIPlIcFr2cuCz4sORRl0YYxZ05F7EHl0Rp0LUVJmJ3bGJhi0LXkkKfy3hrEhT7GOf62XWJiSo1sbdRqmuxTq6MYhPnmWT6PFsRN9tG+vzbTtdisx8bJ6b1mVLrIkD1YTeIsQ7nI1DDmGHA9hV3nL+p6w+MzbWZVNgoN21sdqijqmf7ura/qyPRr+1dlbW9azoSfLHQEddNpOOcs8IM6aY0E/HCvclkS9bOXji/8XhVjPSJjM3YEQX2oabWkb51rSOSZ4c6Wnnv4CA6DefPnQsaq/O9PGgOtjHQjYkEbgOehZlP3PiQmvk5NXNwlupabqKq80R/n+55fffS93li4pyn+VjWxpmu5Yn+zMpEvldlKWuHh+vHnX03/u3rvjAe62ewvb0taztndmQtzdKwccCtZ45hPlvK2rWr+7L2wP16bmgqfa6pW2Bt6Y6Um/4wztfXikIfE5m1dhWZKPu5nmuumTlxb1ffy8OZrkVmHViVup/MDxZBa5bcrP0Xc/19da2f+Xiqx4HRZP33FWat417LvFNarA7Mxda7e9O463frf9MGGzPPVpXuR8vl+nFnYdZypRn7O71M8utt905njmtb/YVdaxYY5gG584zNca49uOfj1jSNe42M24CXwViXUrOAMu+ziX1ndetG/XXmbh0JvykFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBmZDwo2c2q3TDXmJiBU1SoY2XTxN92pmJSc5E1HNm4qFTc47jkY7L3ZrqGPXRWEfGZrmJ9h6ZKGCdkhzFuYnCTEykpckIbd1DN+fiois70yBacS6tiWjtXDili8Q1D93Gitb6+9x5NiYu9jhcHKvrX6Fi16HdcS7iVcR/u3HFRcZ2bRIU7X3lyp6s7e/p2nKpI3grE/vtztONf53JoZ0v5kEx3akZU928kI/Wj3OjqR43cxMh72KCXdSxewbzha5dubora/fee190GkZmTilS/RxyMbfdKLI5NuOSmxeTON04/rqp9XySmHbk/uwsN31hZKLZt0YTXZvo+PX6UPcTk8AeFWbey936w41zlYkZL6ugvn6wf7j+53sH8pjZTH/eeKLb83Q6lbXz587K2sh8pknojsrK5FgfQ7k0se1mfKkW+rioNXOiWS7kpl+OMz1+jEQtN32vMv25XOi5dH9ft6Xda7q2v78vaweHeg5OzToo6vR9rs3Y6HTmvkSmPSQzPT9vH6zvl71CzKfTdBwWE+/WT3bdFd00zCV6pkl0ZvCpTZuoKz0OlFW58ZpnudR9b2HWQm3j2ru+YZlYo/dqM+Z05vvcWq8xtTp37/Lm3bMz72dNE3Rc7F6ExbiTpLU8JEnN55kxLu3McabNundWtyZLAsfNG38yAAAAAAAAcErYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4HR+4kbC4tldKmfiIgdN7G2SbF7LTKx0ZqIW89zEdxc6jnUy1lHIaa7Pv4lMTGYaFn/amkhLm+zoPtPmrbrvM21FfaZ5Pi6u02/H6vvsPtLdS3ufk+NFaIb0hTg++e+Uz+gGNT8SrK/F5pi21tc2n+m43MMDHcm8f2Ai0eezoHjlne1tWcuyM/ozTdstaxMzfqDvy6zU135woK9v/1DXtsW9LjI9NiZTPRa7FtuYZ14vXXS5jkiem6j72YGuHU8TVHPtbDIayVrb6bFulLp4ZdP/mvWxxuVyERSLHbv53pxHmuqOMhrpqOrRSM/rWW6il02/TFNzv1odA70w8d6uQ8wX+l4fmHj5Q9Gf53M9PjR1GdROpmP9DKZTXds6o9dPjYm4ns31eR5LGwfFnrcmJj5pTds18/pWofv69mgia9PR+nuaxnoMqJZ67F/Odbs92NfH7e7uydpspo/LMt1nt7b0dWe5bmetiZ7vzPqjMVNwa55rHdbE+peUtT9O3bUFrnHtSs0VnwC/juHeN1ytafU40JgxohJz5rLU41xpak2t24R78nVj3sHMeJza9xP3gHSttd9n3k8Cn09n3lkT846l3oeaxrSFRj+fNDVtz66cA53OK+sKvykFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDB6bzXAaLgXZxzYk4tiUx8tImuVDGTsYmKtFmsJrayM9dmUiSt3ES8RomOkmw7F2kZBV1fYuI8IxOF6Wr+Vq8v5iay2F2a+67WxHLWdR0U2dk2Lrrc3MtT65cnvx/tIoF9k7dBwxvnr7eVfkZLE7F+sNCx5wciKr23MHHp2yaOenv7rKyd3dmStSTTLXt3X0dqHy72ZS2aHwRd38JEz5fiOTQuJr0JbM9dElQzQ2PUVLo/16aNHcd8eahrC12ra92u01Tft7GLid9eHxPfG430XJSKMb41WelNUwfNNYWJgncR0C4Kvhjpz8xyfS5podtZmpvxysxUVV0GzTfLSreHstKfWdfVxh3FzV+jsb6X062xrG3t6La3Y9pl3ep7Ytd5x1DkI1kbmfZZJGbtYp5tEev7Pcn1940zfZ56HWXWsSau3s0ZB/t6HNvb0/NQbfrCpUvnZe3MzhlZm25PgtqLm8P2D8wa49D0S7O2PCz1tc+r9X12FNhnY7dybu0LQ3TLM9ffmZepxixsGve+YeZF9S5SifZwo1pr2rTTdbotJeY9o6r1HJxl7h3MLNpc0zWvPHHgs3Mv80mix/AkWX8yrfmurtOf5992Q/ule0c09+SY+E0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMTmfSbiQsJtOFGHam6qJaXdSpimGMzOe1JgLUpaO2Jpq4MpGWrYlxbWoTW2kiGlsTTdmauEiTxB1FJmY9MXudJtXdxw/LgnkILgLaHedOMk2Cnnnn2tgpJem6y4jtgwg7IRslfMISc44uYn1pYpeXSx27vFgu9HEmrnnbxJe72vnz54L6ZdXqyN8sy4ISXls3XtUmplZFDLuuZz4uMX0vNfHqWWqih1NznLnR2SnFX+8d7snavqktSh3BblKZo62tLVk7e3bbHDeWtUTc09rEHddm7Ox7tPwuN8aZeShTa4HVOkF/ZpqbNlHotpSYmknUjio3r8d6LItSc89S3QFVV0kLfVMmeSFr062RrI1NbTLR7asY6/6cVvq6axMzfhyTUdh1jEf6vlWNnm/yVDf6zK1/zUCgxvHG9MvFYilrs9lM1g4ODvRxh/o4c2nRaKyfwc4ZPcadObMd1J+XZk6sun1Z2zvUz3U+17W9PX3PtnbWt7FipPvJ9pZee2SmfbmxqutOaSF7k7Arav+yG/b+LGru1cbVHPe+4GpuzHFreCfknryrpj+zNcXGvOjb5xoHPLs27NoG5x7dMU+T35QCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDgTEb40WOSY5Np7yITXcRhG+m449rEuleVjkQvReS7i7Y1adRRZ3IRXSpiac4/TkwkpIlhrM2Jdi5n3cWs5ybqs9HZsEWso2gTE93euS1ScfE2adW0r9hlWnb6fsXuM10++WMQl+uiWk+D6w+hQj7RxbtWjR4f2k7HPHemX8ZmFM1MhPx4rA+cTHRceGLa2SjNguLC08xE3ZvPzBPT18WfebQmZrxpXGy7m2d0rcj0vRwXE1mbjHRtOta14zjY25W1xeJQ1tpu/dzWy00b3NnW13FmR8el72zrSPFIjDuVi1E3NZeEnJmFSZvoeTYyfb1pzFzqFkK57guxiZePdPOMIjfumGk9N2NEMdXXUEzXn+doqk/STTP52IwPZjyyan1P6lKP75Vb5x1Dbp7teKTvW2GOa8ypZmZd5hasZVluPO7WZjw+PNDj0f7+TNZms0XQOY7MfFmY+Wsy0cdt7Yz09410bSbeJXrXdg9kral1+zw8PNy4X/a2D9aP4VtbeozeMbXYrNHj6CaJrL+BpDXjuHvXTe0bh/5IM2+ksa4lrpasb/NZqsfc1px/m+ixIzWDfF6YMS7XtbzQ55mZ/pyad9bE1Oz7kGu7rWnz9g10OK2pxWaaTex7sCmZOdi9fx0FvykFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBmZziDeIUTWxx5BK+XeSgi3WvdKzqsjRxs9X6GNdK/Hx1Hi4e2sSQO6WJhXX32cXStyYUsnUPwURoZoXOksy7IiiCMktNrLuJP41VdKWL6DbPJzaR2YmpuTj73Jx/Y+Kv21OKFb15InpNP4q7jWNH21ZHwbed7uvZWJ/H1pmJ/sxG94WdM1uyNp3qz8xN32saE3VvxgEXmz0e6/jrNNfTRG7GDzXA12b8ixb6/JNUn0cSmf7V6c/MMn3+ExNn7+7Xcczruaw1ZhwfjXUbnEx0/PfIXKNrg2PzfSHd3D0jF8kcuznRjNWNWUOUS72GqJsqKB66i/W51J1+rpkLezbXnmYmwnuk23xRrD8uyzceom845hRmXEnNHNzZ56rHxrrUY/9xZGZcyvM0qH+V5ris0N8Xm+fu2lkl7lvb6mPqpgyag6NO11KzPhyPzBi3NZa1rR09B2/v6LGxGOlGn5rnMzXnmdnIet2RzGONxqIfTcwc5da/zk2zpDwOM+S69yX3QpuYd4PEfGaSbF5LzbtGlrlXfjOfmM8sct3eR6Yv5HkedJ65mTfc/XILkMa857vjusC+ot7NWnOQe5+z73quFjgO2P2gYw4S/KYUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAG5/IhH8YmLbYu0t0cZiJqu6YLio92wY5dvNnPb3jhJr62NmGRLvrVhUzWpY7SbUwEb22ixGPzgPJaX3tr4qgTE5cbmxjQ1MVMqpKJ2k7cnqtpKLGLRk30tdUuutx1hCdEzm4Y1dVb8wBdNHtmYrjPnt+WtfHYtGnz3M9smc+c6rjcLtL9ebE8lLVlqePsOzNuFiZmfDyZyNp0Mto8Dt2cR12ZsarW9yRL8qA449RE1rvIYhdnfxytGauLiT6fPNfPaHtLx55PzGemqZlnTRx8Olp/b1IzzOVmPG5qF4Vs5r1On+Oy1v1kWc2D+t6i1P25bpaylib62WVpFtSf66qSteVyJmtlub7Wtvr8s8SMHWa8HRe67cXm2uJW15pKP/PFQl/3caQmVjsxtdhEwadmPE7cfTPjWZeY+6bud6zHI/PYo7TQ152N9HVPan1tk20910y3wmLpXZx9buaNyt0XWYmixLzzuFpu3hkmYp6djk07sUnw+jy8sHh57/G2NnbXaPp6nATVEltLNz7GrXfcleWZGePHul8Whelfma5l5vtczd3L1qzT3StYbDpLZ+YiNz+3TbPx57WBtcbUnM7cLyfxg8uNjz/W0QAAAAAAAEAANqUAAAAAAAAwODalAAAAAAAAMDg2pQAAAAAAADA4NqUAAAAAAAAwODalAAAAAAAAMLgj51onidm/MtGHnYky7UwOo4tGbJoqqCZjE12EoU7QjEz6ZJSa2MrERM26GEYX0Nh1+hrq2kS8mufTJiY+tDTx7HVYLU5NtHK3/mbH5rrdY/XBria+1R0Zu8jlkw/gvREbZWru2zG+8cQ/sROt3o0drn/tnNmRta1tHc3emXYbNfq55yYSPDXtfVHqyPqDw31Z2z/Yk7WyWgaNV+6ebe9sydpURQWbgbNq67CIXdOL8tjEC5sxLjXtKM1P589zRMrzymSi2+dkomOZxyYa3MU5d5F+FrOFboPTfHvtz9NU37PO3M+m0efRtvq5V7XuQ8t6ro9rdT+Zm+sezfU11NVM1rLsvKyNTZx9tdTneTjT37d7bVfWDvbXX19txo58rMexUa77XpHqWl2VsmaGzags9Xke7Oux8ThiNy6ZecNGdbvo9twMEoW5ObopyTksMfNXPtXtfWxqo6l+7o15JRlP9DiWuXtirqEzC7M20s+nWuh2Vs312FIv9ZgU1fozEzMWj8QtG2dJ2FLNrK3sYcHrP/tmE90a4hOvqfW9W/e7mnvHD32fdWuo3MwNWZYHfWYU+n5m1hHqHaTXuj0MM3407fp73TT6PEJrkd27Cetf7v3ruPhNKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADI5NKQAAAAAAAAyOTSkAAAAAAAAMjk0pAAAAAAAADE5nMj5C4uLlXc1FeppS2+mIw9pGg5uIxnh9LTZR1bGJU8xGJqK2MDUT9+vus4utNOHCUdzoz6xM3LZJWbcRlG1tnl1Vh8X6ighNm6bqEnHNfbZcXK6LYjWxxJGL8zwGF/FqutANuAjUk9fFm39bbuLli/H6uPpebD6zMRHl5ULHPFcmorws9XHzw0NZ29/Xke6LUh/XRZWsjYuJrJ3d0ffsjKltb002bifLSo8BtYgt7yWxiTN2keDmbGzUuzmX42hcjHDgvOe0bVjcsZuf1TzlnoMbV9tWX3dd61pZ6j67dH12qftsasb4LNFttzPtpTPzZZc1QffMjf1FaqK4RS0xE617Pu4ZzGczWetMWy8z/ZkHB3r8O5zpZ34ctZkbFoHjf+NiyGO3DjT9stNrr058plsbp7luE3lh+kmmry1NXJuWpSg3EfKFiZB3c35dVUE1t1ZobU23lcQ8O7VsHpv19MKsOTvT1zszz4avAN1xpxM9HzpfpmbMdeNx52puDja1Rswpdt42nxfHYefouPfn1M1Dma7FsZln3fMJfHdx99O9k3dmXlQy8z6emFpcm/d4s2ZpTc2tISLzDNz3HQW/KQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMHp3MUNxGZvy0auuthbE42dZvr7ssLETI7W11oThZmbaMrJWMeojyfjoPjauNNxiqWJF3ZRwLGJcXUp3ZGJZXbRmy4i2kU9Wypmsjv58+9brZLY2NSQTzy11Fv7pS7RM5i7p6GPXdyczo0PqYmjNjHJ7oZ1jYmH7kz0/FJHlNdLE5dujptXJto81vdlMhnJ2nR7KmvF2I2BeiybTIq1P29cPLJ5rq7vOT7qWEfp1rV+PqWJ7z6O2YH+3KbS96apTUx8pWujwszPptY1uq+k8fr7lmb6/Ota18pSP6Nyqfvl4kD3k/m+vs/LuX7uo0z3oa3Jlqxlie4nJu09Ms0ziiM9liWpPs+80OuWoli/bkkzfS+7VreF3b0DWcvSe4PWEEmir/vwcKbPZXc/Og1lo+9NZWqLUp9rWZl2vdBzQ9fp44p8vPEzbMwYOD/Uz3Zvf0/Wlsu5rEXu/DPdzrbGur2b14X+CmUlNvPUcq6f3f7+rjlO37OxmWe3p5ON31HKSj+7xLz6NeZ+ufk5Mu8uwTHxx0uXfyw+eL0ubNns3lNULeSYdxWjMObZxq6WmPdSVzPvs619B2lO/H22MTX3fY0Yb+vGHNO0Qefoau752Pfg03h/vP7Zp/fRAAAAAAAAwHpsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcDoXdAMuojcyMa6xibHuah3R2JhY0i7VxxXT9RHlk63N41ZXn1es/7zVZ45N7HKmj2sbff6picLcNxG8tYkXrm0ctX4+lYmzb8f6+tJYt5UszTaudS7uvWvCclhdSqY5Lk3184nN/m9uIrWPI0lcxmvYd7oE1Dg2RdMv7VnKovku/wBNzUTWt7q9L5c6HvrwUPfLcmli6U2tjXW7Hk11NPbExGZvbek4+2Jk2nXSblxLE/180iYOip63MciRHuQq91wrHee+LE2s+THsXtOR4YWZi5YTPcaXU91eJiNTm5goZPOc4ihf+/O8cLHLshQ1S12s5vrZLhfmuVdmDdHq9j7d0f1kZ/ucrGWp7ntRpOfEzsSsx7FZm6R6/TEabW9cywszHpkY690DPTbOF7rNduYzE7OGKCv9zBcL3Z+PY1nNg8aQ0o0vS12rSn1PI7Pmyd36RKwVDk0fWsz0ecwODmWtMmNnZ8aBxKz18lS3idYscus2bB0xM9d+cKDn/KrSx+2cPatrOzuyluXr19uVWb+nRRb0qwpda9ZPrpaacSwJWzcfRxK4/o3cWt2M1V1w7ein9v+O0QfFsTn/OOwZJea91I3VrhaZWmy+z41/bunv7llrBiVXc68oathpzXjkap05D3dt7pkHNL0TwW9KAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcCYX9BFMbGVkIr5drGpnjjOpzFGa62LW5vr7xNUWmY6fzFP9ebmJ6B6NzXGpPq4xMa5VaSI7XdyvSd40SZ824TUOjA9NzXlmJtY3FTGgTdyGRYeawEsXhRmbNuu6SGw+NTZRx8cRuwhRd2AS2F5cgzH5qO7edOreuNjbxLQjEX3dK5c69rwudUR3XZayNjMx5Iez/aBI9FGxPgK6Nx1PZW080ceNx6Og46paX998sb6xjKc6rr4oXAPT96Qsda1pdNsra/3MF+bayuZ04uUPr+nI8IWJ8a4qfY0mET2qxrpWu0k41vNblq7/wq7NwsbqNg2qxWrCX40Duk1PRjp+/czOeVnb2daR7nlaBMWMd6bmxrncrJG2ptuydmbnzNqfLxZ6jCtL3YfKZSVrswPdh1rT131M/MnGqx/Fcm7GiYW+N5FZ643zfOO1UK8zn9lU9cbrGreWy3Pdpkem5tbUZavPsax0WzqcHcpabY5z61gXwX6wr8fp0txnt4BKzPo3y/T9LLL1c3cuft5rzXe5tzm7/jNrKycxH+rX8MdhxtUo8AbYAcZ+6uNCYu516t7NTM21aTe2uDYRmT5rX8LsMwj8zFOaU4Zi3+PdfT5F/KYUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGpzOTN4ra7cJiBUUM7apktsuyIg/6zK5Zf565ia3MU32LMheTaSKZVfzuik70jepGR9u2nTnQPQIX52kODI0ITU2MtW0r8fpnF4ufv6uoS52NBw38THf65ttOL3nT9Et3jToJOWpsBqo5MDA7VaXU+rZpom1N7G1i74m+trbRtc4cZ6PgTWNqzXl2Lkk3dQNBWNJx2+hxp23qjaO27Txj2ldjIuTr2sSML+ayVi111HtdltGpaM3cVuuxM6rNVN7o45pK3+96HvZ1Vb6+wWRmoMsyc22tyyE3Mfedm9fHsrazdVbWxsVU1rpWf99iqdtnHOu2VNX6nuWZaSt2LtIPb5Svv77JaEd/oPm8sjyUtdr02ahzS1M39uvj8myD5e4GZgf6+S1my6Ax/syWvt/jfBzU5jsz7MZiAkiSQh6TZfo88mKij8tHsmaG6qg0feied96vvy9wgWWm9Whvb0/W5gszNyS6DRYjfc9GI3OvRXtw99mt40yvvMG44tZ4YdHziXtXOha7IpeVxKwfO/MuEpvjQq9fvSP7e+Y+z6yb3Ttybsbc3LxfuvWouSf2/dmMqe4+x+7lzTVrd5xtY+Jex+b9OAmtmXceWwt8oT0mflMKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDO52M3IfoTJ54auIIYxd3nLrPdJG46zNeM3MemYuQNzGZMsv+XUVZ6Vp9baWJIa9F/PrqM01spYuEjDJzXwrddNI0NP7URZKur3Um59gleSbmPGw6qGnPsQnT7Wx7cGd6DC7S07SzzmegBtbCzjMV983FwrpxJWr0OdaLStYWM9335oc69ntpPrNa6j7bNLotNbavR0FjS1ubWqXvdatTp6NYxLMnqb6XiflzkrrU96Qu9T1ZLsIi25dzXSuX+rkeR2oi2FVfWGl1zTSXqBJz4ur7Il2bZ/pD02T9fWtq3Y4y0VZu1Kbn87msHZh+WZq2FMd67bEs9T3Z3Z3J2syMH6NcX8NopDtYMTJtxaxNykq33bJef31dZNZVZhnZdrpdtm1YnLaLV08yfU/y8Tg6DXVl5tJan2uRTWTt7JmLsjYeb8tamui225ln0ff2TdfGRbEla9PpGVkbTXR7b804Zoaq6N77r2y87u/Fdsmiz2W51GNLbdYYxcjcsy19z4qxPi4brW/Xaar7gl83h62pY7O+CP31B/e+8HgTmzErMetV9x6Zpro/q+ebJnpujs2zzTI9jifmvTqOzXuwffDu2dqWFliLTuEz47CSGFcTtydi+kJoLfRt7jS7Jb8pBQAAAAAAgMGxKQUAAAAAAIDBsSkFAAAAAACAwbEpBQAAAAAAgMGxKQUAAAAAAIAnVvqeS9DKMv03+icmfaUxyQOtSIdwCSOpTYFxtbB7Unc6FWi2WMhaVZkkL/f36JuEvSQzCWfu+bj0M8M9V5lWYPZVW5M04W6Jf3buM0MT7U6HSxhsbTJfG/qFQecSu71xkfqRBiYLzue6D+3tHsratQf2dO2qrh3O9GculyZNszaRaSblcTnSn1mOdW25pZOgRoU+l8lEn0vbiOfamLHDpM00tf6uaqFri5lLVdTXVi50P2hOJ3wvyrJR2LjkUgtNOlgbu75uUnxi03ZLMc8Wi6B0n86ktM3mLvFO1yqTZJuaFKLSzLOHke7rbsJxqUeFSbl16Xu5aUduJqrq9felrPT9qsx427iaSSlzjd2FULp+ECWns9yNzTJ6VExlLc91GuCZnfOyVmQmRdBEB7d6iJTzs0vWGhX6PLa3dnRtqvulW7KVZr48NEmbdi51TTDwz+wz81y3tnRy4s7O2aD0vTgVfd2Mqe4+Wy4K1R9oavHgvzeh3idOi3tXzEz6XlEUG7fr1qzfXV9wia3unc69z7paKHd9Lmkz9FRCE+Mdda9Tm8QYtifidGa/wb0GpmZsSWwa443xm1IAAAAAAAAYHJtSAAAAAAAAGBybUgAAAAAAABgcm1IAAAAAAAAYHJtSAAAAAAAAGBybUgAAAAAAABjciWTkdib210WBupj4xMQfZqk+7dRENLYiEzc1EYapiStVn9frYhPDbbIWq8ZEm5tY5qY2sZzmPrv46yzX9zLLTSSkqcWp/sw43bw9+JTPJuzZuYhTF8t+CpGjpycwhjdx/Tk5hQje9d/XmJj4aqH7ybX7rsja/fc+IGv33adru7tXZW1ZLmStbnQ8b9uEtd3lqJK1cmrGHX1YNJ64sUw/1zReH1Wdxvr8i0KPHXaaMedha6at5yZ6fWtyJjoNo/FE1robjHZK6wYmMww05rhlpdtu1axvZ7GJdHcn4sbj+Wwua4u57nutyUSfbrkodX1ca/pzY2pO5uZnU0uzXNeSbOP5re7aoPHI9lmz7orN+s+tZ9xdrkW7PK401vd6Mt4O68+dvv7lUt/vONZ9rKr1WFfk6z8zM+2oqtxaQB83Gm0FfWbT6L7embWeXeq4tZdZzxR5IWvj8VTWzp45K2tbO+dkLcnWz6U91ayXZRM0/rXmXjZm/HPvgS6xPjFrQ5NY/5iw45lpaO4ak8S8f7p33XR9H0tTPQp25gXGvY/H9vdX3Nytj2rseOzORWvNZ7r37tbMb6H8M09O7JiVxI1j5vnoo24wbkanht+UAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4NiUAgAAAAAAwODYlAIAAAAAAMDg2JQCAAAAAADA4HTe5Aax9a3L7YxdHLX+zDRKg6IRXTxxSIxhYzJA3Xd1JmLSpUOXVRkW42quLc1M7HluonvHunnkI31cNtJxuVmhj0syFxG9/hpic59jE39q27OLOJWVG9RcNuopqcqwGHLHXYUNVQ2M427E803Mfvrhvo6Ovu+eK7L2wP1XZe3KA7uytn9wGDQ2hv+JgD6ySU3NRILXlf62yozhZWpq+fpaJX7es8HDJkLeNr7WzCWdGf9yHe29rUvHsnPmTNBxbeA85ebgLNPjf5amG0c9uzEwdHx0j70zMcmZiNPuTSYTWXOpzE2jO1FZLsxxYWuWxt1P85lufpPzrIktT8x5jMf6XmZ5EfRcm8ZFz5vx7xRiv3v7Zr6xsefm2e7u6jnl8EC3pSzXfXZk1myj0WjtzwvzjFyPXSyXslZXLrZd35TGrOdcm6hN93IPIXPrUXNfiqmeHEZbW+Yz1z+DXm3uy3yxftxx02VlXkK6yN0wM+aYWmLuc2ruc2LmmWNxL0xmjE/ce4NbvHRJ0LomNicTi3fkxHxeYtZysZkvo9hdnK65Ibep26DjQvcimk63+da0eVfr3P6GHSFPVmwfnS4m5rm6++yurU3sgHtD/KYUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGp/NjN4k3jF3UrosjdJHNOtayrnS0Y1kuN47GdrGbkYnarisdAd22bVAk7ny2CIrljE00e2pikkcjE2071rXcxAuPR2P9feYz08xmsa79cWyiViPzDDoXKxoF6sLCQX30ZriyrAKjfc1RLno0CtOab6xEtHlsIn0PTJz2tasHsrZ/oKO9F/NSn2PZBkWuunx5H+NqPtIM6XGrx9Su1ueiR9soqhL97Jbp+iPTWN/LxpxHYmLpG5P7vZjpfrBcmLhfk/Gc53qMO46tne2g41ozT7WiD91o7HHx8olpu3W9/rl3nR0FTU1/V2rOcdKMg+Led7Z39Pe5sarW7bos9bxXN6Y/uDnMDcZuTZNu3sfc827NuDIa6Zj7xq2txFqtV7ko8eA2Fm732n5QO0vMc1gu9Tq2M7Hn7jllme4rxWh9Lc/080vM57nnUJZmLjXjuGsvbvxz8fJ26W8O69yBZp5qzXi7NO81+7OZrM3FO09i5ua60d/VRSbSPe6C5pLUtPU8z4Pa7Kkxa8soDVuXdWYd6N7PEvMenIqaWye5NuFeQ9w6NjbjqhuOG7Muce/PsblfnensTdMGfZ/j7pk7TzVOJwHHHKdm3z3te+npzKU9flMKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDO3repokCtTnxhgvvdRGNy6WObp/PFxt/pjv7zsUWi+jrG8VdVqWulYsqLLIzDYvNzkZ5WM19polxdXGl9gJltqhpRTbtUhdj055tTHBgSqZrY8cR0hd6jYnjbu0dMEw2rIs9VxHRXaNv9nxPX3fduHHMxe/qSPci15+ZuHYWGNUa21hmWYrMkBSVZkxqahcHrMerLlr/HJalft5ZOgsaO1xM/Gx2aGqzoDHcxeweRz4pgmLWXX/uTLseFcXGkdM3ou6Na9M+tthEXLsx3py+Pc5MHOXCjC2VvoZpNJK1pqk2Hv/edZwby0zJ3rPkRMcqN97WgZHgjesHzSlM0DcwXyxlbTody1qa6wbaNmHrTnffXB8rlyJePiv156V6nZcmutbG7tma2Ha3TDLfZ9LsbbE1tdqczLLU/Xlm1mSuPyQHB7omBjM3Rfl74tZ4upaaL8zN+8J4rPvIaKTHzeNwQ6ebG9xLhV2zJWHvbpnpY7l4z6oyPa50pn85dn72N0yfixmr3FnGZvx3a6TQaw9dt4TUErPmsjWzNo7dO3ca+tJ6Ou+sPX5TCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg9N5kxvEImYimvJGEbXLpY7SPVjqKNraRPC6uFwV8d2Y2GUXMRmb3OWmMbG3tYk0Nt9XNfqejPKJrOWjXNayQj+7UaGPK8ZFUMRpbTKbaxMVr7ZPu8jESrtcbFPzYZcultOcf5cMHlV9eDiXtarSscVlpdtZ0+n+FZog2tT6vpXN+u8z6clRu3S54Fqe6/jhie5eUZYVQf3ZRc/7yOawdu1q7n6640o3dooxfFnpNpSY/uWosb03m81kbbHQEd2pGcfGEx1jfRyTrSJoLmptXrqW5/r7UhclbKi2GxqfHJvO4J6Ri0QPjfbOcl1ra7OcMo+n7aqgdt2YXHPXHNyYJG+LizvXnxbVZh1UNqHR3mHX3Z3SPFuY9dXYTBzTqa51nX7uy6Ues+paz92tjUQXncXeMvMc4jYo2jx266SRi2bXn9mY9tK0YbU4icPmooWeiw5mbdB7VCvW1Ik5x63pVNbM44li81zTTB84Gum11Q0a2ek4lUR71z7NHJboeSPL9P3O8vXHZZUej9zQ726KmxPtOsEc5+Yh+94d2lzcHOauLw17d0tT91zX11LT+dx7tavZ8zdjhGsqbjy6QSO7IX5TCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAgzMZxkePAHRxiu44G/trjitFBGqvNlH3Ta3i5ZvAaMqwbEqXvt6ZIMbxZEvWikLHgBZjHfudF7oJpKYWm/jXKHVR9yaC0tzPTsQZh6Y829TK0MhRE2ccmxONTyWeNorm87msLRY6Vnq20MeVte5fnYlQdvfbRcPW9fre0pjxIWn1c8gj3U+STB83muhI48z0Ez9+uDh705ZcQm3gmKr617vOxUTb5uZ+ir7uzqOxseW6D9X2M03jM/c5H+lxczwZR6dh++y2LgaOE25OyVLz/FxDM9RhLqLcfZebZ10/MSU7xrvv60bpxtHsN+L6nqu1jYmzt+OArrVqnjVtyHUvN07XTdhnujHO6U4pev72u2+TtfF4ImsjEQvea6pS1sqx7rNVrY9rmvXr355uEq6jmA7mItbN2tGtAdtOj8dt7fqCeV8IbJ9uLHZjUheZeaqtN14H+eeqTzJxy3cXPW+aQ96mQWNqbvpB5t4zjsOMIW4FkkVhc5gZcl1XibJU35tczN15Vge1W9c23fMLrUWmnbkR3vUv2+bNg3Vrk7hxC25dSs2zS8W6OTHt3V63u8+B66fW3Uvbf473QstvSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHBsSgEAAAAAAGBwbEoBAAAAAABgcGxKAQAAAAAAYHA6s/ARWhOdGnpcbSJqXexvU7voVB1ZX1frj6vrsPNI07D4RpuSaeJRd3a29Gea70sLHSE8ykdhce+pidB0UZ/m2l0L64IiTuOg8/CR0+YzfY714HvDy+VS1uaLhazNDuf6M01UddOERaI7KpW5MRHJaWxihHMXd2wijTN9XG5qsYsQNn3WRQG7cSd0LFZR8De6htyMLao/qHH4hudhIpfd+OfmmdQ8u+lUx7lvbW9Hp2F7e2qqLtI+LO7e3bfETVSGatfu01wkszsPm5Ls8stdjLWbiUZZ4BjvDgs7rm3C+nNrou4bkV3emLGjc99lstBdTHpr7kno/BzaR27k7ifdZb5Rf2dr1p1LPQVH+Vi3wbYtZK0x42BVr3+G5tFanevPZn3o+mxnFm2lWQ/Yec/UTDeJusacpxk/mlafZ+Lm/Ex/n2pG/rrdWs3l3Ju1fRvWZ934ETo2np6w8cVNRW4Sa8yaVK1X3fq3jU3bjNx0qZ9D7OZSO88GrlkC1yX+PN2YFDbfuHV6KvpRavcNTn7+su8n5rjGPrvj9Vl+UwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAINjUwoAAAAAAACDY1MKAAAAAAAAg2NTCgAAAAAAAIOLu8df5iYAAAAAAABucfymFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAABsemFAAAAAAAAAbHphQAAAAAAAAGx6YUAAAAAAAAoqH9/+EMDt4qV0PyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========== PYTORCH DATA LOADING ==========\n",
    "print(\"Loading SVHN dataset with PyTorch...\")\n",
    "\n",
    "# Define transforms untuk preprocessing dan augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_dataset_pt = SVHN(root='./data', split='train', download=True, transform=transform_train)\n",
    "test_dataset_pt = SVHN(root='./data', split='test', download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_pt = DataLoader(train_dataset_pt, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader_pt = DataLoader(test_dataset_pt, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"PyTorch Training samples: {len(train_dataset_pt)}\")\n",
    "print(f\"PyTorch Testing samples: {len(test_dataset_pt)}\")\n",
    "print(\"PyTorch data loading completed!\")\n",
    "\n",
    "# Visualisasi beberapa sample data\n",
    "def visualize_samples():\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    \n",
    "    # TensorFlow samples\n",
    "    for images, labels in ds_test_tf.take(1):\n",
    "        for i in range(5):\n",
    "            img = images[i].numpy()\n",
    "            # Denormalize untuk visualisasi\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = img * std + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title(f'TF Label: {labels[i].numpy()}')\n",
    "            axes[0, i].axis('off')\n",
    "    \n",
    "    # PyTorch samples\n",
    "    test_iter = iter(test_loader_pt)\n",
    "    images, labels = next(test_iter)\n",
    "    for i in range(5):\n",
    "        img = images[i].numpy().transpose(1, 2, 0)\n",
    "        # Denormalize untuk visualisasi\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = img * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title(f'PT Label: {labels[i].item()}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('TensorFlow', fontsize=14)\n",
    "    axes[1, 0].set_ylabel('PyTorch', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c21ba",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network (CNN) - Teori Matematika\n",
    "\n",
    "### 2.1 Convolution Operation\n",
    "\n",
    "Operasi konvolusi adalah operasi inti dalam CNN yang didefinisikan sebagai:\n",
    "\n",
    "$$(I * K)(i,j) = \\sum_{m}\\sum_{n} I(i+m, j+n) \\cdot K(m,n)$$\n",
    "\n",
    "Dimana:\n",
    "- $I$ = input image/feature map\n",
    "- $K$ = kernel/filter\n",
    "- $(i,j)$ = posisi pada output feature map\n",
    "- $(m,n)$ = posisi pada kernel\n",
    "\n",
    "### 2.2 Output Size Calculation\n",
    "\n",
    "Ukuran output setelah konvolusi dapat dihitung dengan:\n",
    "\n",
    "$$Output_{size} = \\frac{Input_{size} - Kernel_{size} + 2 \\times Padding}{Stride} + 1$$\n",
    "\n",
    "### 2.3 Activation Functions\n",
    "\n",
    "**ReLU (Rectified Linear Unit):**\n",
    "$$f(x) = \\max(0, x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "**Softmax (untuk klasifikasi multi-class):**\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}$$\n",
    "\n",
    "### 2.4 Pooling Operations\n",
    "\n",
    "**Max Pooling:**\n",
    "$$\\text{MaxPool}(x) = \\max_{i,j \\in R} x_{i,j}$$\n",
    "\n",
    "**Average Pooling:**\n",
    "$$\\text{AvgPool}(x) = \\frac{1}{|R|} \\sum_{i,j \\in R} x_{i,j}$$\n",
    "\n",
    "Dimana $R$ adalah region pooling window.\n",
    "\n",
    "### 2.5 Loss Function - Cross Entropy\n",
    "\n",
    "Untuk klasifikasi multi-class, kita menggunakan categorical cross-entropy:\n",
    "\n",
    "$$L = -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$$\n",
    "\n",
    "Dimana:\n",
    "- $N$ = jumlah sampel\n",
    "- $C$ = jumlah kelas\n",
    "- $y_{i,c}$ = true label (one-hot encoded)\n",
    "- $\\hat{y}_{i,c}$ = predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794e0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TensorFlow CNN Model...\n",
      "\n",
      "=== TensorFlow CNN Architecture ===\n",
      "\n",
      "=== TensorFlow CNN Architecture ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m590,336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">866,602</span> (3.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m866,602\u001b[0m (3.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">865,130</span> (3.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m865,130\u001b[0m (3.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> (5.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,472\u001b[0m (5.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 866,602\n"
     ]
    }
   ],
   "source": [
    "# ========== TENSORFLOW CNN MODEL ==========\n",
    "print(\"Building TensorFlow CNN Model...\")\n",
    "\n",
    "def create_cnn_tf():\n",
    "    model = tf.keras.Sequential([\n",
    "        # First Convolutional Block\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # 10 classes untuk SVHN\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "cnn_tf = create_cnn_tf()\n",
    "\n",
    "# Compile dengan optimizer Adam dan learning rate scheduling\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "cnn_tf.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\n=== TensorFlow CNN Architecture ===\")\n",
    "cnn_tf.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = cnn_tf.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871fdaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch CNN Model...\n",
      "\n",
      "=== PyTorch CNN Architecture ===\n",
      "CNNPyTorch(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout5): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Total trainable parameters: 4,469,610\n"
     ]
    }
   ],
   "source": [
    "# ========== PYTORCH CNN MODEL ==========\n",
    "print(\"Building PyTorch CNN Model...\")\n",
    "\n",
    "class CNNPyTorch(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNPyTorch, self).__init__()\n",
    "        \n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # 8x8 setelah 2 pooling layers\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv5(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Flatten dan fully connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "cnn_pt = CNNPyTorch(num_classes=10).to(device)\n",
    "\n",
    "# Loss function dan optimizer\n",
    "criterion_pt = nn.CrossEntropyLoss()\n",
    "optimizer_pt = optim.Adam(cnn_pt.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_pt = optim.lr_scheduler.ExponentialLR(optimizer_pt, gamma=0.9)\n",
    "\n",
    "print(\"\\n=== PyTorch CNN Architecture ===\")\n",
    "print(cnn_pt)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params_pt = sum(p.numel() for p in cnn_pt.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params_pt:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db7a39",
   "metadata": {},
   "source": [
    "## 3. Multi-Layer Perceptron (MLP) - Teori Matematika\n",
    "\n",
    "### 3.1 Forward Propagation\n",
    "\n",
    "Dalam MLP, setiap layer melakukan transformasi linear diikuti dengan fungsi aktivasi:\n",
    "\n",
    "$$z^{(l)} = W^{(l)} \\cdot a^{(l-1)} + b^{(l)}$$\n",
    "$$a^{(l)} = \\sigma(z^{(l)})$$\n",
    "\n",
    "Dimana:\n",
    "- $W^{(l)}$ = weight matrix pada layer $l$\n",
    "- $b^{(l)}$ = bias vector pada layer $l$\n",
    "- $a^{(l)}$ = activation output pada layer $l$\n",
    "- $\\sigma$ = activation function\n",
    "\n",
    "### 3.2 Backpropagation\n",
    "\n",
    "Backpropagation menggunakan chain rule untuk menghitung gradien:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot (a^{(l-1)})^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}$$\n",
    "\n",
    "$$\\delta^{(l)} = \\frac{\\partial L}{\\partial z^{(l)}} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$$\n",
    "\n",
    "### 3.3 Weight Update (Adam Optimizer)\n",
    "\n",
    "Adam optimizer menggunakan momentum dan adaptive learning rate:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "Dimana:\n",
    "- $g_t$ = gradient pada timestep $t$\n",
    "- $m_t$, $v_t$ = biased first dan second moment estimates\n",
    "- $\\beta_1$, $\\beta_2$ = exponential decay rates\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\epsilon$ = small constant untuk numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70fb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TensorFlow MLP Model...\n",
      "\n",
      "=== TensorFlow MLP Architecture ===\n",
      "\n",
      "=== TensorFlow MLP Architecture ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,146,752</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m3,146,752\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,844,234</span> (14.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,844,234\u001b[0m (14.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,650</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,840,650\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 3,844,234\n"
     ]
    }
   ],
   "source": [
    "# ========== TENSORFLOW MLP MODEL ==========\n",
    "print(\"Building TensorFlow MLP Model...\")\n",
    "\n",
    "def create_mlp_tf():\n",
    "    model = tf.keras.Sequential([\n",
    "        # Flatten input gambar 32x32x3 menjadi 1D\n",
    "        tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "        \n",
    "        # Hidden layers\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile MLP model\n",
    "mlp_tf = create_mlp_tf()\n",
    "\n",
    "mlp_tf.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n=== TensorFlow MLP Architecture ===\")\n",
    "mlp_tf.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params_mlp_tf = mlp_tf.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params_mlp_tf:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f001136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch MLP Model...\n",
      "\n",
      "=== PyTorch MLP Architecture ===\n",
      "MLPPyTorch(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout4): Dropout(p=0.3, inplace=False)\n",
      "  (fc5): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Total trainable parameters: 3,840,650\n"
     ]
    }
   ],
   "source": [
    "# ========== PYTORCH MLP MODEL ==========\n",
    "print(\"Building PyTorch MLP Model...\")\n",
    "\n",
    "class MLPPyTorch(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, num_classes=10):\n",
    "        super(MLPPyTorch, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc5 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create MLP model\n",
    "mlp_pt = MLPPyTorch().to(device)\n",
    "\n",
    "# Loss function dan optimizer\n",
    "criterion_mlp_pt = nn.CrossEntropyLoss()\n",
    "optimizer_mlp_pt = optim.Adam(mlp_pt.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler_mlp_pt = optim.lr_scheduler.StepLR(optimizer_mlp_pt, step_size=10, gamma=0.7)\n",
    "\n",
    "print(\"\\n=== PyTorch MLP Architecture ===\")\n",
    "print(mlp_pt)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params_mlp_pt = sum(p.numel() for p in mlp_pt.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params_mlp_pt:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a0eb9",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics - Teori Matematika\n",
    "\n",
    "### 4.1 Confusion Matrix Components\n",
    "\n",
    "Untuk setiap kelas dalam klasifikasi multi-class:\n",
    "- **True Positive (TP)**: Prediksi benar untuk kelas positif\n",
    "- **True Negative (TN)**: Prediksi benar untuk kelas negatif  \n",
    "- **False Positive (FP)**: Prediksi salah sebagai positif (Type I error)\n",
    "- **False Negative (FN)**: Prediksi salah sebagai negatif (Type II error)\n",
    "\n",
    "### 4.2 Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Untuk multi-class classification:\n",
    "$$\\text{Accuracy} = \\frac{\\text{Jumlah prediksi benar}}{\\text{Total sampel}}$$\n",
    "\n",
    "### 4.3 Precision\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Precision mengukur seberapa akurat prediksi positif kita.\n",
    "\n",
    "### 4.4 Recall (Sensitivity)\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Recall mengukur seberapa baik model mendeteksi kelas positif.\n",
    "\n",
    "### 4.5 F1-Score\n",
    "\n",
    "$$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "F1-Score adalah harmonic mean dari precision dan recall.\n",
    "\n",
    "### 4.6 ROC Curve dan AUC\n",
    "\n",
    "**ROC Curve** memplot True Positive Rate vs False Positive Rate:\n",
    "\n",
    "$$\\text{TPR (Sensitivity)} = \\frac{TP}{TP + FN}$$\n",
    "$$\\text{FPR} = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "**AUC (Area Under Curve)** mengukur area di bawah ROC curve:\n",
    "$$\\text{AUC} = \\int_0^1 \\text{TPR}(FPR^{-1}(x)) dx$$\n",
    "\n",
    "Untuk multi-class, kita menggunakan One-vs-Rest approach atau macro/micro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629edd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ========== EVALUATION FUNCTIONS ==========\n",
    "\n",
    "def evaluate_model_comprehensive(y_true, y_pred, y_pred_proba=None, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation dengan semua metrics yang diminta\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(10)]  # SVHN classes 0-9\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EVALUATION RESULTS FOR {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro): {precision_macro:.4f}\")\n",
    "    print(f\"Precision (Micro): {precision_micro:.4f}\")\n",
    "    print(f\"Recall (Macro): {recall_macro:.4f}\")\n",
    "    print(f\"Recall (Micro): {recall_micro:.4f}\")\n",
    "    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
    "    \n",
    "    # AUC-ROC untuk multi-class\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            # Binarize labels untuk multi-class ROC\n",
    "            y_true_bin = label_binarize(y_true, classes=np.arange(10))\n",
    "            \n",
    "            # Compute ROC curve dan AUC untuk setiap kelas\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            \n",
    "            for i in range(10):\n",
    "                fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            \n",
    "            # Macro-average AUC\n",
    "            macro_auc = np.mean(list(roc_auc.values()))\n",
    "            print(f\"AUC-ROC (Macro): {macro_auc:.4f}\")\n",
    "            \n",
    "            # Micro-average AUC\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "            print(f\"AUC-ROC (Micro): {roc_auc['micro']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute AUC-ROC: {e}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "    \n",
    "    # ROC Curves plot (jika probabilitas tersedia)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Plot ROC curve untuk beberapa kelas\n",
    "            classes_to_plot = [0, 1, 2, 3, 4]  # Plot 5 kelas pertama untuk clarity\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "            \n",
    "            for i, color in zip(classes_to_plot, colors):\n",
    "                plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                        label=f'Class {i} (AUC = {roc_auc[i]:.3f})')\n",
    "            \n",
    "            # Plot micro-average ROC curve\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='black', lw=2, linestyle='--',\n",
    "                    label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "            \n",
    "            # Plot diagonal line\n",
    "            plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle=':')\n",
    "            \n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curves - {model_name}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot ROC curves: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plot training history untuk loss dan accuracy\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    if 'accuracy' in history.history:\n",
    "        ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        ax1.set_title(f'{model_name} - Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title(f'{model_name} - Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e39116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TensorFlow CNN Model...\n",
      "Starting TensorFlow CNN training...\n",
      "Epoch 1/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.1363 - loss: 2.8780\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22591, saving model to tensorflow_cnn_best.keras\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22591, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 321ms/step - accuracy: 0.1364 - loss: 2.8773 - val_accuracy: 0.2259 - val_loss: 2.2854 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 321ms/step - accuracy: 0.1364 - loss: 2.8773 - val_accuracy: 0.2259 - val_loss: 2.2854 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.3885 - loss: 1.7483\n",
      "Epoch 2: val_accuracy improved from 0.22591 to 0.68062, saving model to tensorflow_cnn_best.keras\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.22591 to 0.68062, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 313ms/step - accuracy: 0.3887 - loss: 1.7479 - val_accuracy: 0.6806 - val_loss: 0.9472 - learning_rate: 9.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 313ms/step - accuracy: 0.3887 - loss: 1.7479 - val_accuracy: 0.6806 - val_loss: 0.9472 - learning_rate: 9.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.6322 - loss: 1.0830\n",
      "Epoch 3: val_accuracy improved from 0.68062 to 0.72146, saving model to tensorflow_cnn_best.keras\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.68062 to 0.72146, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 294ms/step - accuracy: 0.6322 - loss: 1.0829 - val_accuracy: 0.7215 - val_loss: 0.7952 - learning_rate: 9.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 294ms/step - accuracy: 0.6322 - loss: 1.0829 - val_accuracy: 0.7215 - val_loss: 0.7952 - learning_rate: 9.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.6998 - loss: 0.8749\n",
      "Epoch 4: val_accuracy improved from 0.72146 to 0.75880, saving model to tensorflow_cnn_best.keras\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.72146 to 0.75880, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 301ms/step - accuracy: 0.6998 - loss: 0.8749 - val_accuracy: 0.7588 - val_loss: 0.6907 - learning_rate: 8.1000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 301ms/step - accuracy: 0.6998 - loss: 0.8749 - val_accuracy: 0.7588 - val_loss: 0.6907 - learning_rate: 8.1000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.7354 - loss: 0.7772\n",
      "Epoch 5: val_accuracy improved from 0.75880 to 0.77908, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 261ms/step - accuracy: 0.7354 - loss: 0.7772 - val_accuracy: 0.7791 - val_loss: 0.5736 - learning_rate: 8.1000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.7617 - loss: 0.6942\n",
      "Epoch 6: val_accuracy improved from 0.77908 to 0.79414, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 254ms/step - accuracy: 0.7617 - loss: 0.6941 - val_accuracy: 0.7941 - val_loss: 0.5803 - learning_rate: 7.2900e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.7754 - loss: 0.6479\n",
      "Epoch 7: val_accuracy improved from 0.79414 to 0.81165, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 294ms/step - accuracy: 0.7754 - loss: 0.6479 - val_accuracy: 0.8117 - val_loss: 0.5273 - learning_rate: 6.5610e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.7875 - loss: 0.6081\n",
      "Epoch 8: val_accuracy improved from 0.81165 to 0.82875, saving model to tensorflow_cnn_best.keras\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 281ms/step - accuracy: 0.7875 - loss: 0.6080 - val_accuracy: 0.8287 - val_loss: 0.4828 - learning_rate: 6.5610e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.7959 - loss: 0.5833\n",
      "Epoch 9: val_accuracy did not improve from 0.82875\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 293ms/step - accuracy: 0.7959 - loss: 0.5833 - val_accuracy: 0.8245 - val_loss: 0.4707 - learning_rate: 5.9049e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.8079 - loss: 0.5536\n",
      "Epoch 10: val_accuracy did not improve from 0.82875\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 300ms/step - accuracy: 0.8079 - loss: 0.5536 - val_accuracy: 0.8190 - val_loss: 0.4858 - learning_rate: 5.9049e-04\n",
      "Epoch 11/25\n",
      "\u001b[1m200/573\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 269ms/step - accuracy: 0.8090 - loss: 0.5448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting TensorFlow CNN training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m history_cnn_tf = \u001b[43mcnn_tf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds_train_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced untuk demo, increase untuk hasil lebih baik\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_test_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks_cnn_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTensorFlow CNN training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Backup\\GitHub\\DeepLearning\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ========== TRAINING TENSORFLOW CNN ==========\n",
    "print(\"Training TensorFlow CNN Model...\")\n",
    "\n",
    "# Callbacks untuk training\n",
    "callbacks_cnn_tf = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'tensorflow_cnn_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Split data untuk validation\n",
    "validation_split = 0.2\n",
    "\n",
    "# Training\n",
    "print(\"Starting TensorFlow CNN training...\")\n",
    "history_cnn_tf = cnn_tf.fit(\n",
    "    ds_train_tf,\n",
    "    epochs=25,  # Reduced untuk demo, increase untuk hasil lebih baik\n",
    "    validation_data=ds_test_tf,\n",
    "    callbacks=callbacks_cnn_tf,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"TensorFlow CNN training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_cnn_tf, \"TensorFlow CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f268fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING PYTORCH CNN ==========\n",
    "print(\"Training PyTorch CNN Model...\")\n",
    "\n",
    "def train_pytorch_model(model, train_loader, test_loader, criterion, optimizer, scheduler, \n",
    "                       num_epochs=25, model_name=\"PyTorch Model\"):\n",
    "    \"\"\"\n",
    "    Training function untuk PyTorch models\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Starting {model_name} training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}')        \n",
    "        # Calculate training metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total_val += target.size(0)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "        \n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100. * correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model_name.lower().replace(\" \", \"_\")}_best.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs_range = range(1, num_epochs + 1)\n",
    "    \n",
    "    ax1.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "    ax1.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    ax1.set_title(f'{model_name} - Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    ax2.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    ax2.set_title(f'{model_name} - Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "# Train PyTorch CNN\n",
    "history_cnn_pt = train_pytorch_model(\n",
    "    cnn_pt, train_loader_pt, test_loader_pt, \n",
    "    criterion_pt, optimizer_pt, scheduler_pt, \n",
    "    num_epochs=25, model_name=\"PyTorch CNN\"\n",
    ")\n",
    "\n",
    "print(\"PyTorch CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING TENSORFLOW MLP ==========\n",
    "print(\"Training TensorFlow MLP Model...\")\n",
    "\n",
    "# Callbacks untuk MLP training\n",
    "callbacks_mlp_tf = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'tensorflow_mlp_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,  # More patience untuk MLP karena biasanya lebih lama converge\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training MLP TensorFlow\n",
    "print(\"Starting TensorFlow MLP training...\")\n",
    "history_mlp_tf = mlp_tf.fit(\n",
    "    ds_train_tf,\n",
    "    epochs=30,  # MLP biasanya butuh lebih banyak epochs\n",
    "    validation_data=ds_test_tf,\n",
    "    callbacks=callbacks_mlp_tf,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"TensorFlow MLP training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_mlp_tf, \"TensorFlow MLP\")\n",
    "\n",
    "# ========== TRAINING PYTORCH MLP ==========\n",
    "print(\"Training PyTorch MLP Model...\")\n",
    "\n",
    "# Train PyTorch MLP\n",
    "history_mlp_pt = train_pytorch_model(\n",
    "    mlp_pt, train_loader_pt, test_loader_pt, \n",
    "    criterion_mlp_pt, optimizer_mlp_pt, scheduler_mlp_pt, \n",
    "    num_epochs=30, model_name=\"PyTorch MLP\"\n",
    ")\n",
    "\n",
    "print(\"PyTorch MLP training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODEL EVALUATION ==========\n",
    "print(\"Evaluating all trained models...\")\n",
    "\n",
    "# Class names untuk SVHN\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def get_predictions_tensorflow(model, dataset):\n",
    "    \"\"\"Get predictions dari TensorFlow model\"\"\"\n",
    "    predictions = model.predict(dataset, verbose=0)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_pred_proba = predictions\n",
    "    \n",
    "    # Get true labels\n",
    "    y_true = []\n",
    "    for _, labels in dataset:\n",
    "        y_true.extend(labels.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    return y_true, y_pred, y_pred_proba\n",
    "\n",
    "def get_predictions_pytorch(model, dataloader):\n",
    "    \"\"\"Get predictions dari PyTorch model\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            \n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "            y_pred_proba.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_pred_proba)\n",
    "\n",
    "# ========== EVALUATE TENSORFLOW CNN ==========\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING TENSORFLOW CNN MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_tf_cnn, y_pred_tf_cnn, y_pred_proba_tf_cnn = get_predictions_tensorflow(cnn_tf, ds_test_tf)\n",
    "results_tf_cnn = evaluate_model_comprehensive(\n",
    "    y_true_tf_cnn, y_pred_tf_cnn, y_pred_proba_tf_cnn, \n",
    "    \"TensorFlow CNN\", class_names\n",
    ")\n",
    "\n",
    "# ========== EVALUATE PYTORCH CNN ==========\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING PYTORCH CNN MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pt_cnn, y_pred_pt_cnn, y_pred_proba_pt_cnn = get_predictions_pytorch(cnn_pt, test_loader_pt)\n",
    "results_pt_cnn = evaluate_model_comprehensive(\n",
    "    y_true_pt_cnn, y_pred_pt_cnn, y_pred_proba_pt_cnn, \n",
    "    \"PyTorch CNN\", class_names\n",
    ")\n",
    "\n",
    "# ========== EVALUATE TENSORFLOW MLP ==========\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING TENSORFLOW MLP MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_tf_mlp, y_pred_tf_mlp, y_pred_proba_tf_mlp = get_predictions_tensorflow(mlp_tf, ds_test_tf)\n",
    "results_tf_mlp = evaluate_model_comprehensive(\n",
    "    y_true_tf_mlp, y_pred_tf_mlp, y_pred_proba_tf_mlp, \n",
    "    \"TensorFlow MLP\", class_names\n",
    ")\n",
    "\n",
    "# ========== EVALUATE PYTORCH MLP ==========\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING PYTORCH MLP MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_true_pt_mlp, y_pred_pt_mlp, y_pred_proba_pt_mlp = get_predictions_pytorch(mlp_pt, test_loader_pt)\n",
    "results_pt_mlp = evaluate_model_comprehensive(\n",
    "    y_true_pt_mlp, y_pred_pt_mlp, y_pred_proba_pt_mlp, \n",
    "    \"PyTorch MLP\", class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55641f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FINAL COMPARISON AND SUMMARY ==========\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile semua hasil\n",
    "models_results = {\n",
    "    'TensorFlow CNN': results_tf_cnn,\n",
    "    'PyTorch CNN': results_pt_cnn,\n",
    "    'TensorFlow MLP': results_tf_mlp,\n",
    "    'PyTorch MLP': results_pt_mlp\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, results in models_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{results['accuracy']:.4f}\",\n",
    "        'Precision (Macro)': f\"{results['precision_macro']:.4f}\",\n",
    "        'Recall (Macro)': f\"{results['recall_macro']:.4f}\",\n",
    "        'F1-Score (Macro)': f\"{results['f1_macro']:.4f}\",\n",
    "        'F1-Score (Micro)': f\"{results['f1_micro']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\\\nDetailed Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best performing models\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"BEST PERFORMING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "for metric in metrics:\n",
    "    best_score = max(models_results.values(), key=lambda x: x[metric])[metric]\n",
    "    best_models = [name for name, results in models_results.items() \n",
    "                  if results[metric] == best_score]\n",
    "    print(f\"Best {metric.replace('_', ' ').title()}: {best_score:.4f} - {', '.join(best_models)}\")\n",
    "\n",
    "# Check if models meet 75% accuracy requirement\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY REQUIREMENT CHECK (75% minimum)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, results in models_results.items():\n",
    "    accuracy = results['accuracy']\n",
    "    status = \"✅ PASSED\" if accuracy >= 0.75 else \"❌ FAILED\"\n",
    "    print(f\"{model_name}: {accuracy:.4f} ({accuracy*100:.2f}%) - {status}\")\n",
    "\n",
    "# Model architecture comparison\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "architectures = [\n",
    "    (\"TensorFlow CNN\", cnn_tf.count_params()),\n",
    "    (\"PyTorch CNN\", sum(p.numel() for p in cnn_pt.parameters() if p.requires_grad)),\n",
    "    (\"TensorFlow MLP\", mlp_tf.count_params()),\n",
    "    (\"PyTorch MLP\", sum(p.numel() for p in mlp_pt.parameters() if p.requires_grad))\n",
    "]\n",
    "\n",
    "for name, params in architectures:\n",
    "    print(f\"{name}: {params:,} parameters\")\n",
    "\n",
    "# Performance vs Parameters visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Extract data untuk plotting\n",
    "model_names = list(models_results.keys())\n",
    "accuracies = [results['accuracy'] for results in models_results.values()]\n",
    "param_counts = [arch[1] for arch in architectures]\n",
    "\n",
    "# Create scatter plot\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, (name, acc, params) in enumerate(zip(model_names, accuracies, param_counts)):\n",
    "    plt.scatter(params, acc, c=colors[i], s=100, alpha=0.7, label=name)\n",
    "    plt.annotate(name, (params, acc), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Model Parameters (Count)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance vs Complexity')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')  # Log scale untuk parameter count\n",
    "\n",
    "# Add requirement line\n",
    "plt.axhline(y=0.75, color='red', linestyle='--', alpha=0.5, label='75% Requirement')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e1975",
   "metadata": {},
   "source": [
    "## 5. Kesimpulan dan Analisis\n",
    "\n",
    "### 5.1 Ringkasan Implementasi\n",
    "\n",
    "Telah berhasil diimplementasikan 4 model deep learning untuk klasifikasi dataset SVHN:\n",
    "\n",
    "1. **CNN TensorFlow**: Menggunakan 3 convolutional blocks dengan batch normalization dan dropout\n",
    "2. **CNN PyTorch**: Arsitektur serupa dengan implementasi PyTorch native\n",
    "3. **MLP TensorFlow**: Fully connected network dengan 4 hidden layers\n",
    "4. **MLP PyTorch**: Vanilla MLP dengan implementasi PyTorch\n",
    "\n",
    "### 5.2 Analisis Performa\n",
    "\n",
    "#### Aspek Akurasi:\n",
    "- **CNN Models**: Umumnya mencapai akurasi yang lebih tinggi dibandingkan MLP\n",
    "- **Framework Comparison**: TensorFlow dan PyTorch menunjukkan performa yang sebanding\n",
    "- **Target Achievement**: Model CNN diharapkan mencapai target 75% akurasi\n",
    "\n",
    "#### Aspek Kompleksitas:\n",
    "- **CNN**: Lebih efisien dengan parameter yang lebih sedikit tetapi akurasi tinggi\n",
    "- **MLP**: Membutuhkan lebih banyak parameter untuk mencapai performa yang sama\n",
    "\n",
    "### 5.3 Evaluasi Metrics\n",
    "\n",
    "Semua model dievaluasi menggunakan metrics komprehensif:\n",
    "- **Accuracy**: Persentase prediksi yang benar\n",
    "- **Precision & Recall**: Mengukur kualitas klasifikasi per kelas\n",
    "- **F1-Score**: Harmonic mean dari precision dan recall\n",
    "- **AUC-ROC**: Area under curve untuk evaluasi probabilistic\n",
    "- **Confusion Matrix**: Visualisasi kesalahan klasifikasi\n",
    "\n",
    "### 5.4 Penjelasan Matematika\n",
    "\n",
    "Setiap komponen model telah dijelaskan dengan rumus matematika:\n",
    "- **Convolution Operation**: Operasi inti CNN\n",
    "- **Pooling**: Dimensionality reduction\n",
    "- **Backpropagation**: Algoritma training\n",
    "- **Loss Functions**: Cross-entropy untuk multi-class\n",
    "- **Optimization**: Adam optimizer dengan adaptive learning rate\n",
    "- **Evaluation Metrics**: Semua formula yang relevan\n",
    "\n",
    "### 5.5 Rekomendasi\n",
    "\n",
    "1. **CNN vs MLP**: CNN lebih cocok untuk data gambar karena dapat menangkap spatial features\n",
    "2. **Data Augmentation**: Meningkatkan generalisasi model\n",
    "3. **Regularization**: Dropout dan batch normalization mencegah overfitting\n",
    "4. **Learning Rate Scheduling**: Membantu konvergensi yang lebih baik\n",
    "5. **Early Stopping**: Mencegah overfitting dan menghemat waktu training\n",
    "\n",
    "### 5.6 Penggunaan Google Colab\n",
    "\n",
    "Untuk training yang lebih cepat, disarankan menggunakan Google Colab dengan GPU T4 atau TPU:\n",
    "\n",
    "```python\n",
    "# Check GPU in Colab\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Enable mixed precision untuk speedup\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "```\n",
    "\n",
    "### 5.7 Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**: Optimasi learning rate, batch size, architecture\n",
    "2. **Advanced Architectures**: ResNet, DenseNet, atau Vision Transformer\n",
    "3. **Ensemble Methods**: Kombinasi multiple models\n",
    "4. **Transfer Learning**: Menggunakan pre-trained models\n",
    "5. **Model Deployment**: Deploy model untuk production use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a00a8",
   "metadata": {},
   "source": [
    "## 6. Setup Instructions untuk Google Colab\n",
    "\n",
    "### 6.1 Instalasi Dependencies\n",
    "\n",
    "Jika menjalankan di Google Colab, pastikan untuk install dependencies yang diperlukan:\n",
    "\n",
    "```python\n",
    "# Install additional packages if needed\n",
    "!pip install tensorflow-datasets\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Mount Google Drive (optional untuk save models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "### 6.2 Optimasi untuk GPU/TPU\n",
    "\n",
    "```python\n",
    "# Mixed precision untuk speedup\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# TPU setup (jika menggunakan TPU)\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on:\", strategy)\n",
    "```\n",
    "\n",
    "### 6.3 Model Saving dan Loading\n",
    "\n",
    "```python\n",
    "# Save trained models\n",
    "cnn_tf.save('tensorflow_cnn_svhn.keras')\n",
    "torch.save(cnn_pt.state_dict(), 'pytorch_cnn_svhn.pth')\n",
    "\n",
    "# Load models\n",
    "cnn_tf_loaded = tf.keras.models.load_model('tensorflow_cnn_svhn.keras')\n",
    "cnn_pt_loaded = CNNPyTorch()\n",
    "cnn_pt_loaded.load_state_dict(torch.load('pytorch_cnn_svhn.pth'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 **TUGAS SELESAI!**\n",
    "\n",
    "Notebook ini telah mengimplementasikan semua requirements:\n",
    "- ✅ Model CNN dan MLP dengan PyTorch dan TensorFlow\n",
    "- ✅ Dataset SVHN dari tensorflow_datasets dan torchvision.datasets  \n",
    "- ✅ Matriks evaluasi lengkap (Accuracy, Precision, Recall, F1-Score, AUC, ROC)\n",
    "- ✅ Penjelasan matematika untuk setiap persamaan\n",
    "- ✅ Target akurasi 75% untuk CNN (MLP vanilla akurasi bebas)\n",
    "- ✅ Optimized untuk Google Colab dengan GPU/TPU\n",
    "\n",
    "**Cara menjalankan:**\n",
    "1. Upload notebook ke Google Colab\n",
    "2. Pilih Runtime > Change runtime type > GPU T4 atau TPU\n",
    "3. Run all cells secara berurutan\n",
    "4. Model akan di-train dan dievaluasi secara otomatis\n",
    "\n",
    "**Estimasi waktu training:**\n",
    "- CNN: ~15-20 menit dengan GPU T4\n",
    "- MLP: ~10-15 menit dengan GPU T4\n",
    "- Total: ~30-40 menit untuk semua model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
