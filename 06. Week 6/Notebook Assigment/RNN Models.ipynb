{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0c221c",
   "metadata": {},
   "source": [
    "# RNN Models untuk Deteksi Sarkasme\n",
    "\n",
    "**Tugas Week 6 - Deep Learning**\n",
    "\n",
    "## Deskripsi Tugas:\n",
    "1. Buat model Deep Learning (PyTorch dan TensorFlow)\n",
    "2. Gunakan Matriks Evaluasi (Akurasi, Presisi, Recall, F1-Score, AUC, ROC)\n",
    "3. Visualisasikan matriks akurasi dan loss\n",
    "4. Lakukan Hyperparameter Tuning dengan Keras Tuner (TensorFlow)\n",
    "5. Akurasi pada Training dan Testing Set minimal 70%\n",
    "\n",
    "## Dataset:\n",
    "- **Sumber**: DeteksiSarkasme.json\n",
    "- **Task**: Binary Classification (Sarcastic vs Non-Sarcastic)\n",
    "- **Input**: Text headlines\n",
    "- **Output**: Binary label (0: Not Sarcastic, 1: Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea2fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: torch in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: keras-tuner in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: scikit-learn in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: filelock in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: kt-legacy in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\backup\\github\\deeplearning\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Libraries imported successfully!\n",
      "TensorFlow version: 2.19.0\n",
      "PyTorch version: 2.7.1+cpu\n",
      "GPU available: []\n",
      "PyTorch CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow torch transformers keras-tuner scikit-learn matplotlib seaborn wordcloud\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN, Embedding, Dropout, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras_tuner as kt\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319c324",
   "metadata": {},
   "source": [
    "## 1. Data Loading dan Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (26709, 3)\n",
      "\n",
      "Column names: ['article_link', 'headline', 'is_sarcastic']\n",
      "\n",
      "First few rows:\n",
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "2  https://local.theonion.com/mom-starting-to-fea...   \n",
      "3  https://politics.theonion.com/boehner-just-wan...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "\n",
      "                                            headline  is_sarcastic  \n",
      "0  former versace store clerk sues over secret 'b...             0  \n",
      "1  the 'roseanne' revival catches up to our thorn...             0  \n",
      "2  mom starting to fear son's web series closest ...             1  \n",
      "3  boehner just wants wife to listen, not come up...             1  \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26709 entries, 0 to 26708\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   article_link  26709 non-null  object\n",
      " 1   headline      26709 non-null  object\n",
      " 2   is_sarcastic  26709 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 626.1+ KB\n",
      "None\n",
      "\n",
      "Class distribution:\n",
      "is_sarcastic\n",
      "0    14985\n",
      "1    11724\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class percentage:\n",
      "is_sarcastic\n",
      "0    56.104684\n",
      "1    43.895316\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../Dataset/DeteksiSarkasme.json\"\n",
    "\n",
    "# Read JSON data\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['is_sarcastic'].value_counts())\n",
    "\n",
    "print(\"\\nClass percentage:\")\n",
    "print(df['is_sarcastic'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Class distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "df['is_sarcastic'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])\n",
    "plt.title('Distribution of Sarcastic vs Non-Sarcastic Headlines')\n",
    "plt.xlabel('Is Sarcastic')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Sarcastic', 'Sarcastic'], rotation=0)\n",
    "\n",
    "# 2. Text length distribution\n",
    "df['text_length'] = df['headline'].str.len()\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df['text_length'], bins=50, alpha=0.7, color='green')\n",
    "plt.title('Distribution of Headline Lengths')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 3. Text length by class\n",
    "plt.subplot(2, 3, 3)\n",
    "df[df['is_sarcastic'] == 0]['text_length'].hist(alpha=0.5, label='Non-Sarcastic', bins=30, color='blue')\n",
    "df[df['is_sarcastic'] == 1]['text_length'].hist(alpha=0.5, label='Sarcastic', bins=30, color='red')\n",
    "plt.title('Text Length Distribution by Class')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Word count distribution\n",
    "df['word_count'] = df['headline'].str.split().str.len()\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(df['word_count'], bins=30, alpha=0.7, color='purple')\n",
    "plt.title('Distribution of Word Counts')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 5. Word count by class\n",
    "plt.subplot(2, 3, 5)\n",
    "df[df['is_sarcastic'] == 0]['word_count'].hist(alpha=0.5, label='Non-Sarcastic', bins=20, color='blue')\n",
    "df[df['is_sarcastic'] == 1]['word_count'].hist(alpha=0.5, label='Sarcastic', bins=20, color='red')\n",
    "plt.title('Word Count Distribution by Class')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 6. Box plot for text length by class\n",
    "plt.subplot(2, 3, 6)\n",
    "df.boxplot(column='text_length', by='is_sarcastic', ax=plt.gca())\n",
    "plt.title('Text Length by Class')\n",
    "plt.xlabel('Is Sarcastic')\n",
    "plt.ylabel('Text Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics summary\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['text_length'].describe())\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds for sarcastic and non-sarcastic headlines\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Sarcastic headlines word cloud\n",
    "sarcastic_text = ' '.join(df[df['is_sarcastic'] == 1]['headline'].values)\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud_sarcastic = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(sarcastic_text)\n",
    "plt.imshow(wordcloud_sarcastic, interpolation='bilinear')\n",
    "plt.title('Word Cloud - Sarcastic Headlines', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "# Non-sarcastic headlines word cloud\n",
    "non_sarcastic_text = ' '.join(df[df['is_sarcastic'] == 0]['headline'].values)\n",
    "plt.subplot(1, 2, 2)\n",
    "wordcloud_non_sarcastic = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(non_sarcastic_text)\n",
    "plt.imshow(wordcloud_non_sarcastic, interpolation='bilinear')\n",
    "plt.title('Word Cloud - Non-Sarcastic Headlines', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample headlines\n",
    "print(\"Sample Sarcastic Headlines:\")\n",
    "print(df[df['is_sarcastic'] == 1]['headline'].head(5).values)\n",
    "print(\"\\nSample Non-Sarcastic Headlines:\")\n",
    "print(df[df['is_sarcastic'] == 0]['headline'].head(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf393c0",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation except apostrophes\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned headlines:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {df['headline'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_headline'].iloc[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Prepare data\n",
    "texts = df['cleaned_headline'].values\n",
    "labels = df['is_sarcastic'].values\n",
    "\n",
    "# Set parameters\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Create tokenizer for TensorFlow\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of labels: {labels.shape}\")\n",
    "\n",
    "# Word frequency analysis\n",
    "word_freq = Counter()\n",
    "for text in texts:\n",
    "    word_freq.update(text.split())\n",
    "\n",
    "print(f\"\\nMost common words:\")\n",
    "for word, freq in word_freq.most_common(10):\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Sequence length distribution\n",
    "seq_lengths = [len(seq) for seq in sequences]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(seq_lengths, bins=50, alpha=0.7, color='blue')\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='red', linestyle='--', label=f'Max Length ({MAX_SEQUENCE_LENGTH})')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Percentage of sequences that fit in max length: {(np.array(seq_lengths) <= MAX_SEQUENCE_LENGTH).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} ({c/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set class distribution:\")\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} ({c/len(y_val)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} ({c/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c8225",
   "metadata": {},
   "source": [
    "## 3. TensorFlow/Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30815ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(rnn_type='LSTM', units=64, dropout=0.5, recurrent_dropout=0.5):\n",
    "    \"\"\"Create RNN model\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        \n",
    "        # Choose RNN type\n",
    "        {'SimpleRNN': SimpleRNN(units, dropout=dropout, recurrent_dropout=recurrent_dropout),\n",
    "         'LSTM': LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout),\n",
    "         'GRU': GRU(units, dropout=dropout, recurrent_dropout=recurrent_dropout),\n",
    "         'BiLSTM': Bidirectional(LSTM(units//2, dropout=dropout, recurrent_dropout=recurrent_dropout)),\n",
    "         'BiGRU': Bidirectional(GRU(units//2, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        }[rnn_type],\n",
    "        \n",
    "        Dropout(dropout),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_advanced_rnn_model():\n",
    "    \"\"\"Create advanced RNN model with multiple layers\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        \n",
    "        # First LSTM layer\n",
    "        LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "print(\"Model creation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different RNN models\n",
    "models_tf = {}\n",
    "histories_tf = {}\n",
    "\n",
    "# List of models to train\n",
    "model_types = ['SimpleRNN', 'LSTM', 'GRU', 'BiLSTM', 'BiGRU']\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_type} model...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_rnn_model(rnn_type=model_type, units=64)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store model and history\n",
    "    models_tf[model_type] = model\n",
    "    histories_tf[model_type] = history\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"{model_type} Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nAll TensorFlow models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train advanced model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Advanced Multi-layer LSTM model...\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "advanced_model = create_advanced_rnn_model()\n",
    "advanced_model.summary()\n",
    "\n",
    "advanced_history = advanced_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Store advanced model\n",
    "models_tf['Advanced_LSTM'] = advanced_model\n",
    "histories_tf['Advanced_LSTM'] = advanced_history\n",
    "\n",
    "# Evaluate advanced model\n",
    "test_loss, test_accuracy = advanced_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Advanced LSTM Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Advanced model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with Keras Tuner\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def build_tuning_model(hp):\n",
    "    \"\"\"Build model for hyperparameter tuning\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(MAX_VOCAB_SIZE, \n",
    "                       hp.Int('embedding_dim', min_value=50, max_value=200, step=50),\n",
    "                       input_length=MAX_SEQUENCE_LENGTH))\n",
    "    \n",
    "    # RNN type selection\n",
    "    rnn_type = hp.Choice('rnn_type', ['LSTM', 'GRU', 'BiLSTM'])\n",
    "    units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    dropout = hp.Float('dropout', min_value=0.2, max_value=0.6, step=0.1)\n",
    "    recurrent_dropout = hp.Float('recurrent_dropout', min_value=0.2, max_value=0.6, step=0.1)\n",
    "    \n",
    "    if rnn_type == 'LSTM':\n",
    "        model.add(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    elif rnn_type == 'GRU':\n",
    "        model.add(GRU(units, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    elif rnn_type == 'BiLSTM':\n",
    "        model.add(Bidirectional(LSTM(units//2, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    dense_units = hp.Int('dense_units', min_value=16, max_value=64, step=16)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dense_dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuning_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Reduced for faster execution\n",
    "    directory='hyperparameter_tuning',\n",
    "    project_name='sarcasm_detection'\n",
    ")\n",
    "\n",
    "# Search for best hyperparameters\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,  # Reduced epochs for faster tuning\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in best_hyperparameters.values.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train best model with more epochs\n",
    "print(\"\\nTraining best model with optimal hyperparameters...\")\n",
    "best_history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Store best model\n",
    "models_tf['Best_Tuned'] = best_model\n",
    "histories_tf['Best_Tuned'] = best_history\n",
    "\n",
    "# Evaluate best model\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Best Tuned Model Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c62082",
   "metadata": {},
   "source": [
    "## 4. PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# PyTorch RNN Model classes\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.5, rnn_type='LSTM'):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                              dropout=dropout if n_layers > 1 else 0, \n",
    "                              batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
    "                             dropout=dropout if n_layers > 1 else 0, \n",
    "                             batch_first=True)\n",
    "        elif rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, \n",
    "                             dropout=dropout if n_layers > 1 else 0, \n",
    "                             batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Use the last output for classification\n",
    "        output = self.fc(rnn_out[:, -1, :])\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "class BiRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers=1, dropout=0.5, rnn_type='LSTM'):\n",
    "        super(BiRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                              dropout=dropout if n_layers > 1 else 0, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
    "                             dropout=dropout if n_layers > 1 else 0, \n",
    "                             batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Use the last output for classification\n",
    "        output = self.fc(rnn_out[:, -1, :])\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SarcasmDataset(X_train, y_train)\n",
    "val_dataset = SarcasmDataset(X_val, y_val)\n",
    "test_dataset = SarcasmDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"PyTorch datasets and models defined!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch training functions\n",
    "def train_pytorch_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                       scheduler=None, num_epochs=20, device='cpu'):\n",
    "    \"\"\"Train PyTorch model\"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            print('-' * 50)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_loss': val_losses,\n",
    "        'val_acc': val_accuracies\n",
    "    }\n",
    "\n",
    "def evaluate_pytorch_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluate PyTorch model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(true_labels), np.array(probabilities)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"PyTorch training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ae75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyTorch models\n",
    "models_pytorch = {}\n",
    "histories_pytorch = {}\n",
    "\n",
    "# Model configurations\n",
    "pytorch_configs = [\n",
    "    {'name': 'LSTM', 'model_class': RNNClassifier, 'rnn_type': 'LSTM'},\n",
    "    {'name': 'GRU', 'model_class': RNNClassifier, 'rnn_type': 'GRU'},\n",
    "    {'name': 'RNN', 'model_class': RNNClassifier, 'rnn_type': 'RNN'},\n",
    "    {'name': 'BiLSTM', 'model_class': BiRNNClassifier, 'rnn_type': 'LSTM'},\n",
    "    {'name': 'BiGRU', 'model_class': BiRNNClassifier, 'rnn_type': 'GRU'},\n",
    "]\n",
    "\n",
    "for config in pytorch_configs:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training PyTorch {config['name']} model...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = config['model_class'](\n",
    "        vocab_size=MAX_VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=64,\n",
    "        output_dim=1,\n",
    "        n_layers=1,\n",
    "        dropout=0.5,\n",
    "        rnn_type=config['rnn_type']\n",
    "    )\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, history = train_pytorch_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        scheduler, num_epochs=20, device=device\n",
    "    )\n",
    "    \n",
    "    # Store model and history\n",
    "    models_pytorch[config['name']] = trained_model\n",
    "    histories_pytorch[config['name']] = history\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    predictions, true_labels, probabilities = evaluate_pytorch_model(trained_model, test_loader, device)\n",
    "    test_accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"PyTorch {config['name']} Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nAll PyTorch models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a7ad2",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation dan Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models():\n",
    "    \"\"\"Comprehensive evaluation of all models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Evaluating TensorFlow Models:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, model in models_tf.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[f\"TF_{name}\"] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Evaluating PyTorch Models:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, model in models_pytorch.items():\n",
    "        print(f\"\\nEvaluating PyTorch {name}...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred, _, y_pred_proba = evaluate_pytorch_model(model, test_loader, device)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[f\"PyTorch_{name}\"] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = evaluate_all_models()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'AUC': metrics['auc']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY OF ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Check which models meet the 70% accuracy requirement\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODELS MEETING 70% ACCURACY REQUIREMENT:\")\n",
    "print(\"=\"*60)\n",
    "passing_models = summary_df[summary_df['Accuracy'] >= 0.70]\n",
    "if len(passing_models) > 0:\n",
    "    print(passing_models.to_string(index=False, float_format='%.4f'))\n",
    "else:\n",
    "    print(\"No models achieved 70% accuracy requirement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605443fe",
   "metadata": {},
   "source": [
    "## 6. Visualisasi Training History dan Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for TensorFlow models\n",
    "def plot_training_history_tf():\n",
    "    \"\"\"Plot training history for TensorFlow models\"\"\"\n",
    "    n_models = len(histories_tf)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(5*n_models, 10))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, (name, history) in enumerate(histories_tf.items()):\n",
    "        # Plot accuracy\n",
    "        axes[0, i].plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
    "        axes[0, i].plot(history.history['val_accuracy'], label='Val Accuracy', color='red')\n",
    "        axes[0, i].set_title(f'{name} - Accuracy')\n",
    "        axes[0, i].set_xlabel('Epoch')\n",
    "        axes[0, i].set_ylabel('Accuracy')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1, i].plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "        axes[1, i].plot(history.history['val_loss'], label='Val Loss', color='red')\n",
    "        axes[1, i].set_title(f'{name} - Loss')\n",
    "        axes[1, i].set_xlabel('Epoch')\n",
    "        axes[1, i].set_ylabel('Loss')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history for PyTorch models\n",
    "def plot_training_history_pytorch():\n",
    "    \"\"\"Plot training history for PyTorch models\"\"\"\n",
    "    n_models = len(histories_pytorch)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(5*n_models, 10))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, (name, history) in enumerate(histories_pytorch.items()):\n",
    "        # Plot accuracy\n",
    "        axes[0, i].plot(history['train_acc'], label='Train Accuracy', color='blue')\n",
    "        axes[0, i].plot(history['val_acc'], label='Val Accuracy', color='red')\n",
    "        axes[0, i].set_title(f'PyTorch {name} - Accuracy')\n",
    "        axes[0, i].set_xlabel('Epoch')\n",
    "        axes[0, i].set_ylabel('Accuracy')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[1, i].plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[1, i].plot(history['val_loss'], label='Val Loss', color='red')\n",
    "        axes[1, i].set_title(f'PyTorch {name} - Loss')\n",
    "        axes[1, i].set_xlabel('Epoch')\n",
    "        axes[1, i].set_ylabel('Loss')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"TensorFlow Models Training History:\")\n",
    "plot_training_history_tf()\n",
    "\n",
    "print(\"\\nPyTorch Models Training History:\")\n",
    "plot_training_history_pytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "def plot_confusion_matrices():\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    # Get top 6 models based on accuracy\n",
    "    top_models = summary_df.head(6)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_models.iterrows()):\n",
    "        model_name = row['Model']\n",
    "        y_pred = evaluation_results[model_name]['y_pred']\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                   xticklabels=['Non-Sarcastic', 'Sarcastic'],\n",
    "                   yticklabels=['Non-Sarcastic', 'Sarcastic'])\n",
    "        axes[i].set_title(f'{model_name}\\nAccuracy: {row[\"Accuracy\"]:.4f}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "def plot_roc_curves():\n",
    "    \"\"\"Plot ROC curves for all models\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(evaluation_results)))\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(evaluation_results.items()):\n",
    "        y_pred_proba = results['y_pred_proba']\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = results['auc']\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[i], \n",
    "                label=f'{model_name} (AUC = {auc_score:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves - All Models')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot model comparison\n",
    "def plot_model_comparison():\n",
    "    \"\"\"Plot model comparison metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    summary_df.plot(x='Model', y='Accuracy', kind='bar', ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title('Model Accuracy Comparison')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_xlabel('Model')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0.7, color='red', linestyle='--', label='70% Threshold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Multiple metrics comparison\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "    x = np.arange(len(summary_df))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        axes[1].bar(x + i*width, summary_df[metric], width, label=metric)\n",
    "    \n",
    "    axes[1].set_title('Model Performance Metrics Comparison')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].set_xticks(x + width * 2)\n",
    "    axes[1].set_xticklabels(summary_df['Model'], rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Confusion Matrices:\")\n",
    "plot_confusion_matrices()\n",
    "\n",
    "print(\"\\nROC Curves:\")\n",
    "plot_roc_curves()\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "plot_model_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d15e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports for best models\n",
    "def show_classification_reports():\n",
    "    \"\"\"Show detailed classification reports for top 3 models\"\"\"\n",
    "    top_3_models = summary_df.head(3)\n",
    "    \n",
    "    print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for _, row in top_3_models.iterrows():\n",
    "        model_name = row['Model']\n",
    "        y_pred = evaluation_results[model_name]['y_pred']\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(classification_report(y_test, y_pred, \n",
    "                                  target_names=['Non-Sarcastic', 'Sarcastic'],\n",
    "                                  digits=4))\n",
    "\n",
    "show_classification_reports()\n",
    "\n",
    "# Error analysis\n",
    "def analyze_errors():\n",
    "    \"\"\"Analyze prediction errors\"\"\"\n",
    "    best_model_name = summary_df.iloc[0]['Model']\n",
    "    best_predictions = evaluation_results[best_model_name]['y_pred']\n",
    "    best_probabilities = evaluation_results[best_model_name]['y_pred_proba']\n",
    "    \n",
    "    # Find misclassified examples\n",
    "    incorrect_mask = (best_predictions != y_test)\n",
    "    incorrect_indices = np.where(incorrect_mask)[0]\n",
    "    \n",
    "    print(f\"\\nERROR ANALYSIS - {best_model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total misclassified examples: {len(incorrect_indices)}\")\n",
    "    print(f\"Error rate: {len(incorrect_indices)/len(y_test)*100:.2f}%\")\n",
    "    \n",
    "    # False positives (predicted sarcastic, actually not)\n",
    "    fp_mask = (best_predictions == 1) & (y_test == 0)\n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    \n",
    "    # False negatives (predicted not sarcastic, actually sarcastic)\n",
    "    fn_mask = (best_predictions == 0) & (y_test == 1)\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    \n",
    "    print(f\"\\nFalse Positives: {len(fp_indices)}\")\n",
    "    print(f\"False Negatives: {len(fn_indices)}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    if len(fp_indices) > 0:\n",
    "        print(f\"\\nSample False Positives (predicted sarcastic, actually not):\")\n",
    "        for i, idx in enumerate(fp_indices[:3]):\n",
    "            test_idx = X_test.shape[0] - len(y_test) + idx if idx < len(y_test) else idx\n",
    "            headline = df.iloc[test_idx]['headline'] if test_idx < len(df) else \"N/A\"\n",
    "            prob = best_probabilities[idx]\n",
    "            print(f\"{i+1}. Prob: {prob:.3f} - {headline}\")\n",
    "    \n",
    "    if len(fn_indices) > 0:\n",
    "        print(f\"\\nSample False Negatives (predicted not sarcastic, actually sarcastic):\")\n",
    "        for i, idx in enumerate(fn_indices[:3]):\n",
    "            test_idx = X_test.shape[0] - len(y_test) + idx if idx < len(y_test) else idx\n",
    "            headline = df.iloc[test_idx]['headline'] if test_idx < len(df) else \"N/A\"\n",
    "            prob = best_probabilities[idx]\n",
    "            print(f\"{i+1}. Prob: {prob:.3f} - {headline}\")\n",
    "\n",
    "analyze_errors()\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = summary_df.iloc[0]\n",
    "print(f\"Best Overall Model: {best_model['Model']}\")\n",
    "print(f\"Best Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"Best F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "print(f\"Best AUC: {best_model['AUC']:.4f}\")\n",
    "\n",
    "models_above_70 = summary_df[summary_df['Accuracy'] >= 0.70]\n",
    "print(f\"\\nModels achieving 70% accuracy: {len(models_above_70)}\")\n",
    "\n",
    "if len(models_above_70) > 0:\n",
    "    print(f\"Average accuracy of models 70%: {models_above_70['Accuracy'].mean():.4f}\")\n",
    "    print(f\"Best performing framework: {models_above_70.iloc[0]['Model'].split('_')[0]}\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TUGAS COMPLETED SUCCESSFULLY!\")\n",
    "print(\" Models created using both PyTorch and TensorFlow\")\n",
    "print(\" Evaluation metrics calculated (Accuracy, Precision, Recall, F1, AUC)\")\n",
    "print(\" Training history and loss visualized\")\n",
    "print(\" Hyperparameter tuning performed with Keras Tuner\")\n",
    "print(\" Model performance meets requirements\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
