{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4998623e",
   "metadata": {},
   "source": [
    "# GRU Models for Sarcasm Detection\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements GRU (Gated Recurrent Unit) models for sarcasm detection using both PyTorch and TensorFlow frameworks. The task involves:\n",
    "\n",
    "1. **Binary Classification**: Detecting sarcasm in news headlines\n",
    "2. **Deep Learning Models**: GRU-based architectures in PyTorch and TensorFlow\n",
    "3. **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, AUC, ROC\n",
    "4. **Hyperparameter Tuning**: Using Keras Tuner for TensorFlow model optimization\n",
    "5. **Target Performance**: Minimum 80% accuracy on both training and testing sets\n",
    "\n",
    "## Dataset\n",
    "- **Source**: DeteksiSarkasme.json\n",
    "- **Features**: News headlines\n",
    "- **Target**: Binary classification (0: Not Sarcastic, 1: Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Keras Tuner for hyperparameter tuning\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "    KERAS_TUNER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Keras Tuner not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'keras-tuner'])\n",
    "    import keras_tuner as kt\n",
    "    KERAS_TUNER_AVAILABLE = True\n",
    "\n",
    "# Scikit-learn for metrics and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"GPU Available (PyTorch):\", torch.cuda.is_available())\n",
    "print(\"GPU Available (TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983740a",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7057d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Dataset/DeteksiSarkasme.json'\n",
    "\n",
    "# Read JSON data line by line\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['is_sarcastic'].value_counts())\n",
    "print(\"\\nTarget distribution (percentage):\")\n",
    "print(df['is_sarcastic'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Target distribution\n",
    "df['is_sarcastic'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'orange'])\n",
    "axes[0,0].set_title('Sarcasm Distribution')\n",
    "axes[0,0].set_xlabel('Is Sarcastic')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].set_xticklabels(['Not Sarcastic', 'Sarcastic'], rotation=0)\n",
    "\n",
    "# 2. Headline length distribution\n",
    "df['headline_length'] = df['headline'].str.len()\n",
    "df['headline_length'].hist(bins=50, ax=axes[0,1], color='lightgreen', alpha=0.7)\n",
    "axes[0,1].set_title('Headline Length Distribution')\n",
    "axes[0,1].set_xlabel('Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Word count distribution\n",
    "df['word_count'] = df['headline'].str.split().str.len()\n",
    "df['word_count'].hist(bins=30, ax=axes[1,0], color='salmon', alpha=0.7)\n",
    "axes[1,0].set_title('Word Count Distribution')\n",
    "axes[1,0].set_xlabel('Number of Words')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Length comparison by sarcasm\n",
    "df.boxplot(column='headline_length', by='is_sarcastic', ax=axes[1,1])\n",
    "axes[1,1].set_title('Headline Length by Sarcasm')\n",
    "axes[1,1].set_xlabel('Is Sarcastic')\n",
    "axes[1,1].set_ylabel('Length (characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nHeadline Length Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['headline_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e268985",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep punctuation that might be important for sarcasm\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?;:\\'\\\"-]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "\n",
    "# Show examples of cleaned text\n",
    "print(\"Original vs Cleaned Headlines:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {df['headline'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_headline'].iloc[i]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df['cleaned_headline'].values\n",
    "y = df['is_sarcastic'].values\n",
    "\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {Counter(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c29a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {Counter(y_train)}\")\n",
    "print(f\"Test class distribution: {Counter(y_test)}\")\n",
    "\n",
    "# Create validation split from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter validation split:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e3944",
   "metadata": {},
   "source": [
    "## 3. PyTorch GRU Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tokenization and dataset preparation\n",
    "class TextTokenizer:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            word_freq.update(text.split())\n",
    "        \n",
    "        # Create vocabulary with most frequent words\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "        for word, freq in word_freq.most_common(self.max_vocab_size - 2):\n",
    "            idx = len(self.word_to_idx)\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "            \n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = [self.word_to_idx.get(word, 1) for word in text.split()]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts)\n",
    "        return self.transform(texts)\n",
    "\n",
    "# Initialize tokenizer and fit on training data\n",
    "pytorch_tokenizer = TextTokenizer(max_vocab_size=10000)\n",
    "X_train_seq = pytorch_tokenizer.fit_transform(X_train)\n",
    "X_val_seq = pytorch_tokenizer.transform(X_val)\n",
    "X_test_seq = pytorch_tokenizer.transform(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 50  # Maximum sequence length\n",
    "def pad_sequences_pytorch(sequences, max_len, pad_value=0):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            padded.append(seq[:max_len])\n",
    "        else:\n",
    "            padded.append(seq + [pad_value] * (max_len - len(seq)))\n",
    "    return np.array(padded)\n",
    "\n",
    "X_train_padded = pad_sequences_pytorch(X_train_seq, max_length)\n",
    "X_val_padded = pad_sequences_pytorch(X_val_seq, max_length)\n",
    "X_test_padded = pad_sequences_pytorch(X_test_seq, max_length)\n",
    "\n",
    "print(f\"PyTorch padded shapes:\")\n",
    "print(f\"Train: {X_train_padded.shape}\")\n",
    "print(f\"Validation: {X_val_padded.shape}\")\n",
    "print(f\"Test: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.LongTensor(texts)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SarcasmDataset(X_train_padded, y_train)\n",
    "val_dataset = SarcasmDataset(X_val_padded, y_val)\n",
    "test_dataset = SarcasmDataset(X_test_padded, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# PyTorch GRU Model\n",
    "class GRUSarcasmClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(GRUSarcasmClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(last_output)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return self.sigmoid(x).squeeze()\n",
    "\n",
    "# Initialize PyTorch model\n",
    "pytorch_model = GRUSarcasmClassifier(\n",
    "    vocab_size=pytorch_tokenizer.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"PyTorch Model Architecture:\")\n",
    "print(pytorch_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in pytorch_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch training function\n",
    "def train_pytorch_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    patience = 7\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_texts, batch_labels in train_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in val_loader:\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss_avg)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_avg)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_pytorch_gru_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "              f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the PyTorch model\n",
    "print(\"Training PyTorch GRU Model...\")\n",
    "pytorch_train_losses, pytorch_val_losses, pytorch_train_accs, pytorch_val_accs = train_pytorch_model(\n",
    "    pytorch_model, train_loader, val_loader, epochs=30, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model evaluation\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in test_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_texts)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_probabilities.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_probabilities)\n",
    "\n",
    "# Load best model and evaluate\n",
    "pytorch_model.load_state_dict(torch.load('best_pytorch_gru_model.pth'))\n",
    "pytorch_predictions, pytorch_true_labels, pytorch_probabilities = evaluate_pytorch_model(pytorch_model, test_loader)\n",
    "\n",
    "# Calculate metrics\n",
    "pytorch_accuracy = accuracy_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_precision = precision_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_recall = recall_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_f1 = f1_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_auc = roc_auc_score(pytorch_true_labels, pytorch_probabilities)\n",
    "\n",
    "print(\"PyTorch GRU Model Results:\")\n",
    "print(f\"Accuracy: {pytorch_accuracy:.4f}\")\n",
    "print(f\"Precision: {pytorch_precision:.4f}\")\n",
    "print(f\"Recall: {pytorch_recall:.4f}\")\n",
    "print(f\"F1-Score: {pytorch_f1:.4f}\")\n",
    "print(f\"AUC: {pytorch_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(pytorch_true_labels, pytorch_predictions, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae77a3",
   "metadata": {},
   "source": [
    "## 4. TensorFlow/Keras GRU Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow tokenization and data preparation\n",
    "max_vocab_size = 10000\n",
    "max_length = 50\n",
    "\n",
    "# Initialize tokenizer\n",
    "tf_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
    "tf_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_tf = tf_tokenizer.texts_to_sequences(X_train)\n",
    "X_val_tf = tf_tokenizer.texts_to_sequences(X_val)\n",
    "X_test_tf = tf_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_tf = pad_sequences(X_train_tf, maxlen=max_length, padding='post')\n",
    "X_val_tf = pad_sequences(X_val_tf, maxlen=max_length, padding='post')\n",
    "X_test_tf = pad_sequences(X_test_tf, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"TensorFlow tokenizer vocabulary size: {len(tf_tokenizer.word_index) + 1}\")\n",
    "print(f\"TensorFlow padded shapes:\")\n",
    "print(f\"Train: {X_train_tf.shape}\")\n",
    "print(f\"Validation: {X_val_tf.shape}\")\n",
    "print(f\"Test: {X_test_tf.shape}\")\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_tf = np.array(y_train)\n",
    "y_val_tf = np.array(y_val)\n",
    "y_test_tf = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow GRU Model\n",
    "def create_gru_model(vocab_size, embedding_dim=128, gru_units=128, dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        Bidirectional(GRU(gru_units, return_sequences=True, dropout=dropout_rate)),\n",
    "        Bidirectional(GRU(gru_units//2, dropout=dropout_rate)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create TensorFlow model\n",
    "tf_model = create_gru_model(\n",
    "    vocab_size=len(tf_tokenizer.word_index) + 1,\n",
    "    embedding_dim=128,\n",
    "    gru_units=128,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "tf_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"TensorFlow Model Architecture:\")\n",
    "tf_model.summary()\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_tensorflow_gru_model.keras', monitor='val_accuracy', \n",
    "                   save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining TensorFlow GRU Model...\")\n",
    "tf_history = tf_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow model evaluation\n",
    "# Load best model\n",
    "tf_model = tf.keras.models.load_model('best_tensorflow_gru_model.keras')\n",
    "\n",
    "# Make predictions\n",
    "tf_predictions_prob = tf_model.predict(X_test_tf)\n",
    "tf_predictions = (tf_predictions_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "tf_accuracy = accuracy_score(y_test_tf, tf_predictions)\n",
    "tf_precision = precision_score(y_test_tf, tf_predictions)\n",
    "tf_recall = recall_score(y_test_tf, tf_predictions)\n",
    "tf_f1 = f1_score(y_test_tf, tf_predictions)\n",
    "tf_auc = roc_auc_score(y_test_tf, tf_predictions_prob)\n",
    "\n",
    "print(\"TensorFlow GRU Model Results:\")\n",
    "print(f\"Accuracy: {tf_accuracy:.4f}\")\n",
    "print(f\"Precision: {tf_precision:.4f}\")\n",
    "print(f\"Recall: {tf_recall:.4f}\")\n",
    "print(f\"F1-Score: {tf_f1:.4f}\")\n",
    "print(f\"AUC: {tf_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tf, tf_predictions, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))\n",
    "\n",
    "# Training and validation accuracy/loss\n",
    "train_acc_tf = tf_history.history['accuracy']\n",
    "val_acc_tf = tf_history.history['val_accuracy']\n",
    "train_loss_tf = tf_history.history['loss']\n",
    "val_loss_tf = tf_history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e694423",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Keras Tuner\n",
    "def build_tuner_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=len(tf_tokenizer.word_index) + 1,\n",
    "        output_dim=hp.Int('embedding_dim', min_value=64, max_value=256, step=32),\n",
    "        input_length=max_length\n",
    "    ))\n",
    "    \n",
    "    # First GRU layer\n",
    "    model.add(Bidirectional(GRU(\n",
    "        hp.Int('gru_units_1', min_value=64, max_value=256, step=32),\n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    # Second GRU layer\n",
    "    model.add(Bidirectional(GRU(\n",
    "        hp.Int('gru_units_2', min_value=32, max_value=128, step=16),\n",
    "        dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        hp.Int('dense_units', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuner_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Reduce for faster execution\n",
    "    directory='keras_tuner',\n",
    "    project_name='gru_sarcasm_detection'\n",
    ")\n",
    "\n",
    "print(\"Tuner search space summary:\")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Search for best hyperparameters\n",
    "print(\"\\nStarting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_tf, y_train_tf,\n",
    "    epochs=10,  # Reduce epochs for faster tuning\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param in ['embedding_dim', 'gru_units_1', 'gru_units_2', 'dense_units', \n",
    "              'dropout_1', 'dropout_2', 'dropout_3', 'learning_rate']:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee185fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model with optimal hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Extended training with best hyperparameters\n",
    "callbacks_tuned = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_tuned_gru_model.keras', monitor='val_accuracy', \n",
    "                   save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "print(\"Training tuned model with extended epochs...\")\n",
    "tuned_history = best_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks_tuned,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_model = tf.keras.models.load_model('best_tuned_gru_model.keras')\n",
    "tuned_predictions_prob = best_model.predict(X_test_tf)\n",
    "tuned_predictions = (tuned_predictions_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics for tuned model\n",
    "tuned_accuracy = accuracy_score(y_test_tf, tuned_predictions)\n",
    "tuned_precision = precision_score(y_test_tf, tuned_predictions)\n",
    "tuned_recall = recall_score(y_test_tf, tuned_predictions)\n",
    "tuned_f1 = f1_score(y_test_tf, tuned_predictions)\n",
    "tuned_auc = roc_auc_score(y_test_tf, tuned_predictions_prob)\n",
    "\n",
    "print(\"\\nTuned TensorFlow GRU Model Results:\")\n",
    "print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"Precision: {tuned_precision:.4f}\")\n",
    "print(f\"Recall: {tuned_recall:.4f}\")\n",
    "print(f\"F1-Score: {tuned_f1:.4f}\")\n",
    "print(f\"AUC: {tuned_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5cc62",
   "metadata": {},
   "source": [
    "## 6. Model Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Performance Visualization\n",
    "\n",
    "# 1. Training History Plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# PyTorch Training History\n",
    "epochs_pytorch = range(1, len(pytorch_train_losses) + 1)\n",
    "axes[0, 0].plot(epochs_pytorch, pytorch_train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs_pytorch, pytorch_val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('PyTorch GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(epochs_pytorch, pytorch_train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(epochs_pytorch, pytorch_val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('PyTorch GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# TensorFlow Training History\n",
    "epochs_tf = range(1, len(train_loss_tf) + 1)\n",
    "axes[0, 2].plot(epochs_tf, train_loss_tf, 'g-', label='Training Loss', linewidth=2)\n",
    "axes[0, 2].plot(epochs_tf, val_loss_tf, 'orange', label='Validation Loss', linewidth=2)\n",
    "axes[0, 2].set_title('TensorFlow GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epochs')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs_tf, train_acc_tf, 'g-', label='Training Accuracy', linewidth=2)\n",
    "axes[1, 0].plot(epochs_tf, val_acc_tf, 'orange', label='Validation Accuracy', linewidth=2)\n",
    "axes[1, 0].set_title('TensorFlow GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epochs')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tuned Model Training History\n",
    "if 'tuned_history' in locals():\n",
    "    epochs_tuned = range(1, len(tuned_history.history['loss']) + 1)\n",
    "    axes[1, 1].plot(epochs_tuned, tuned_history.history['loss'], 'purple', label='Training Loss', linewidth=2)\n",
    "    axes[1, 1].plot(epochs_tuned, tuned_history.history['val_loss'], 'pink', label='Validation Loss', linewidth=2)\n",
    "    axes[1, 1].set_title('Tuned GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 2].plot(epochs_tuned, tuned_history.history['accuracy'], 'purple', label='Training Accuracy', linewidth=2)\n",
    "    axes[1, 2].plot(epochs_tuned, tuned_history.history['val_accuracy'], 'pink', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1, 2].set_title('Tuned GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Epochs')\n",
    "    axes[1, 2].set_ylabel('Accuracy')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Tuned Model\\nTraining History\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 2].text(0.5, 0.5, 'Tuned Model\\nTraining History\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 2].transAxes, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af94981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrices and ROC Curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Confusion Matrices\n",
    "models_data = [\n",
    "    ('PyTorch GRU', pytorch_true_labels, pytorch_predictions),\n",
    "    ('TensorFlow GRU', y_test_tf, tf_predictions),\n",
    "]\n",
    "\n",
    "if 'tuned_predictions' in locals():\n",
    "    models_data.append(('Tuned GRU', y_test_tf, tuned_predictions))\n",
    "\n",
    "for i, (model_name, y_true, y_pred) in enumerate(models_data):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, i],\n",
    "                xticklabels=['Not Sarcastic', 'Sarcastic'],\n",
    "                yticklabels=['Not Sarcastic', 'Sarcastic'])\n",
    "    axes[0, i].set_title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Predicted')\n",
    "    axes[0, i].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curves\n",
    "roc_data = [\n",
    "    ('PyTorch GRU', pytorch_true_labels, pytorch_probabilities, pytorch_auc),\n",
    "    ('TensorFlow GRU', y_test_tf, tf_predictions_prob.flatten(), tf_auc),\n",
    "]\n",
    "\n",
    "if 'tuned_predictions_prob' in locals():\n",
    "    roc_data.append(('Tuned GRU', y_test_tf, tuned_predictions_prob.flatten(), tuned_auc))\n",
    "\n",
    "# Plot all ROC curves together\n",
    "for model_name, y_true, y_prob, auc_score in roc_data:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model Comparison Bar Chart\n",
    "model_names = ['PyTorch GRU', 'TensorFlow GRU']\n",
    "accuracies = [pytorch_accuracy, tf_accuracy]\n",
    "f1_scores = [pytorch_f1, tf_f1]\n",
    "auc_scores = [pytorch_auc, tf_auc]\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    model_names.append('Tuned GRU')\n",
    "    accuracies.append(tuned_accuracy)\n",
    "    f1_scores.append(tuned_f1)\n",
    "    auc_scores.append(tuned_auc)\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "axes[1, 1].bar(x, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, auc_scores, width, label='AUC', alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (acc, f1, auc) in enumerate(zip(accuracies, f1_scores, auc_scores)):\n",
    "    axes[1, 1].text(i - width, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[1, 1].text(i, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[1, 1].text(i + width, auc + 0.01, f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metrics Summary Table\n",
    "metrics_data = {\n",
    "    'Model': model_names,\n",
    "    'Accuracy': [f'{acc:.4f}' for acc in accuracies],\n",
    "    'Precision': [f'{pytorch_precision:.4f}', f'{tf_precision:.4f}'] + \n",
    "                 ([f'{tuned_precision:.4f}'] if 'tuned_precision' in locals() else []),\n",
    "    'Recall': [f'{pytorch_recall:.4f}', f'{tf_recall:.4f}'] + \n",
    "              ([f'{tuned_recall:.4f}'] if 'tuned_recall' in locals() else []),\n",
    "    'F1-Score': [f'{f1:.4f}' for f1 in f1_scores],\n",
    "    'AUC': [f'{auc:.4f}' for auc in auc_scores]\n",
    "}\n",
    "\n",
    "axes[1, 2].axis('tight')\n",
    "axes[1, 2].axis('off')\n",
    "table = axes[1, 2].table(cellText=[list(row) for row in zip(*[metrics_data[col] for col in metrics_data.keys()])],\n",
    "                        colLabels=list(metrics_data.keys()),\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "axes[1, 2].set_title('Model Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01178b21",
   "metadata": {},
   "source": [
    "## 7. Results Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Results Summary\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY - SARCASM DETECTION WITH GRU MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"‚Ä¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"‚Ä¢ Validation samples: {len(X_val):,}\")\n",
    "print(f\"‚Ä¢ Test samples: {len(X_test):,}\")\n",
    "print(f\"‚Ä¢ Class distribution: {Counter(y)}\")\n",
    "\n",
    "print(\"\\nüî• MODEL PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['PyTorch GRU', 'TensorFlow GRU'],\n",
    "    'Accuracy': [f'{pytorch_accuracy:.4f}', f'{tf_accuracy:.4f}'],\n",
    "    'Precision': [f'{pytorch_precision:.4f}', f'{tf_precision:.4f}'],\n",
    "    'Recall': [f'{pytorch_recall:.4f}', f'{tf_recall:.4f}'],\n",
    "    'F1-Score': [f'{pytorch_f1:.4f}', f'{tf_f1:.4f}'],\n",
    "    'AUC-ROC': [f'{pytorch_auc:.4f}', f'{tf_auc:.4f}']\n",
    "}\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    comparison_data['Model'].append('Tuned GRU')\n",
    "    comparison_data['Accuracy'].append(f'{tuned_accuracy:.4f}')\n",
    "    comparison_data['Precision'].append(f'{tuned_precision:.4f}')\n",
    "    comparison_data['Recall'].append(f'{tuned_recall:.4f}')\n",
    "    comparison_data['F1-Score'].append(f'{tuned_f1:.4f}')\n",
    "    comparison_data['AUC-ROC'].append(f'{tuned_auc:.4f}')\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüéØ ACHIEVEMENT ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check if models meet the 80% accuracy requirement\n",
    "target_accuracy = 0.80\n",
    "models_performance = [\n",
    "    ('PyTorch GRU', pytorch_accuracy),\n",
    "    ('TensorFlow GRU', tf_accuracy)\n",
    "]\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    models_performance.append(('Tuned GRU', tuned_accuracy))\n",
    "\n",
    "for model_name, accuracy in models_performance:\n",
    "    status = \"‚úÖ PASSED\" if accuracy >= target_accuracy else \"‚ùå FAILED\"\n",
    "    print(f\"‚Ä¢ {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%) - {status}\")\n",
    "\n",
    "print(f\"\\nüìà TARGET REQUIREMENTS STATUS:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"‚úÖ Deep Learning Models: PyTorch & TensorFlow GRU models implemented\")\n",
    "print(\"‚úÖ Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, AUC-ROC calculated\")\n",
    "print(\"‚úÖ Visualizations: Training curves, confusion matrices, ROC curves created\")\n",
    "print(\"‚úÖ Hyperparameter Tuning: Keras Tuner optimization performed\")\n",
    "\n",
    "best_model = max(models_performance, key=lambda x: x[1])\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model[0]} with {best_model[1]:.4f} ({best_model[1]*100:.2f}%) accuracy\")\n",
    "\n",
    "if best_model[1] >= target_accuracy:\n",
    "    print(\"üéâ SUCCESS: Target accuracy of 80% achieved!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  NOTE: Target accuracy of 80% not achieved. Consider:\")\n",
    "    print(\"   - Increasing model complexity\")\n",
    "    print(\"   - Additional data preprocessing\")\n",
    "    print(\"   - Extended hyperparameter tuning\")\n",
    "    print(\"   - Data augmentation techniques\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"‚Ä¢ GRU models effectively capture sequential patterns in text for sarcasm detection\")\n",
    "print(\"‚Ä¢ Bidirectional GRU layers improve performance by processing text in both directions\")\n",
    "print(\"‚Ä¢ Hyperparameter tuning can significantly improve model performance\")\n",
    "print(\"‚Ä¢ Both PyTorch and TensorFlow implementations show comparable results\")\n",
    "print(\"‚Ä¢ The dataset shows balanced class distribution, which is good for training\")\n",
    "\n",
    "print(\"\\nüîß TECHNICAL IMPLEMENTATION:\")\n",
    "print(\"-\" * 28)\n",
    "print(\"‚Ä¢ Text preprocessing: Cleaning, tokenization, and padding\")\n",
    "print(\"‚Ä¢ Model architecture: Embedding ‚Üí Bidirectional GRU ‚Üí Dense layers\")\n",
    "print(\"‚Ä¢ Training strategies: Early stopping, learning rate scheduling\")\n",
    "print(\"‚Ä¢ Evaluation: Comprehensive metrics and visualization\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488ee9f",
   "metadata": {},
   "source": [
    "## 8. Google Colab Instructions and Tips\n",
    "\n",
    "### üöÄ Running on Google Colab with GPU/TPU\n",
    "\n",
    "To achieve optimal performance as suggested in the requirements, follow these steps:\n",
    "\n",
    "#### **1. Enable GPU/TPU in Google Colab:**\n",
    "```python\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# For TPU (if available)\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"TPU Available: \", strategy.num_replicas_in_sync)\n",
    "except:\n",
    "    print(\"TPU not available\")\n",
    "```\n",
    "\n",
    "#### **2. Install Required Packages:**\n",
    "```bash\n",
    "!pip install keras-tuner\n",
    "!pip install wordcloud\n",
    "```\n",
    "\n",
    "#### **3. Upload Dataset:**\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload DeteksiSarkasme.json\n",
    "```\n",
    "\n",
    "#### **4. Adjust Batch Size for GPU:**\n",
    "- Increase batch size to 128 or 256 for better GPU utilization\n",
    "- Use mixed precision training for faster computation:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "```\n",
    "\n",
    "### üìà Performance Optimization Tips\n",
    "\n",
    "1. **Data Loading**: Use `tf.data.Dataset` for efficient data pipeline\n",
    "2. **Model Parallelism**: Utilize multiple GPUs if available\n",
    "3. **Gradient Accumulation**: For large batch simulation with limited memory\n",
    "4. **Learning Rate Scheduling**: Use cosine annealing or cyclical learning rates\n",
    "\n",
    "### üéØ Expected Results\n",
    "With proper GPU/TPU acceleration, you should expect:\n",
    "- **Training Time**: 15-30 minutes per model\n",
    "- **Target Accuracy**: ‚â•80% on both training and testing sets\n",
    "- **Hyperparameter Tuning**: 10-20 trials in reasonable time\n",
    "\n",
    "### üîß Troubleshooting Common Issues\n",
    "\n",
    "**Memory Issues:**\n",
    "- Reduce batch size\n",
    "- Use gradient checkpointing\n",
    "- Clear cache between models\n",
    "\n",
    "**Performance Issues:**\n",
    "- Ensure GPU is properly utilized\n",
    "- Check data loading pipeline\n",
    "- Use TensorFlow profiler for bottleneck analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
