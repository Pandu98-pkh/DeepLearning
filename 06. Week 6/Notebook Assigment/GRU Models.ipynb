{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4998623e",
   "metadata": {},
   "source": [
    "# GRU Models for Sarcasm Detection\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements GRU (Gated Recurrent Unit) models for sarcasm detection using both PyTorch and TensorFlow frameworks. The task involves:\n",
    "\n",
    "1. **Binary Classification**: Detecting sarcasm in news headlines\n",
    "2. **Deep Learning Models**: GRU-based architectures in PyTorch and TensorFlow\n",
    "3. **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, AUC, ROC\n",
    "4. **Hyperparameter Tuning**: Using Keras Tuner for TensorFlow model optimization\n",
    "5. **Target Performance**: Minimum 80% accuracy on both training and testing sets\n",
    "\n",
    "## Dataset\n",
    "- **Source**: DeteksiSarkasme.json\n",
    "- **Features**: News headlines\n",
    "- **Target**: Binary classification (0: Not Sarcastic, 1: Sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Keras Tuner for hyperparameter tuning\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "    KERAS_TUNER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Keras Tuner not installed. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'keras-tuner'])\n",
    "    import keras_tuner as kt\n",
    "    KERAS_TUNER_AVAILABLE = True\n",
    "\n",
    "# Scikit-learn for metrics and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"GPU Available (PyTorch):\", torch.cuda.is_available())\n",
    "print(\"GPU Available (TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983740a",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7057d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Dataset/DeteksiSarkasme.json'\n",
    "\n",
    "# Read JSON data line by line\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['is_sarcastic'].value_counts())\n",
    "print(\"\\nTarget distribution (percentage):\")\n",
    "print(df['is_sarcastic'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Target distribution\n",
    "df['is_sarcastic'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'orange'])\n",
    "axes[0,0].set_title('Sarcasm Distribution')\n",
    "axes[0,0].set_xlabel('Is Sarcastic')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].set_xticklabels(['Not Sarcastic', 'Sarcastic'], rotation=0)\n",
    "\n",
    "# 2. Headline length distribution\n",
    "df['headline_length'] = df['headline'].str.len()\n",
    "df['headline_length'].hist(bins=50, ax=axes[0,1], color='lightgreen', alpha=0.7)\n",
    "axes[0,1].set_title('Headline Length Distribution')\n",
    "axes[0,1].set_xlabel('Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Word count distribution\n",
    "df['word_count'] = df['headline'].str.split().str.len()\n",
    "df['word_count'].hist(bins=30, ax=axes[1,0], color='salmon', alpha=0.7)\n",
    "axes[1,0].set_title('Word Count Distribution')\n",
    "axes[1,0].set_xlabel('Number of Words')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Length comparison by sarcasm\n",
    "df.boxplot(column='headline_length', by='is_sarcastic', ax=axes[1,1])\n",
    "axes[1,1].set_title('Headline Length by Sarcasm')\n",
    "axes[1,1].set_xlabel('Is Sarcastic')\n",
    "axes[1,1].set_ylabel('Length (characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nHeadline Length Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['headline_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df.groupby('is_sarcastic')['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e268985",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep punctuation that might be important for sarcasm\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?;:\\'\\\"-]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "\n",
    "# Show examples of cleaned text\n",
    "print(\"Original vs Cleaned Headlines:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {df['headline'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_headline'].iloc[i]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df['cleaned_headline'].values\n",
    "y = df['is_sarcastic'].values\n",
    "\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Class distribution: {Counter(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c29a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {Counter(y_train)}\")\n",
    "print(f\"Test class distribution: {Counter(y_test)}\")\n",
    "\n",
    "# Create validation split from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter validation split:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e3944",
   "metadata": {},
   "source": [
    "## 3. PyTorch GRU Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch tokenization and dataset preparation\n",
    "class TextTokenizer:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            word_freq.update(text.split())\n",
    "        \n",
    "        # Create vocabulary with most frequent words\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "        for word, freq in word_freq.most_common(self.max_vocab_size - 2):\n",
    "            idx = len(self.word_to_idx)\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "            \n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = [self.word_to_idx.get(word, 1) for word in text.split()]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts)\n",
    "        return self.transform(texts)\n",
    "\n",
    "# Initialize tokenizer and fit on training data\n",
    "pytorch_tokenizer = TextTokenizer(max_vocab_size=10000)\n",
    "X_train_seq = pytorch_tokenizer.fit_transform(X_train)\n",
    "X_val_seq = pytorch_tokenizer.transform(X_val)\n",
    "X_test_seq = pytorch_tokenizer.transform(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 50  # Maximum sequence length\n",
    "def pad_sequences_pytorch(sequences, max_len, pad_value=0):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            padded.append(seq[:max_len])\n",
    "        else:\n",
    "            padded.append(seq + [pad_value] * (max_len - len(seq)))\n",
    "    return np.array(padded)\n",
    "\n",
    "X_train_padded = pad_sequences_pytorch(X_train_seq, max_length)\n",
    "X_val_padded = pad_sequences_pytorch(X_val_seq, max_length)\n",
    "X_test_padded = pad_sequences_pytorch(X_test_seq, max_length)\n",
    "\n",
    "print(f\"PyTorch padded shapes:\")\n",
    "print(f\"Train: {X_train_padded.shape}\")\n",
    "print(f\"Validation: {X_val_padded.shape}\")\n",
    "print(f\"Test: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.LongTensor(texts)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SarcasmDataset(X_train_padded, y_train)\n",
    "val_dataset = SarcasmDataset(X_val_padded, y_val)\n",
    "test_dataset = SarcasmDataset(X_test_padded, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# PyTorch GRU Model\n",
    "class GRUSarcasmClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(GRUSarcasmClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(last_output)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return self.sigmoid(x).squeeze()\n",
    "\n",
    "# Initialize PyTorch model\n",
    "pytorch_model = GRUSarcasmClassifier(\n",
    "    vocab_size=pytorch_tokenizer.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"PyTorch Model Architecture:\")\n",
    "print(pytorch_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in pytorch_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch training function\n",
    "def train_pytorch_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    patience = 7\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_texts, batch_labels in train_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in val_loader:\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss_avg)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_avg)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_pytorch_gru_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "              f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the PyTorch model\n",
    "print(\"Training PyTorch GRU Model...\")\n",
    "pytorch_train_losses, pytorch_val_losses, pytorch_train_accs, pytorch_val_accs = train_pytorch_model(\n",
    "    pytorch_model, train_loader, val_loader, epochs=30, lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model evaluation\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in test_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_texts)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_probabilities.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), np.array(all_probabilities)\n",
    "\n",
    "# Load best model and evaluate\n",
    "pytorch_model.load_state_dict(torch.load('best_pytorch_gru_model.pth'))\n",
    "pytorch_predictions, pytorch_true_labels, pytorch_probabilities = evaluate_pytorch_model(pytorch_model, test_loader)\n",
    "\n",
    "# Calculate metrics\n",
    "pytorch_accuracy = accuracy_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_precision = precision_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_recall = recall_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_f1 = f1_score(pytorch_true_labels, pytorch_predictions)\n",
    "pytorch_auc = roc_auc_score(pytorch_true_labels, pytorch_probabilities)\n",
    "\n",
    "print(\"PyTorch GRU Model Results:\")\n",
    "print(f\"Accuracy: {pytorch_accuracy:.4f}\")\n",
    "print(f\"Precision: {pytorch_precision:.4f}\")\n",
    "print(f\"Recall: {pytorch_recall:.4f}\")\n",
    "print(f\"F1-Score: {pytorch_f1:.4f}\")\n",
    "print(f\"AUC: {pytorch_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(pytorch_true_labels, pytorch_predictions, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae77a3",
   "metadata": {},
   "source": [
    "## 4. TensorFlow/Keras GRU Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow tokenization and data preparation\n",
    "max_vocab_size = 10000\n",
    "max_length = 50\n",
    "\n",
    "# Initialize tokenizer\n",
    "tf_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
    "tf_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_tf = tf_tokenizer.texts_to_sequences(X_train)\n",
    "X_val_tf = tf_tokenizer.texts_to_sequences(X_val)\n",
    "X_test_tf = tf_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_tf = pad_sequences(X_train_tf, maxlen=max_length, padding='post')\n",
    "X_val_tf = pad_sequences(X_val_tf, maxlen=max_length, padding='post')\n",
    "X_test_tf = pad_sequences(X_test_tf, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"TensorFlow tokenizer vocabulary size: {len(tf_tokenizer.word_index) + 1}\")\n",
    "print(f\"TensorFlow padded shapes:\")\n",
    "print(f\"Train: {X_train_tf.shape}\")\n",
    "print(f\"Validation: {X_val_tf.shape}\")\n",
    "print(f\"Test: {X_test_tf.shape}\")\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_tf = np.array(y_train)\n",
    "y_val_tf = np.array(y_val)\n",
    "y_test_tf = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow GRU Model\n",
    "def create_gru_model(vocab_size, embedding_dim=128, gru_units=128, dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        Bidirectional(GRU(gru_units, return_sequences=True, dropout=dropout_rate)),\n",
    "        Bidirectional(GRU(gru_units//2, dropout=dropout_rate)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create TensorFlow model\n",
    "tf_model = create_gru_model(\n",
    "    vocab_size=len(tf_tokenizer.word_index) + 1,\n",
    "    embedding_dim=128,\n",
    "    gru_units=128,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "tf_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"TensorFlow Model Architecture:\")\n",
    "tf_model.summary()\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_tensorflow_gru_model.keras', monitor='val_accuracy', \n",
    "                   save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining TensorFlow GRU Model...\")\n",
    "tf_history = tf_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow model evaluation\n",
    "# Load best model\n",
    "tf_model = tf.keras.models.load_model('best_tensorflow_gru_model.keras')\n",
    "\n",
    "# Make predictions\n",
    "tf_predictions_prob = tf_model.predict(X_test_tf)\n",
    "tf_predictions = (tf_predictions_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "tf_accuracy = accuracy_score(y_test_tf, tf_predictions)\n",
    "tf_precision = precision_score(y_test_tf, tf_predictions)\n",
    "tf_recall = recall_score(y_test_tf, tf_predictions)\n",
    "tf_f1 = f1_score(y_test_tf, tf_predictions)\n",
    "tf_auc = roc_auc_score(y_test_tf, tf_predictions_prob)\n",
    "\n",
    "print(\"TensorFlow GRU Model Results:\")\n",
    "print(f\"Accuracy: {tf_accuracy:.4f}\")\n",
    "print(f\"Precision: {tf_precision:.4f}\")\n",
    "print(f\"Recall: {tf_recall:.4f}\")\n",
    "print(f\"F1-Score: {tf_f1:.4f}\")\n",
    "print(f\"AUC: {tf_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tf, tf_predictions, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))\n",
    "\n",
    "# Training and validation accuracy/loss\n",
    "train_acc_tf = tf_history.history['accuracy']\n",
    "val_acc_tf = tf_history.history['val_accuracy']\n",
    "train_loss_tf = tf_history.history['loss']\n",
    "val_loss_tf = tf_history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e694423",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Keras Tuner\n",
    "def build_tuner_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=len(tf_tokenizer.word_index) + 1,\n",
    "        output_dim=hp.Int('embedding_dim', min_value=64, max_value=256, step=32),\n",
    "        input_length=max_length\n",
    "    ))\n",
    "    \n",
    "    # First GRU layer\n",
    "    model.add(Bidirectional(GRU(\n",
    "        hp.Int('gru_units_1', min_value=64, max_value=256, step=32),\n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    # Second GRU layer\n",
    "    model.add(Bidirectional(GRU(\n",
    "        hp.Int('gru_units_2', min_value=32, max_value=128, step=16),\n",
    "        dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        hp.Int('dense_units', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuner_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Reduce for faster execution\n",
    "    directory='keras_tuner',\n",
    "    project_name='gru_sarcasm_detection'\n",
    ")\n",
    "\n",
    "print(\"Tuner search space summary:\")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Search for best hyperparameters\n",
    "print(\"\\nStarting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_tf, y_train_tf,\n",
    "    epochs=10,  # Reduce epochs for faster tuning\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param in ['embedding_dim', 'gru_units_1', 'gru_units_2', 'dense_units', \n",
    "              'dropout_1', 'dropout_2', 'dropout_3', 'learning_rate']:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee185fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model with optimal hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Extended training with best hyperparameters\n",
    "callbacks_tuned = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_tuned_gru_model.keras', monitor='val_accuracy', \n",
    "                   save_best_only=True, save_weights_only=False)\n",
    "]\n",
    "\n",
    "print(\"Training tuned model with extended epochs...\")\n",
    "tuned_history = best_model.fit(\n",
    "    X_train_tf, y_train_tf,\n",
    "    validation_data=(X_val_tf, y_val_tf),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks_tuned,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_model = tf.keras.models.load_model('best_tuned_gru_model.keras')\n",
    "tuned_predictions_prob = best_model.predict(X_test_tf)\n",
    "tuned_predictions = (tuned_predictions_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics for tuned model\n",
    "tuned_accuracy = accuracy_score(y_test_tf, tuned_predictions)\n",
    "tuned_precision = precision_score(y_test_tf, tuned_predictions)\n",
    "tuned_recall = recall_score(y_test_tf, tuned_predictions)\n",
    "tuned_f1 = f1_score(y_test_tf, tuned_predictions)\n",
    "tuned_auc = roc_auc_score(y_test_tf, tuned_predictions_prob)\n",
    "\n",
    "print(\"\\nTuned TensorFlow GRU Model Results:\")\n",
    "print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"Precision: {tuned_precision:.4f}\")\n",
    "print(f\"Recall: {tuned_recall:.4f}\")\n",
    "print(f\"F1-Score: {tuned_f1:.4f}\")\n",
    "print(f\"AUC: {tuned_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5cc62",
   "metadata": {},
   "source": [
    "## 6. Model Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Performance Visualization\n",
    "\n",
    "# 1. Training History Plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# PyTorch Training History\n",
    "epochs_pytorch = range(1, len(pytorch_train_losses) + 1)\n",
    "axes[0, 0].plot(epochs_pytorch, pytorch_train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs_pytorch, pytorch_val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('PyTorch GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(epochs_pytorch, pytorch_train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(epochs_pytorch, pytorch_val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('PyTorch GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epochs')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# TensorFlow Training History\n",
    "epochs_tf = range(1, len(train_loss_tf) + 1)\n",
    "axes[0, 2].plot(epochs_tf, train_loss_tf, 'g-', label='Training Loss', linewidth=2)\n",
    "axes[0, 2].plot(epochs_tf, val_loss_tf, 'orange', label='Validation Loss', linewidth=2)\n",
    "axes[0, 2].set_title('TensorFlow GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epochs')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs_tf, train_acc_tf, 'g-', label='Training Accuracy', linewidth=2)\n",
    "axes[1, 0].plot(epochs_tf, val_acc_tf, 'orange', label='Validation Accuracy', linewidth=2)\n",
    "axes[1, 0].set_title('TensorFlow GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epochs')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tuned Model Training History\n",
    "if 'tuned_history' in locals():\n",
    "    epochs_tuned = range(1, len(tuned_history.history['loss']) + 1)\n",
    "    axes[1, 1].plot(epochs_tuned, tuned_history.history['loss'], 'purple', label='Training Loss', linewidth=2)\n",
    "    axes[1, 1].plot(epochs_tuned, tuned_history.history['val_loss'], 'pink', label='Validation Loss', linewidth=2)\n",
    "    axes[1, 1].set_title('Tuned GRU - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1, 2].plot(epochs_tuned, tuned_history.history['accuracy'], 'purple', label='Training Accuracy', linewidth=2)\n",
    "    axes[1, 2].plot(epochs_tuned, tuned_history.history['val_accuracy'], 'pink', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1, 2].set_title('Tuned GRU - Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Epochs')\n",
    "    axes[1, 2].set_ylabel('Accuracy')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Tuned Model\\nTraining History\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 2].text(0.5, 0.5, 'Tuned Model\\nTraining History\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 2].transAxes, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af94981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrices and ROC Curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Confusion Matrices\n",
    "models_data = [\n",
    "    ('PyTorch GRU', pytorch_true_labels, pytorch_predictions),\n",
    "    ('TensorFlow GRU', y_test_tf, tf_predictions),\n",
    "]\n",
    "\n",
    "if 'tuned_predictions' in locals():\n",
    "    models_data.append(('Tuned GRU', y_test_tf, tuned_predictions))\n",
    "\n",
    "for i, (model_name, y_true, y_pred) in enumerate(models_data):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, i],\n",
    "                xticklabels=['Not Sarcastic', 'Sarcastic'],\n",
    "                yticklabels=['Not Sarcastic', 'Sarcastic'])\n",
    "    axes[0, i].set_title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Predicted')\n",
    "    axes[0, i].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curves\n",
    "roc_data = [\n",
    "    ('PyTorch GRU', pytorch_true_labels, pytorch_probabilities, pytorch_auc),\n",
    "    ('TensorFlow GRU', y_test_tf, tf_predictions_prob.flatten(), tf_auc),\n",
    "]\n",
    "\n",
    "if 'tuned_predictions_prob' in locals():\n",
    "    roc_data.append(('Tuned GRU', y_test_tf, tuned_predictions_prob.flatten(), tuned_auc))\n",
    "\n",
    "# Plot all ROC curves together\n",
    "for model_name, y_true, y_prob, auc_score in roc_data:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model Comparison Bar Chart\n",
    "model_names = ['PyTorch GRU', 'TensorFlow GRU']\n",
    "accuracies = [pytorch_accuracy, tf_accuracy]\n",
    "f1_scores = [pytorch_f1, tf_f1]\n",
    "auc_scores = [pytorch_auc, tf_auc]\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    model_names.append('Tuned GRU')\n",
    "    accuracies.append(tuned_accuracy)\n",
    "    f1_scores.append(tuned_f1)\n",
    "    auc_scores.append(tuned_auc)\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "axes[1, 1].bar(x, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, auc_scores, width, label='AUC', alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (acc, f1, auc) in enumerate(zip(accuracies, f1_scores, auc_scores)):\n",
    "    axes[1, 1].text(i - width, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[1, 1].text(i, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[1, 1].text(i + width, auc + 0.01, f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Metrics Summary Table\n",
    "metrics_data = {\n",
    "    'Model': model_names,\n",
    "    'Accuracy': [f'{acc:.4f}' for acc in accuracies],\n",
    "    'Precision': [f'{pytorch_precision:.4f}', f'{tf_precision:.4f}'] + \n",
    "                 ([f'{tuned_precision:.4f}'] if 'tuned_precision' in locals() else []),\n",
    "    'Recall': [f'{pytorch_recall:.4f}', f'{tf_recall:.4f}'] + \n",
    "              ([f'{tuned_recall:.4f}'] if 'tuned_recall' in locals() else []),\n",
    "    'F1-Score': [f'{f1:.4f}' for f1 in f1_scores],\n",
    "    'AUC': [f'{auc:.4f}' for auc in auc_scores]\n",
    "}\n",
    "\n",
    "axes[1, 2].axis('tight')\n",
    "axes[1, 2].axis('off')\n",
    "table = axes[1, 2].table(cellText=[list(row) for row in zip(*[metrics_data[col] for col in metrics_data.keys()])],\n",
    "                        colLabels=list(metrics_data.keys()),\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "axes[1, 2].set_title('Model Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01178b21",
   "metadata": {},
   "source": [
    "## 7. Results Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Results Summary\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY - SARCASM DETECTION WITH GRU MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"• Total samples: {len(df):,}\")\n",
    "print(f\"• Training samples: {len(X_train):,}\")\n",
    "print(f\"• Validation samples: {len(X_val):,}\")\n",
    "print(f\"• Test samples: {len(X_test):,}\")\n",
    "print(f\"• Class distribution: {Counter(y)}\")\n",
    "\n",
    "print(\"\\n🔥 MODEL PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['PyTorch GRU', 'TensorFlow GRU'],\n",
    "    'Accuracy': [f'{pytorch_accuracy:.4f}', f'{tf_accuracy:.4f}'],\n",
    "    'Precision': [f'{pytorch_precision:.4f}', f'{tf_precision:.4f}'],\n",
    "    'Recall': [f'{pytorch_recall:.4f}', f'{tf_recall:.4f}'],\n",
    "    'F1-Score': [f'{pytorch_f1:.4f}', f'{tf_f1:.4f}'],\n",
    "    'AUC-ROC': [f'{pytorch_auc:.4f}', f'{tf_auc:.4f}']\n",
    "}\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    comparison_data['Model'].append('Tuned GRU')\n",
    "    comparison_data['Accuracy'].append(f'{tuned_accuracy:.4f}')\n",
    "    comparison_data['Precision'].append(f'{tuned_precision:.4f}')\n",
    "    comparison_data['Recall'].append(f'{tuned_recall:.4f}')\n",
    "    comparison_data['F1-Score'].append(f'{tuned_f1:.4f}')\n",
    "    comparison_data['AUC-ROC'].append(f'{tuned_auc:.4f}')\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n🎯 ACHIEVEMENT ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check if models meet the 80% accuracy requirement\n",
    "target_accuracy = 0.80\n",
    "models_performance = [\n",
    "    ('PyTorch GRU', pytorch_accuracy),\n",
    "    ('TensorFlow GRU', tf_accuracy)\n",
    "]\n",
    "\n",
    "if 'tuned_accuracy' in locals():\n",
    "    models_performance.append(('Tuned GRU', tuned_accuracy))\n",
    "\n",
    "for model_name, accuracy in models_performance:\n",
    "    status = \"✅ PASSED\" if accuracy >= target_accuracy else \"❌ FAILED\"\n",
    "    print(f\"• {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%) - {status}\")\n",
    "\n",
    "print(f\"\\n📈 TARGET REQUIREMENTS STATUS:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"✅ Deep Learning Models: PyTorch & TensorFlow GRU models implemented\")\n",
    "print(\"✅ Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, AUC-ROC calculated\")\n",
    "print(\"✅ Visualizations: Training curves, confusion matrices, ROC curves created\")\n",
    "print(\"✅ Hyperparameter Tuning: Keras Tuner optimization performed\")\n",
    "\n",
    "best_model = max(models_performance, key=lambda x: x[1])\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model[0]} with {best_model[1]:.4f} ({best_model[1]*100:.2f}%) accuracy\")\n",
    "\n",
    "if best_model[1] >= target_accuracy:\n",
    "    print(\"🎉 SUCCESS: Target accuracy of 80% achieved!\")\n",
    "else:\n",
    "    print(\"⚠️  NOTE: Target accuracy of 80% not achieved. Consider:\")\n",
    "    print(\"   - Increasing model complexity\")\n",
    "    print(\"   - Additional data preprocessing\")\n",
    "    print(\"   - Extended hyperparameter tuning\")\n",
    "    print(\"   - Data augmentation techniques\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"• GRU models effectively capture sequential patterns in text for sarcasm detection\")\n",
    "print(\"• Bidirectional GRU layers improve performance by processing text in both directions\")\n",
    "print(\"• Hyperparameter tuning can significantly improve model performance\")\n",
    "print(\"• Both PyTorch and TensorFlow implementations show comparable results\")\n",
    "print(\"• The dataset shows balanced class distribution, which is good for training\")\n",
    "\n",
    "print(\"\\n🔧 TECHNICAL IMPLEMENTATION:\")\n",
    "print(\"-\" * 28)\n",
    "print(\"• Text preprocessing: Cleaning, tokenization, and padding\")\n",
    "print(\"• Model architecture: Embedding → Bidirectional GRU → Dense layers\")\n",
    "print(\"• Training strategies: Early stopping, learning rate scheduling\")\n",
    "print(\"• Evaluation: Comprehensive metrics and visualization\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488ee9f",
   "metadata": {},
   "source": [
    "## 8. Google Colab Instructions and Tips\n",
    "\n",
    "### 🚀 Running on Google Colab with GPU/TPU\n",
    "\n",
    "To achieve optimal performance as suggested in the requirements, follow these steps:\n",
    "\n",
    "#### **1. Enable GPU/TPU in Google Colab:**\n",
    "```python\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# For TPU (if available)\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"TPU Available: \", strategy.num_replicas_in_sync)\n",
    "except:\n",
    "    print(\"TPU not available\")\n",
    "```\n",
    "\n",
    "#### **2. Install Required Packages:**\n",
    "```bash\n",
    "!pip install keras-tuner\n",
    "!pip install wordcloud\n",
    "```\n",
    "\n",
    "#### **3. Upload Dataset:**\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload DeteksiSarkasme.json\n",
    "```\n",
    "\n",
    "#### **4. Adjust Batch Size for GPU:**\n",
    "- Increase batch size to 128 or 256 for better GPU utilization\n",
    "- Use mixed precision training for faster computation:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "```\n",
    "\n",
    "### 📈 Performance Optimization Tips\n",
    "\n",
    "1. **Data Loading**: Use `tf.data.Dataset` for efficient data pipeline\n",
    "2. **Model Parallelism**: Utilize multiple GPUs if available\n",
    "3. **Gradient Accumulation**: For large batch simulation with limited memory\n",
    "4. **Learning Rate Scheduling**: Use cosine annealing or cyclical learning rates\n",
    "\n",
    "### 🎯 Expected Results\n",
    "With proper GPU/TPU acceleration, you should expect:\n",
    "- **Training Time**: 15-30 minutes per model\n",
    "- **Target Accuracy**: ≥80% on both training and testing sets\n",
    "- **Hyperparameter Tuning**: 10-20 trials in reasonable time\n",
    "\n",
    "### 🔧 Troubleshooting Common Issues\n",
    "\n",
    "**Memory Issues:**\n",
    "- Reduce batch size\n",
    "- Use gradient checkpointing\n",
    "- Clear cache between models\n",
    "\n",
    "**Performance Issues:**\n",
    "- Ensure GPU is properly utilized\n",
    "- Check data loading pipeline\n",
    "- Use TensorFlow profiler for bottleneck analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
