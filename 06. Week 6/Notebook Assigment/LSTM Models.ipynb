{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f3ed5a",
   "metadata": {},
   "source": [
    "# LSTM Models for Sarcasm Detection\n",
    "\n",
    "This notebook implements LSTM models using both PyTorch and TensorFlow to detect sarcasm in news headlines using the DeteksiSarkasme.json dataset.\n",
    "\n",
    "## Requirements:\n",
    "1. Build deep learning models (PyTorch and TensorFlow)\n",
    "2. Use evaluation metrics (Accuracy, Precision, Recall, F1-Score, AUC, ROC)\n",
    "3. Visualize accuracy matrix and loss\n",
    "4. Hyperparameter tuning with Keras Tuner (TensorFlow)\n",
    "5. Achieve minimum 80% accuracy on training and testing sets\n",
    "\n",
    "## Dataset\n",
    "The dataset contains news headlines with binary labels:\n",
    "- 0: Not sarcastic\n",
    "- 1: Sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb071be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install tensorflow\n",
    "!pip install keras-tuner\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib seaborn\n",
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# TensorFlow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Keras Tuner for hyperparameter tuning\n",
    "import keras_tuner as kt\n",
    "\n",
    "# NLTK for text preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364b4f7",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Dataset/DeteksiSarkasme.json'\n",
    "\n",
    "# Read JSON data\n",
    "data = []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Sarcastic samples: {df['is_sarcastic'].sum()}\")\n",
    "print(f\"Non-sarcastic samples: {len(df) - df['is_sarcastic'].sum()}\")\n",
    "print(f\"Sarcasm ratio: {df['is_sarcastic'].mean():.2%}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['is_sarcastic'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Is Sarcastic')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Sarcastic', 'Sarcastic'], rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['is_sarcastic'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Class Distribution (Percentage)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "df['headline_length'] = df['headline'].str.len()\n",
    "df['word_count'] = df['headline'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['headline_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Headline Length (Characters)')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['word_count'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sarcastic_lengths = df[df['is_sarcastic'] == 1]['word_count']\n",
    "non_sarcastic_lengths = df[df['is_sarcastic'] == 0]['word_count']\n",
    "plt.hist(sarcastic_lengths, bins=30, alpha=0.7, label='Sarcastic', edgecolor='black')\n",
    "plt.hist(non_sarcastic_lengths, bins=30, alpha=0.7, label='Non-Sarcastic', edgecolor='black')\n",
    "plt.title('Word Count by Class')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average headline length: {df['headline_length'].mean():.2f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
    "print(f\"Max word count: {df['word_count'].max()} words\")\n",
    "print(f\"Min word count: {df['word_count'].min()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Visualization\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Sarcastic headlines\n",
    "sarcastic_text = ' '.join(df[df['is_sarcastic'] == 1]['headline'])\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud_sarcastic = WordCloud(width=800, height=400, background_color='white').generate(sarcastic_text)\n",
    "plt.imshow(wordcloud_sarcastic, interpolation='bilinear')\n",
    "plt.title('Word Cloud - Sarcastic Headlines', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "# Non-sarcastic headlines\n",
    "non_sarcastic_text = ' '.join(df[df['is_sarcastic'] == 0]['headline'])\n",
    "plt.subplot(1, 2, 2)\n",
    "wordcloud_non_sarcastic = WordCloud(width=800, height=400, background_color='white').generate(non_sarcastic_text)\n",
    "plt.imshow(wordcloud_non_sarcastic, interpolation='bilinear')\n",
    "plt.title('Word Cloud - Non-Sarcastic Headlines', fontsize=16)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02356e19",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "df['clean_headline'] = df['headline'].apply(clean_text)\n",
    "\n",
    "# Remove stop words (optional - can be beneficial for some models)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Create version with stopwords removed\n",
    "df['clean_headline_no_stop'] = df['clean_headline'].apply(remove_stopwords)\n",
    "\n",
    "# Display examples\n",
    "print(\"Original vs Cleaned Headlines:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    print(f\"Original: {df.iloc[i]['headline']}\")\n",
    "    print(f\"Cleaned: {df.iloc[i]['clean_headline']}\")\n",
    "    print(f\"No Stopwords: {df.iloc[i]['clean_headline_no_stop']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ccf8d",
   "metadata": {},
   "source": [
    "## PyTorch LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04024e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset Class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab_to_int, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab_to_int = vocab_to_int\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to integers\n",
    "        text_ints = [self.vocab_to_int.get(word, 0) for word in text.split()]\n",
    "        \n",
    "        # Pad or truncate to max_len\n",
    "        if len(text_ints) > self.max_len:\n",
    "            text_ints = text_ints[:self.max_len]\n",
    "        else:\n",
    "            text_ints = text_ints + [0] * (self.max_len - len(text_ints))\n",
    "            \n",
    "        return torch.tensor(text_ints, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Create vocabulary\n",
    "def create_vocab(texts, min_freq=2):\n",
    "    \"\"\"Create vocabulary from texts\"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Create vocabulary (words with frequency >= min_freq)\n",
    "    vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(vocab)}\n",
    "    int_to_vocab = {idx: word for word, idx in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "# Prepare PyTorch data\n",
    "texts = df['clean_headline'].tolist()\n",
    "labels = df['is_sarcastic'].tolist()\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_to_int, int_to_vocab = create_vocab(texts)\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample vocabulary: {list(vocab_to_int.items())[:10]}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Create datasets\n",
    "max_len = 50  # Maximum sequence length\n",
    "train_dataset = SarcasmDataset(X_train, y_train, vocab_to_int, max_len)\n",
    "val_dataset = SarcasmDataset(X_val, y_val, vocab_to_int, max_len)\n",
    "test_dataset = SarcasmDataset(X_test, y_test, vocab_to_int, max_len)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0642576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last output\n",
    "        output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        output = self.dropout(output)\n",
    "        output = torch.relu(self.fc1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "pytorch_model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "pytorch_model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31954275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for PyTorch\n",
    "def train_pytorch_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (output > 0.5).float()\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the PyTorch model\n",
    "print(\"Training PyTorch LSTM Model...\")\n",
    "pytorch_train_losses, pytorch_val_losses, pytorch_train_accs, pytorch_val_accs = train_pytorch_model(\n",
    "    pytorch_model, train_loader, val_loader, criterion, optimizer, num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for PyTorch\n",
    "def evaluate_pytorch_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            probabilities.extend(output.cpu().numpy())\n",
    "            predicted = (output > 0.5).float()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    return np.array(true_labels), np.array(predictions), np.array(probabilities)\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "print(\"Evaluating PyTorch LSTM Model...\")\n",
    "pytorch_y_true, pytorch_y_pred, pytorch_y_prob = evaluate_pytorch_model(pytorch_model, test_loader)\n",
    "\n",
    "# Calculate metrics\n",
    "pytorch_accuracy = accuracy_score(pytorch_y_true, pytorch_y_pred)\n",
    "pytorch_precision = precision_score(pytorch_y_true, pytorch_y_pred)\n",
    "pytorch_recall = recall_score(pytorch_y_true, pytorch_y_pred)\n",
    "pytorch_f1 = f1_score(pytorch_y_true, pytorch_y_pred)\n",
    "pytorch_auc = roc_auc_score(pytorch_y_true, pytorch_y_prob)\n",
    "\n",
    "print(\"PyTorch LSTM Model Results:\")\n",
    "print(f\"Accuracy: {pytorch_accuracy:.4f} ({pytorch_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {pytorch_precision:.4f}\")\n",
    "print(f\"Recall: {pytorch_recall:.4f}\")\n",
    "print(f\"F1-Score: {pytorch_f1:.4f}\")\n",
    "print(f\"AUC: {pytorch_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(pytorch_y_true, pytorch_y_pred, target_names=['Non-Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48698ef",
   "metadata": {},
   "source": [
    "## TensorFlow LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow data preprocessing\n",
    "max_features = 10000  # Maximum number of words to keep\n",
    "max_length = 50       # Maximum sequence length\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_tf = np.array(y_train)\n",
    "y_val_tf = np.array(y_val)\n",
    "y_test_tf = np.array(y_test)\n",
    "\n",
    "print(f\"TensorFlow data shapes:\")\n",
    "print(f\"X_train: {X_train_pad.shape}\")\n",
    "print(f\"X_val: {X_val_pad.shape}\")\n",
    "print(f\"X_test: {X_test_pad.shape}\")\n",
    "print(f\"y_train: {y_train_tf.shape}\")\n",
    "print(f\"y_val: {y_val_tf.shape}\")\n",
    "print(f\"y_test: {y_test_tf.shape}\")\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size_tf = min(max_features, len(tokenizer.word_index) + 1)\n",
    "print(f\"TensorFlow vocabulary size: {vocab_size_tf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow LSTM Model\n",
    "def create_lstm_model(vocab_size, embedding_dim=100, lstm_units=128, dropout=0.3):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)),\n",
    "        Bidirectional(LSTM(lstm_units//2, dropout=dropout, recurrent_dropout=dropout)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "tensorflow_model = create_lstm_model(vocab_size_tf)\n",
    "tensorflow_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"TensorFlow Model Summary:\")\n",
    "tensorflow_model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training TensorFlow LSTM Model...\")\n",
    "tensorflow_history = tensorflow_model.fit(\n",
    "    X_train_pad, y_train_tf,\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=(X_val_pad, y_val_tf),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2206c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TensorFlow model\n",
    "print(\"Evaluating TensorFlow LSTM Model...\")\n",
    "\n",
    "# Get predictions\n",
    "tensorflow_y_prob = tensorflow_model.predict(X_test_pad)\n",
    "tensorflow_y_pred = (tensorflow_y_prob > 0.5).astype(int).flatten()\n",
    "tensorflow_y_prob = tensorflow_y_prob.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "tensorflow_accuracy = accuracy_score(y_test_tf, tensorflow_y_pred)\n",
    "tensorflow_precision = precision_score(y_test_tf, tensorflow_y_pred)\n",
    "tensorflow_recall = recall_score(y_test_tf, tensorflow_y_pred)\n",
    "tensorflow_f1 = f1_score(y_test_tf, tensorflow_y_pred)\n",
    "tensorflow_auc = roc_auc_score(y_test_tf, tensorflow_y_prob)\n",
    "\n",
    "print(\"TensorFlow LSTM Model Results:\")\n",
    "print(f\"Accuracy: {tensorflow_accuracy:.4f} ({tensorflow_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {tensorflow_precision:.4f}\")\n",
    "print(f\"Recall: {tensorflow_recall:.4f}\")\n",
    "print(f\"F1-Score: {tensorflow_f1:.4f}\")\n",
    "print(f\"AUC: {tensorflow_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tf, tensorflow_y_pred, target_names=['Non-Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada4187",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Keras Tuner\n",
    "def build_tuner_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(\n",
    "        vocab_size_tf, \n",
    "        hp.Int('embedding_dim', min_value=50, max_value=200, step=50),\n",
    "        input_length=max_length\n",
    "    ))\n",
    "    \n",
    "    # LSTM layers\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        hp.Int('lstm_units_1', min_value=64, max_value=256, step=64),\n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1),\n",
    "        recurrent_dropout=hp.Float('recurrent_dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(\n",
    "        hp.Int('lstm_units_2', min_value=32, max_value=128, step=32),\n",
    "        dropout=hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1),\n",
    "        recurrent_dropout=hp.Float('recurrent_dropout_2', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    )))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('final_dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuner_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Reduced for faster execution\n",
    "    directory='keras_tuner',\n",
    "    project_name='sarcasm_detection'\n",
    ")\n",
    "\n",
    "print(\"Tuner search space:\")\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Perform hyperparameter search\n",
    "print(\"Starting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_pad, y_train_tf,\n",
    "    epochs=10,  # Reduced epochs for faster tuning\n",
    "    validation_data=(X_val_pad, y_val_tf),\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(f\"Embedding dim: {best_hps.get('embedding_dim')}\")\n",
    "print(f\"LSTM units 1: {best_hps.get('lstm_units_1')}\")\n",
    "print(f\"LSTM units 2: {best_hps.get('lstm_units_2')}\")\n",
    "print(f\"Dense units: {best_hps.get('dense_units')}\")\n",
    "print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Dropout 1: {best_hps.get('dropout_1')}\")\n",
    "print(f\"Dropout 2: {best_hps.get('dropout_2')}\")\n",
    "print(f\"Final dropout: {best_hps.get('final_dropout')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef567cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "print(\"Training the best model with optimal hyperparameters...\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train with more epochs\n",
    "best_history = best_model.fit(\n",
    "    X_train_pad, y_train_tf,\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=(X_val_pad, y_val_tf),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Evaluating the best tuned model...\")\n",
    "best_y_prob = best_model.predict(X_test_pad)\n",
    "best_y_pred = (best_y_prob > 0.5).astype(int).flatten()\n",
    "best_y_prob = best_y_prob.flatten()\n",
    "\n",
    "# Calculate metrics for best model\n",
    "best_accuracy = accuracy_score(y_test_tf, best_y_pred)\n",
    "best_precision = precision_score(y_test_tf, best_y_pred)\n",
    "best_recall = recall_score(y_test_tf, best_y_pred)\n",
    "best_f1 = f1_score(y_test_tf, best_y_pred)\n",
    "best_auc = roc_auc_score(y_test_tf, best_y_prob)\n",
    "\n",
    "print(\"Best Tuned LSTM Model Results:\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {best_precision:.4f}\")\n",
    "print(f\"Recall: {best_recall:.4f}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")\n",
    "print(f\"AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_tf, best_y_pred, target_names=['Non-Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31aa29d",
   "metadata": {},
   "source": [
    "## Model Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# PyTorch training plots\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(pytorch_train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(pytorch_val_losses, label='Validation Loss', marker='s')\n",
    "plt.title('PyTorch LSTM - Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(pytorch_train_accs, label='Train Accuracy', marker='o')\n",
    "plt.plot(pytorch_val_accs, label='Validation Accuracy', marker='s')\n",
    "plt.title('PyTorch LSTM - Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# TensorFlow training plots\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(tensorflow_history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(tensorflow_history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('TensorFlow LSTM - Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(tensorflow_history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(tensorflow_history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('TensorFlow LSTM - Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Best model training plots\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.plot(best_history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(best_history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('Best Tuned Model - Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.plot(best_history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('Best Tuned Model - Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Model comparison\n",
    "plt.subplot(3, 3, 3)\n",
    "models = ['PyTorch\\nLSTM', 'TensorFlow\\nLSTM', 'Best Tuned\\nModel']\n",
    "accuracies = [pytorch_accuracy * 100, tensorflow_accuracy * 100, best_accuracy * 100]\n",
    "bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'orange'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, accuracy in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{accuracy:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add horizontal line at 80%\n",
    "plt.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% Target')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "f1_scores = [pytorch_f1, tensorflow_f1, best_f1]\n",
    "bars = plt.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange'])\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, f1 in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 3, 9)\n",
    "auc_scores = [pytorch_auc, tensorflow_auc, best_auc]\n",
    "bars = plt.bar(models, auc_scores, color=['skyblue', 'lightgreen', 'orange'])\n",
    "plt.title('Model AUC Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, auc in zip(bars, auc_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{auc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curves\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Confusion matrices\n",
    "models_data = [\n",
    "    ('PyTorch LSTM', pytorch_y_true, pytorch_y_pred, pytorch_y_prob),\n",
    "    ('TensorFlow LSTM', y_test_tf, tensorflow_y_pred, tensorflow_y_prob),\n",
    "    ('Best Tuned Model', y_test_tf, best_y_pred, best_y_prob)\n",
    "]\n",
    "\n",
    "for i, (model_name, y_true, y_pred, y_prob) in enumerate(models_data):\n",
    "    # Confusion Matrix\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Sarcastic', 'Sarcastic'],\n",
    "                yticklabels=['Non-Sarcastic', 'Sarcastic'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "# ROC Curves\n",
    "plt.subplot(2, 3, 4)\n",
    "for model_name, y_true, y_pred, y_prob in models_data:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall curves\n",
    "plt.subplot(2, 3, 5)\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "for model_name, y_true, y_pred, y_prob in models_data:\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    avg_precision = average_precision_score(y_true, y_prob)\n",
    "    plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})', linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison table\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "\n",
    "metrics_data = []\n",
    "for model_name, y_true, y_pred, y_prob in models_data:\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    metrics_data.append([model_name, f'{accuracy:.3f}', f'{precision:.3f}', \n",
    "                        f'{recall:.3f}', f'{f1:.3f}', f'{auc:.3f}'])\n",
    "\n",
    "table = plt.table(cellText=metrics_data,\n",
    "                 colLabels=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "plt.title('Model Performance Metrics', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d668f1",
   "metadata": {},
   "source": [
    "## Model Saving and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd16a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "# Save PyTorch model\n",
    "torch.save({\n",
    "    'model_state_dict': pytorch_model.state_dict(),\n",
    "    'vocab_to_int': vocab_to_int,\n",
    "    'model_params': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "}, 'pytorch_lstm_sarcasm_model.pth')\n",
    "\n",
    "# Save TensorFlow models\n",
    "tensorflow_model.save('tensorflow_lstm_sarcasm_model.keras')\n",
    "best_model.save('best_tuned_lstm_sarcasm_model.keras')\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- pytorch_lstm_sarcasm_model.pth\")\n",
    "print(\"- tensorflow_lstm_sarcasm_model.keras\")\n",
    "print(\"- best_tuned_lstm_sarcasm_model.keras\")\n",
    "print(\"- tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*80)\n",
    "print(\"LSTM SARCASM DETECTION MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"- Total samples: {len(df):,}\")\n",
    "print(f\"- Training samples: {len(X_train):,}\")\n",
    "print(f\"- Validation samples: {len(X_val):,}\")\n",
    "print(f\"- Test samples: {len(X_test):,}\")\n",
    "print(f\"- Sarcasm ratio: {df['is_sarcastic'].mean():.2%}\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'PyTorch LSTM':<20} {pytorch_accuracy:.3f}     {pytorch_precision:.3f}      {pytorch_recall:.3f}    {pytorch_f1:.3f}      {pytorch_auc:.3f}\")\n",
    "print(f\"{'TensorFlow LSTM':<20} {tensorflow_accuracy:.3f}     {tensorflow_precision:.3f}      {tensorflow_recall:.3f}    {tensorflow_f1:.3f}      {tensorflow_auc:.3f}\")\n",
    "print(f\"{'Best Tuned Model':<20} {best_accuracy:.3f}     {best_precision:.3f}      {best_recall:.3f}    {best_f1:.3f}      {best_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nKey Achievements:\")\n",
    "print(f\"✓ Implemented LSTM models in both PyTorch and TensorFlow\")\n",
    "print(f\"✓ Comprehensive evaluation metrics (Accuracy, Precision, Recall, F1-Score, AUC)\")\n",
    "print(f\"✓ Visualized training progress and model comparisons\")\n",
    "print(f\"✓ Performed hyperparameter tuning with Keras Tuner\")\n",
    "\n",
    "# Check if 80% accuracy target is met\n",
    "models_above_80 = []\n",
    "if pytorch_accuracy >= 0.8:\n",
    "    models_above_80.append(\"PyTorch LSTM\")\n",
    "if tensorflow_accuracy >= 0.8:\n",
    "    models_above_80.append(\"TensorFlow LSTM\")\n",
    "if best_accuracy >= 0.8:\n",
    "    models_above_80.append(\"Best Tuned Model\")\n",
    "\n",
    "if models_above_80:\n",
    "    print(f\"✓ Achieved 80%+ accuracy target with: {', '.join(models_above_80)}\")\n",
    "else:\n",
    "    print(f\"⚠ None of the models achieved the 80% accuracy target\")\n",
    "    print(f\"  Best accuracy: {max(pytorch_accuracy, tensorflow_accuracy, best_accuracy):.3f}\")\n",
    "\n",
    "print(f\"\\nBest Overall Model: \", end=\"\")\n",
    "best_overall_acc = max(pytorch_accuracy, tensorflow_accuracy, best_accuracy)\n",
    "if best_overall_acc == pytorch_accuracy:\n",
    "    print(\"PyTorch LSTM\")\n",
    "elif best_overall_acc == tensorflow_accuracy:\n",
    "    print(\"TensorFlow LSTM\")\n",
    "else:\n",
    "    print(\"Best Tuned Model\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"1. Use Google Colab with T4 GPU or TPU for faster training\")\n",
    "print(f\"2. Consider ensemble methods to improve performance\")\n",
    "print(f\"3. Explore pre-trained embeddings (Word2Vec, GloVe, BERT)\")\n",
    "print(f\"4. Try different text preprocessing techniques\")\n",
    "print(f\"5. Experiment with different model architectures (GRU, Transformer)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e939c",
   "metadata": {},
   "source": [
    "## Sample Predictions\n",
    "\n",
    "Let's test our best model with some sample headlines to see how it performs on individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new text\n",
    "def predict_sarcasm(text, model, tokenizer, max_length=50):\n",
    "    \"\"\"Predict if a headline is sarcastic or not\"\"\"\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    probability = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    prediction = 1 if probability > 0.5 else 0\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test with sample headlines\n",
    "sample_headlines = [\n",
    "    \"Breaking: Local man discovers amazing new way to waste time\",\n",
    "    \"Scientists announce breakthrough in cancer research\",\n",
    "    \"Area man's opinion on politics changes absolutely everything\",\n",
    "    \"New study shows exercise may be good for your health\",\n",
    "    \"Local woman shocked to discover water is wet\",\n",
    "    \"Government announces new policy to help citizens\",\n",
    "    \"Man wins lottery, quits job to pursue dream of being unemployed\",\n",
    "    \"Research reveals shocking truth: people need food to survive\"\n",
    "]\n",
    "\n",
    "print(\"Sample Sarcasm Predictions using Best Tuned Model:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Headline':<60} {'Predicted':<12} {'Probability':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for headline in sample_headlines:\n",
    "    prediction, probability = predict_sarcasm(headline, best_model, tokenizer)\n",
    "    prediction_text = \"Sarcastic\" if prediction == 1 else \"Not Sarcastic\"\n",
    "    print(f\"{headline[:58]:<60} {prediction_text:<12} {probability:.3f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Let's also show some actual examples from our test set with their predictions\n",
    "print(\"\\nActual Test Examples vs Predictions:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Headline':<50} {'Actual':<12} {'Predicted':<12} {'Probability':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Show first 10 test examples\n",
    "for i in range(min(10, len(X_test))):\n",
    "    actual = y_test_tf[i]\n",
    "    prediction, probability = predict_sarcasm(X_test[i], best_model, tokenizer)\n",
    "    actual_text = \"Sarcastic\" if actual == 1 else \"Not Sarcastic\"\n",
    "    prediction_text = \"Sarcastic\" if prediction == 1 else \"Not Sarcastic\"\n",
    "    \n",
    "    # Truncate headline for display\n",
    "    headline_display = X_test[i][:48] + \"...\" if len(X_test[i]) > 48 else X_test[i]\n",
    "    \n",
    "    print(f\"{headline_display:<50} {actual_text:<12} {prediction_text:<12} {probability:.3f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
